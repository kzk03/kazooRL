import json
import pickle  # â–¼â–¼â–¼ pickleã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ â–¼â–¼â–¼
from datetime import datetime

import numpy as np
import torch
import torch.optim as optim
import yaml
from omegaconf import OmegaConf
from tqdm import tqdm

from kazoo.envs.oss_simple import OSSSimpleEnv
from kazoo.envs.task import Task
from kazoo.features.feature_extractor import FeatureExtractor


def main():
    """é€†å¼·åŒ–å­¦ç¿’ã®ãƒ¡ã‚¤ãƒ³å‡¦ç†."""

    print("1. Loading configuration and data...")
    cfg = OmegaConf.load("configs/base_training.yaml")

    try:
        with open(cfg.irl.expert_path, "rb") as f:
            trajectories = pickle.load(f)
            # è»Œè·¡ã¯1ã¤ã—ã‹ãªã„ã®ã§ã€æœ€åˆã®è¦ç´ ã‚’å–å¾—
            expert_trajectory_steps = trajectories[0] if trajectories else []
    except (FileNotFoundError, pickle.UnpicklingError) as e:
        print(f"Error loading expert trajectories from {cfg.irl.expert_path}: {e}")
        print("Please run 'create_expert_trajectories.py' first.")
        return

    if not expert_trajectory_steps:
        print("No expert steps found in the trajectory file. Exiting.")
        return

    with open(cfg.env.backlog_path, "r", encoding="utf-8") as f:
        backlog_data = json.load(f)
    with open(cfg.env.dev_profiles_path, "r", encoding="utf-8") as f:
        dev_profiles_data = yaml.safe_load(f)

    print("2. Initializing environment and feature extractor...")
    env = OSSSimpleEnv(cfg, backlog_data, dev_profiles_data)
    feature_extractor = FeatureExtractor(cfg)
    feature_dim = len(feature_extractor.feature_names)

    # å…¨ã‚¿ã‚¹ã‚¯ã‚’IDã§æ¤œç´¢ã§ãã‚‹ã‚ˆã†ã«è¾žæ›¸åŒ–
    all_tasks_db = {task.id: task for task in env.backlog}

    print(f"3. Setting up IRL model with {feature_dim} features...")
    reward_weights = torch.randn(feature_dim, requires_grad=True)
    optimizer = optim.Adam([reward_weights], lr=cfg.irl.learning_rate)

    print(
        f"4. Starting training loop with {len(expert_trajectory_steps)} expert steps..."
    )

    # é€²æ—è¡¨ç¤ºç”¨ã®å¤‰æ•°
    total_steps = len(expert_trajectory_steps)
    processed_steps = 0
    valid_steps = 0

    # ã‚¨ãƒãƒƒã‚¯ç”¨ã®ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼
    epoch_pbar = tqdm(range(cfg.irl.epochs), desc="ðŸ§  IRL è¨“ç·´", unit="epoch")

    for epoch in epoch_pbar:
        total_loss = 0
        epoch_valid_steps = 0

        # ã‚¹ãƒ†ãƒƒãƒ—ç”¨ã®ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼
        step_pbar = tqdm(
            expert_trajectory_steps, desc=f"Epoch {epoch + 1}", leave=False, unit="step"
        )

        # .pklãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰èª­ã¿è¾¼ã‚“ã è»Œè·¡ã®å„ã‚¹ãƒ†ãƒƒãƒ—ã‚’ãƒ«ãƒ¼ãƒ—
        for step_idx, step_data in enumerate(step_pbar):
            optimizer.zero_grad()

            # --- è»Œè·¡ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰çŠ¶æ…‹ã¨è¡Œå‹•ã‚’å–å¾— ---
            state = step_data["state"]
            action_details = step_data["action_details"]

            developer_id = action_details.get("developer")
            expert_task_id = action_details.get("task_id")
            event_timestamp = datetime.fromisoformat(
                action_details.get("timestamp").replace("Z", "+00:00")
            )

            # --- ç‰¹å¾´é‡è¨ˆç®—ã®æº–å‚™ ---
            developer_profile = dev_profiles_data.get(developer_id)
            expert_task = all_tasks_db.get(expert_task_id)
            if not developer_profile or not expert_task:
                continue

            epoch_valid_steps += 1
            developer_obj = {"name": developer_id, "profile": developer_profile}

            # ç‰¹å¾´é‡è¨ˆç®—ã®ãŸã‚ã«ç’°å¢ƒã®çŠ¶æ…‹ã‚’ä¸€æ™‚çš„ã«è¨­å®š
            env.current_time = event_timestamp

            # --- ç‰¹å¾´é‡ã®è¨ˆç®— ---
            expert_features = feature_extractor.get_features(
                expert_task, developer_obj, env
            )

            if expert_features is None:
                step_pbar.set_postfix(
                    {"warning": f"None features for task {expert_task_id}"}
                )
                continue

            expert_features = torch.from_numpy(expert_features).float()

            other_features_list = []
            # ã€Œå–ã‚Šå¾—ãŸä»–ã®è¡Œå‹•ã€ã¯ã€ãã®æ™‚ç‚¹ã§ã®ã‚ªãƒ¼ãƒ—ãƒ³ãªã‚¿ã‚¹ã‚¯ãƒªã‚¹ãƒˆ
            for other_task_id in state["open_task_ids"]:
                if other_task_id != expert_task_id:
                    other_task = all_tasks_db.get(other_task_id)
                    if other_task:
                        features = feature_extractor.get_features(
                            other_task, developer_obj, env
                        )
                        if features is not None:
                            other_features_list.append(
                                torch.from_numpy(features).float()
                            )

            if not other_features_list:
                epoch_valid_steps -= 1  # æœ‰åŠ¹ã‚¹ãƒ†ãƒƒãƒ—ã‹ã‚‰é™¤å¤–
                continue

            # --- æå¤±ã®è¨ˆç®—ã¨æ›´æ–° ---
            expert_reward = torch.dot(reward_weights, expert_features)
            other_rewards = torch.stack(
                [torch.dot(reward_weights, f) for f in other_features_list]
            )
            log_sum_exp_other_rewards = torch.logsumexp(other_rewards, dim=0)

            loss = -(expert_reward - log_sum_exp_other_rewards)
            total_loss += loss.item()

            loss.backward()
            optimizer.step()

            # ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼ã®æƒ…å ±ã‚’æ›´æ–°
            if epoch_valid_steps > 0:
                avg_loss = total_loss / epoch_valid_steps
                step_pbar.set_postfix(
                    {
                        "loss": f"{avg_loss:.4f}",
                        "valid": f"{epoch_valid_steps}/{step_idx+1}",
                    }
                )

        # ã‚¨ãƒãƒƒã‚¯çµ‚äº†æ™‚ã®çµ±è¨ˆã‚’ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼ã«åæ˜ 
        if epoch_valid_steps > 0:
            avg_loss = total_loss / epoch_valid_steps
            epoch_pbar.set_postfix(
                {
                    "loss": f"{avg_loss:.4f}",
                    "valid_steps": f"{epoch_valid_steps}/{total_steps}",
                }
            )

        valid_steps += epoch_valid_steps

    epoch_pbar.close()
    print(f"\n5. Training finished. Total valid steps processed: {valid_steps}")
    print("Saving learned weights...")
    np.save(cfg.irl.output_weights_path, reward_weights.detach().numpy())
    print(f"âœ… Learned reward weights saved to: {cfg.irl.output_weights_path}")
    print(f"   Weight shape: {reward_weights.shape}")
    print(
        f"   Weight stats: min={reward_weights.min():.4f}, max={reward_weights.max():.4f}, mean={reward_weights.mean():.4f}"
    )


if __name__ == "__main__":
    main()
