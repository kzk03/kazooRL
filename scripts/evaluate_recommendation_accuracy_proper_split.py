#!/usr/bin/env python3
"""
å¼·åŒ–å­¦ç¿’ã§å­¦ç¿’ã—ãŸãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ãŸæ¨è–¦ã‚·ã‚¹ãƒ†ãƒ ã®ç²¾åº¦è©•ä¾¡ï¼ˆé©åˆ‡ãªãƒ‡ãƒ¼ã‚¿åˆ†å‰²ç‰ˆï¼‰

ã“ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã¯ï¼š
1. æ™‚ç³»åˆ—é †ã«ãƒ‡ãƒ¼ã‚¿ã‚’åˆ†å‰²ï¼ˆ70%å­¦ç¿’ã€30%ãƒ†ã‚¹ãƒˆï¼‰
2. å­¦ç¿’æ¸ˆã¿PPOãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿
3. ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã«å¯¾ã—ã¦ã®ã¿æ¨è–¦ã‚’å®Ÿè¡Œ
4. æ­£è§£ãƒ‡ãƒ¼ã‚¿ï¼ˆã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆãƒˆãƒ©ã‚¸ã‚§ã‚¯ãƒˆãƒªï¼‰ã¨æ¯”è¼ƒ
5. æ¨è–¦ç²¾åº¦ï¼ˆTop-K accuracyï¼‰ã‚’è¨ˆç®—
"""

import os
import pickle
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Tuple

import hydra
import numpy as np
import torch
from omegaconf import DictConfig

from kazoo.envs.oss_simple import OSSSimpleEnv
from kazoo.learners.independent_ppo_controller import IndependentPPOController


class ProperSplitRecommendationEvaluator:
    """é©åˆ‡ãªãƒ‡ãƒ¼ã‚¿åˆ†å‰²ã«ã‚ˆã‚‹æ¨è–¦ã‚·ã‚¹ãƒ†ãƒ ã®ç²¾åº¦è©•ä¾¡ã‚¯ãƒ©ã‚¹"""

    def __init__(self, cfg: DictConfig, train_ratio: float = 0.7):
        self.cfg = cfg
        self.train_ratio = train_ratio
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

        # ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ã¨åˆ†å‰²
        import json

        import yaml

        with open(cfg.env.backlog_path, "r", encoding="utf-8") as f:
            self.full_backlog = json.load(f)
        with open(cfg.env.dev_profiles_path, "r", encoding="utf-8") as f:
            self.dev_profiles = yaml.safe_load(f)

        # ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿
        self.expert_trajectories = self.load_expert_data()

        # ãƒ‡ãƒ¼ã‚¿åˆ†å‰²ã®å®Ÿè¡Œ
        self.split_data()

        # ç’°å¢ƒã¨ã‚³ãƒ³ãƒˆãƒ­ãƒ¼ãƒ©ãƒ¼ã®åˆæœŸåŒ–ï¼ˆãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®ã¿ä½¿ç”¨ï¼‰
        self.env = OSSSimpleEnv(cfg, self.test_backlog, self.dev_profiles)
        self.controller = IndependentPPOController(self.env, cfg)

        # å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿
        self.load_trained_models()

    def load_expert_data(self) -> List[Dict]:
        """ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆè»Œè·¡ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã‚€ï¼ˆæ­£è§£ãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦ä½¿ç”¨ï¼‰"""
        expert_path = Path(self.cfg.irl.expert_path)

        if not expert_path.exists():
            print(f"âš ï¸  Expert data not found: {expert_path}")
            return []

        with open(expert_path, "rb") as f:
            expert_data = pickle.load(f)

        print(f"Loaded {len(expert_data)} expert trajectories")
        return expert_data

    def split_data(self):
        """æ™‚ç³»åˆ—é †ã«ãƒ‡ãƒ¼ã‚¿ã‚’å­¦ç¿’ãƒ»ãƒ†ã‚¹ãƒˆç”¨ã«åˆ†å‰²ï¼ˆbotã‚¿ã‚¹ã‚¯ã‚’é™¤å¤–ï¼‰"""
        # ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆãƒ‡ãƒ¼ã‚¿ã®ã‚¿ã‚¹ã‚¯IDã‚’å–å¾—ï¼ˆbotã‚’é™¤å¤–ï¼‰
        expert_task_ids = set()
        bot_task_count = 0
        for trajectory_episode in self.expert_trajectories:
            for step in trajectory_episode:
                if isinstance(step, dict) and "action_details" in step:
                    task_id = step["action_details"].get("task_id")
                    assigned_dev = step["action_details"].get("developer")
                    if task_id and assigned_dev:
                        # botã‚¿ã‚¹ã‚¯ã‚’é™¤å¤–
                        if (
                            "bot" in assigned_dev.lower()
                            or assigned_dev == "stale[bot]"
                        ):
                            bot_task_count += 1
                            continue
                        expert_task_ids.add(task_id)

        print(f"Excluded {bot_task_count} bot tasks from evaluation")

        # ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚‹ã‚¿ã‚¹ã‚¯ã®ã¿ã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
        expert_tasks = []
        for task in self.full_backlog:
            if task["id"] in expert_task_ids and "created_at" in task:
                task["created_at_dt"] = datetime.fromisoformat(
                    task["created_at"].replace("Z", "+00:00")
                )
                expert_tasks.append(task)

        # æ™‚ç³»åˆ—é †ã«ã‚½ãƒ¼ãƒˆ
        expert_tasks_sorted = sorted(expert_tasks, key=lambda x: x["created_at_dt"])

        # åˆ†å‰²ç‚¹ã‚’è¨ˆç®—
        split_index = int(len(expert_tasks_sorted) * self.train_ratio)

        self.train_tasks = expert_tasks_sorted[:split_index]
        self.test_tasks = expert_tasks_sorted[split_index:]

        # ãƒ†ã‚¹ãƒˆç”¨ã®ãƒãƒƒã‚¯ãƒ­ã‚°ã‚’ä½œæˆï¼ˆcreated_at_dtãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’å‰Šé™¤ï¼‰
        self.test_backlog = []
        for task in self.test_tasks:
            task_copy = task.copy()
            if "created_at_dt" in task_copy:
                del task_copy["created_at_dt"]
            self.test_backlog.append(task_copy)

        print(f"Data split completed:")
        print(f"  Total expert tasks: {len(expert_tasks_sorted)}")
        print(
            f"  Train tasks: {len(self.train_tasks)} (up to {self.train_tasks[-1]['created_at'] if self.train_tasks else 'N/A'})"
        )
        print(
            f"  Test tasks: {len(self.test_tasks)} (from {self.test_tasks[0]['created_at'] if self.test_tasks else 'N/A'})"
        )

        # ãƒˆãƒ¬ã‚¤ãƒ³ãƒ»ãƒ†ã‚¹ãƒˆåˆ†å‰²ã•ã‚ŒãŸã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆ
        train_task_ids = {task["id"] for task in self.train_tasks}
        test_task_ids = {task["id"] for task in self.test_tasks}

        self.train_expert_assignments = {}
        self.test_expert_assignments = {}

        for trajectory_episode in self.expert_trajectories:
            for step in trajectory_episode:
                if isinstance(step, dict) and "action_details" in step:
                    action_details = step["action_details"]
                    task_id = action_details.get("task_id")
                    assigned_dev = action_details.get("developer")
                    if task_id and assigned_dev:
                        # botã‚¿ã‚¹ã‚¯ã‚’é™¤å¤–
                        if (
                            "bot" in assigned_dev.lower()
                            or assigned_dev == "stale[bot]"
                        ):
                            continue
                        if task_id in train_task_ids:
                            self.train_expert_assignments[task_id] = assigned_dev
                        elif task_id in test_task_ids:
                            self.test_expert_assignments[task_id] = assigned_dev

        print(f"  Train expert assignments: {len(self.train_expert_assignments)}")
        print(f"  Test expert assignments: {len(self.test_expert_assignments)}")

    def load_trained_models(self):
        """å­¦ç¿’æ¸ˆã¿PPOãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã‚€"""
        model_dir = Path(self.cfg.rl.output_model_dir)

        if not model_dir.exists():
            raise FileNotFoundError(f"Model directory not found: {model_dir}")

        print(f"Loading trained models from: {model_dir}")

        loaded_count = 0
        for agent_id in self.controller.agent_ids:
            model_path = model_dir / f"ppo_agent_{agent_id}.pth"
            if model_path.exists():
                try:
                    self.controller.agents[agent_id].load(str(model_path))
                    loaded_count += 1
                    print(f"âœ… Loaded model for {agent_id}")
                except Exception as e:
                    print(f"âŒ Failed to load model for {agent_id}: {e}")
            else:
                print(f"âš ï¸  Model not found for {agent_id}: {model_path}")

        print(
            f"Successfully loaded {loaded_count}/{len(self.controller.agent_ids)} models"
        )

    def get_model_recommendations(self, max_k: int = 5) -> List[Tuple[str, List[str]]]:
        """
        å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ã¦ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã«å¯¾ã™ã‚‹æ¨è–¦ã‚’ç”Ÿæˆï¼ˆTop-Kå¯¾å¿œï¼‰
        ãƒ¢ãƒ‡ãƒ«ãŒèª­ã¿è¾¼ã‚ãªã„å ´åˆã¯ãƒ©ãƒ³ãƒ€ãƒ æ¨è–¦ã‚’ç”Ÿæˆ

        Args:
            max_k: æ¨è–¦ã™ã‚‹é–‹ç™ºè€…ã®æœ€å¤§æ•°

        Returns:
            List of (task_id, [recommended_developers]) tuples
            æ¨è–¦é–‹ç™ºè€…ãƒªã‚¹ãƒˆã¯ç¢ºç‡ã®é«˜ã„é †ã«ã‚½ãƒ¼ãƒˆæ¸ˆã¿
        """
        print(f"Generating recommendations for {len(self.test_backlog)} test tasks")

        recommendations = []

        # ãƒ¢ãƒ‡ãƒ«ãŒèª­ã¿è¾¼ã¾ã‚Œã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
        models_loaded = False
        for agent_id in self.controller.agent_ids:
            try:
                # ãƒ¢ãƒ‡ãƒ«ãŒä½¿ç”¨å¯èƒ½ã‹ãƒ†ã‚¹ãƒˆ
                test_obs = np.random.rand(451)  # ç¾åœ¨ã®è¦³æ¸¬æ¬¡å…ƒ
                obs_tensor = torch.FloatTensor(test_obs).to(
                    self.controller.agents[agent_id].device
                )
                with torch.no_grad():
                    action_probs = (
                        self.controller.agents[agent_id]
                        .policy.actor(obs_tensor)
                        .cpu()
                        .numpy()
                    )
                models_loaded = True
                break
            except:
                continue

        if not models_loaded:
            print(
                "âš ï¸  No models loaded successfully. Using random baseline for comparison."
            )
            # ãƒ©ãƒ³ãƒ€ãƒ æ¨è–¦ã‚’ç”Ÿæˆï¼ˆbotã‚’é™¤å¤–ï¼‰
            import random

            developer_list = [
                dev
                for dev in self.dev_profiles.keys()
                if not ("bot" in dev.lower() or dev == "stale[bot]")
            ]
            for task in self.test_backlog:
                random_devs = random.sample(
                    developer_list, min(max_k, len(developer_list))
                )
                recommendations.append((task["id"], random_devs))
            return recommendations

        # ç’°å¢ƒã‚’ãƒªã‚»ãƒƒãƒˆ
        observations = self.env.reset()

        for step in range(len(self.test_backlog)):
            if not self.env.backlog:
                break

            # ç¾åœ¨ã®ã‚¿ã‚¹ã‚¯ã‚’å–å¾—
            current_task = self.env.backlog[0]

            # å„é–‹ç™ºè€…ã«å¯¾ã™ã‚‹è¡Œå‹•ç¢ºç‡ã‚’è¨ˆç®—
            developer_scores = {}

            for agent_id in self.controller.agent_ids:
                if agent_id in observations:
                    obs = observations[agent_id]

                    try:
                        # ãƒ¢ãƒ‡ãƒ«ã‹ã‚‰è¡Œå‹•ç¢ºç‡ã‚’å–å¾—
                        with torch.no_grad():
                            obs_tensor = torch.FloatTensor(obs).to(
                                self.controller.agents[agent_id].device
                            )
                            action_probs = (
                                self.controller.agents[agent_id]
                                .policy.actor(obs_tensor)
                                .cpu()
                                .numpy()
                            )

                        # ã‚¢ã‚¯ã‚·ãƒ§ãƒ³2ï¼ˆã‚¿ã‚¹ã‚¯ã‚’å—ã‘å…¥ã‚Œã‚‹ï¼‰ã®ç¢ºç‡ã‚’ä½¿ç”¨
                        accept_prob = (
                            action_probs[2].item() if len(action_probs) > 2 else 0.0
                        )
                        developer_scores[agent_id] = accept_prob
                    except:
                        # ãƒ¢ãƒ‡ãƒ«ã‚¨ãƒ©ãƒ¼ã®å ´åˆã¯ãƒ©ãƒ³ãƒ€ãƒ å€¤ã‚’ä½¿ç”¨
                        developer_scores[agent_id] = np.random.random()

            # ç¢ºç‡ã®é«˜ã„é †ã«é–‹ç™ºè€…ã‚’ã‚½ãƒ¼ãƒˆã—ã¦Top-Kæ¨è–¦ã‚’ç”Ÿæˆ
            if developer_scores:
                sorted_developers = sorted(
                    developer_scores.items(), key=lambda x: x[1], reverse=True
                )
                top_k_developers = [dev for dev, score in sorted_developers[:max_k]]
                recommendations.append((current_task.id, top_k_developers))

                # æœ€ã‚‚ç¢ºç‡ã®é«˜ã„é–‹ç™ºè€…ãŒã‚¿ã‚¹ã‚¯ã‚’å—ã‘å…¥ã‚Œã‚‹ï¼ˆã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ï¼‰
                recommended_dev = top_k_developers[0] if top_k_developers else None
            else:
                recommendations.append((current_task.id, []))
                recommended_dev = None

            # æ¨è–¦ã‚’å®Ÿè¡Œï¼ˆã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ï¼‰
            actions = {
                agent_id: 0 for agent_id in self.controller.agent_ids
            }  # å…¨å“¡ãŒå¾…æ©Ÿ
            if recommended_dev and recommended_dev in actions:
                actions[recommended_dev] = 2  # æ¨è–¦ã•ã‚ŒãŸé–‹ç™ºè€…ãŒã‚¿ã‚¹ã‚¯ã‚’å—ã‘å…¥ã‚Œ

            observations, rewards, terminateds, truncateds, infos = self.env.step(
                actions
            )

            # çµ‚äº†æ¡ä»¶ãƒã‚§ãƒƒã‚¯
            if all(terminateds.values()) or all(truncateds.values()):
                break

        print(f"Generated {len(recommendations)} recommendations for test data")
        return recommendations

    def calculate_accuracy(
        self,
        recommendations: List[Tuple[str, List[str]]],
        k_values: List[int] = [1, 3, 5],
    ) -> Dict[str, float]:
        """
        æ¨è–¦ç²¾åº¦ã‚’è¨ˆç®—ï¼ˆTop-Kå¯¾å¿œã€ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®ã¿ä½¿ç”¨ã€botã‚¿ã‚¹ã‚¯ã‚’é™¤å¤–ï¼‰

        Args:
            recommendations: ãƒ¢ãƒ‡ãƒ«ã®æ¨è–¦çµæœ [(task_id, [recommended_devs]), ...]
            k_values: Top-Kç²¾åº¦ã‚’è¨ˆç®—ã™ã‚‹Kã®å€¤ã®ãƒªã‚¹ãƒˆ

        Returns:
            å„Kå€¤ã«å¯¾ã™ã‚‹ç²¾åº¦ã®è¾æ›¸
        """
        # botã‚¿ã‚¹ã‚¯ã‚’é™¤å¤–ã—ãŸãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®ã¿ã‚’ä½¿ç”¨
        human_test_assignments = {
            task_id: dev
            for task_id, dev in self.test_expert_assignments.items()
            if not ("bot" in dev.lower() or dev == "stale[bot]")
        }

        print(
            f"Human test expert assignments available for {len(human_test_assignments)} tasks"
        )
        print(
            f"Bot test assignments excluded: {len(self.test_expert_assignments) - len(human_test_assignments)}"
        )

        # ç²¾åº¦è¨ˆç®—
        accuracies = {}

        for k in k_values:
            correct_predictions = 0
            valid_recommendations = 0

            for task_id, recommended_devs in recommendations:
                if task_id in human_test_assignments:
                    valid_recommendations += 1
                    expert_dev = human_test_assignments[task_id]

                    # Top-Kç²¾åº¦ï¼šæ¨è–¦ãƒªã‚¹ãƒˆã®ãƒˆãƒƒãƒ—Kå€‹ã«æ­£è§£ãŒå«ã¾ã‚Œã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
                    top_k_recommendations = recommended_devs[:k]
                    if expert_dev in top_k_recommendations:
                        correct_predictions += 1

            if valid_recommendations > 0:
                accuracy = correct_predictions / valid_recommendations
                accuracies[f"top_{k}_accuracy"] = accuracy
            else:
                accuracies[f"top_{k}_accuracy"] = 0.0

        # è©³ç´°æƒ…å ±ã®è¿½åŠ 
        accuracies["total_valid_recommendations"] = valid_recommendations
        accuracies["total_recommendations"] = len(recommendations)
        accuracies["human_test_set_size"] = len(human_test_assignments)
        accuracies["bot_test_set_size"] = len(self.test_expert_assignments) - len(
            human_test_assignments
        )

        return accuracies

    def analyze_baseline_performance(self):
        """ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³æ‰‹æ³•ã¨ã®æ¯”è¼ƒåˆ†æï¼ˆbotã‚’é™¤å¤–ï¼‰"""
        print("\nğŸ” Baseline Analysis (excluding bots):")

        # é–‹ç™ºè€…ã®æ•°ã‚’å–å¾—ï¼ˆbotã‚’é™¤å¤–ï¼‰
        human_developers = [
            dev
            for dev in self.dev_profiles.keys()
            if not ("bot" in dev.lower() or dev == "stale[bot]")
        ]
        num_developers = len(human_developers)

        # ãƒ©ãƒ³ãƒ€ãƒ ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³
        random_accuracy_1 = 1.0 / num_developers
        random_accuracy_3 = min(3.0 / num_developers, 1.0)
        random_accuracy_5 = min(5.0 / num_developers, 1.0)

        print(f"Random baseline (assuming {num_developers} human developers):")
        print(f"  Top-1: {random_accuracy_1:.3f} ({random_accuracy_1*100:.1f}%)")
        print(f"  Top-3: {random_accuracy_3:.3f} ({random_accuracy_3*100:.1f}%)")
        print(f"  Top-5: {random_accuracy_5:.3f} ({random_accuracy_5*100:.1f}%)")

        # æœ€é »é–‹ç™ºè€…ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ï¼ˆbotã‚’é™¤å¤–ï¼‰
        from collections import Counter

        human_assignments = {
            task_id: dev
            for task_id, dev in self.train_expert_assignments.items()
            if not ("bot" in dev.lower() or dev == "stale[bot]")
        }
        dev_counts = Counter(human_assignments.values())
        most_frequent_devs = [dev for dev, count in dev_counts.most_common(5)]

        print(f"\nMost frequent human developers in training data:")
        total_human_assignments = len(human_assignments)
        for i, (dev, count) in enumerate(dev_counts.most_common(5)):
            print(
                f"  {i+1}. {dev}: {count} assignments ({count/total_human_assignments*100:.1f}%)"
            )

        # æœ€é »é–‹ç™ºè€…ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã®ç²¾åº¦è¨ˆç®—ï¼ˆbotã‚’é™¤å¤–ã—ãŸãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ï¼‰
        human_test_assignments = {
            task_id: dev
            for task_id, dev in self.test_expert_assignments.items()
            if not ("bot" in dev.lower() or dev == "stale[bot]")
        }

        frequent_dev_accuracies = {}
        for k in [1, 3, 5]:
            correct = 0
            for task_id, expert_dev in human_test_assignments.items():
                if expert_dev in most_frequent_devs[:k]:
                    correct += 1
            accuracy = (
                correct / len(human_test_assignments) if human_test_assignments else 0.0
            )
            frequent_dev_accuracies[f"frequent_dev_top_{k}"] = accuracy

        print(f"\nMost frequent developers baseline (human tasks only):")
        for k in [1, 3, 5]:
            acc = frequent_dev_accuracies[f"frequent_dev_top_{k}"]
            print(f"  Top-{k}: {acc:.3f} ({acc*100:.1f}%)")

        print(f"\nHuman task statistics:")
        print(f"  Human training tasks: {total_human_assignments}")
        print(f"  Human test tasks: {len(human_test_assignments)}")
        print(
            f"  Bot training tasks excluded: {len(self.train_expert_assignments) - total_human_assignments}"
        )
        print(
            f"  Bot test tasks excluded: {len(self.test_expert_assignments) - len(human_test_assignments)}"
        )

        return frequent_dev_accuracies

    def evaluate(self):
        """æ¨è–¦ã‚·ã‚¹ãƒ†ãƒ ã®ç·åˆè©•ä¾¡ã‚’å®Ÿè¡Œ"""
        print(
            "ğŸ¯ Starting proper train/test split recommendation accuracy evaluation..."
        )
        print(f"Train ratio: {self.train_ratio:.1%}")

        # ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³åˆ†æ
        baseline_results = self.analyze_baseline_performance()

        # æ¨è–¦ã®ç”Ÿæˆï¼ˆãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®ã¿ï¼‰
        print("\nğŸ“Š Generating recommendations for test data...")
        recommendations = self.get_model_recommendations(max_k=5)

        # ã‚µãƒ³ãƒ—ãƒ«æ¨è–¦ã®è¡¨ç¤ºï¼ˆäººé–“ã®ã‚¿ã‚¹ã‚¯ã®ã¿ï¼‰
        print("\nğŸ“‹ Sample test recommendations (human tasks only):")
        human_test_assignments = {
            task_id: dev
            for task_id, dev in self.test_expert_assignments.items()
            if not ("bot" in dev.lower() or dev == "stale[bot]")
        }

        sample_count = 0
        for i, (task_id, devs) in enumerate(recommendations):
            if task_id in human_test_assignments and sample_count < 5:
                top_3_devs = devs[:3] if len(devs) >= 3 else devs
                expert_dev = human_test_assignments.get(task_id, "Unknown")
                print(
                    f"  {sample_count+1}. Task {task_id} â†’ {top_3_devs} (Expert: {expert_dev})"
                )
                sample_count += 1

        # ç²¾åº¦è¨ˆç®—
        print("\nğŸ¯ Calculating accuracy on test data...")
        accuracies = self.calculate_accuracy(recommendations)

        # çµæœè¡¨ç¤º
        print("\nğŸ“ˆ Test Set Recommendation Accuracy Results (Human Tasks Only):")
        print("=" * 70)

        if accuracies:
            print("Model Performance:")
            for metric, value in accuracies.items():
                if metric.endswith("_accuracy"):
                    print(f"  {metric:20s}: {value:.3f} ({value*100:.1f}%)")

            human_developers = [
                dev
                for dev in self.dev_profiles.keys()
                if not ("bot" in dev.lower() or dev == "stale[bot]")
            ]
            print(f"\nBaseline Comparisons:")
            print(
                f"  Random Top-1       : {1.0/len(human_developers):.3f} ({100.0/len(human_developers):.1f}%)"
            )
            print(
                f"  Frequent Dev Top-1 : {baseline_results.get('frequent_dev_top_1', 0):.3f} ({baseline_results.get('frequent_dev_top_1', 0)*100:.1f}%)"
            )
        else:
            print(
                "âŒ Could not calculate accuracy (no test data or valid recommendations)"
            )

        # è©³ç´°çµ±è¨ˆ
        print(f"\nDetailed Statistics:")
        print(f"  Human test set size: {accuracies.get('human_test_set_size', 0)}")
        print(
            f"  Bot test set size (excluded): {accuracies.get('bot_test_set_size', 0)}"
        )
        print(
            f"  Total test recommendations: {accuracies.get('total_recommendations', len(recommendations))}"
        )
        print(
            f"  Valid human recommendations: {accuracies.get('total_valid_recommendations', 0)}"
        )
        print(
            f"  Human train set size: {len([task for task, dev in self.train_expert_assignments.items() if not ('bot' in dev.lower() or dev == 'stale[bot]')])}"
        )

        return accuracies, baseline_results


@hydra.main(version_base="1.3", config_path="../configs", config_name="base")
def main(cfg: DictConfig):
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""

    print("ğŸš€ Proper Train/Test Split Recommendation Accuracy Evaluation")
    print("=" * 70)

    try:
        evaluator = ProperSplitRecommendationEvaluator(cfg, train_ratio=0.7)
        model_results, baseline_results = evaluator.evaluate()

        print("\nâœ… Evaluation completed successfully!")

        # çµæœã®ä¿å­˜
        results = {
            "model_performance": model_results,
            "baseline_performance": baseline_results,
            "train_ratio": 0.7,
        }

        return results

    except Exception as e:
        print(f"âŒ Evaluation failed: {e}")
        raise


if __name__ == "__main__":
    main()
