#!/usr/bin/env python3
"""
ã‚·ãƒ³ãƒ—ãƒ«çµ±åˆå¼·åŒ–å­¦ç¿’ã‚·ã‚¹ãƒ†ãƒ  - ç°¡å˜ãªGymç’°å¢ƒç‰ˆ
è¤‡é›‘ãªDictè¦³æ¸¬ç©ºé–“ã‚’é¿ã‘ã¦ã€Stable-Baselines3ã§å‹•ä½œã™ã‚‹ç‰ˆ
"""

import json
import os
import sys
from collections import Counter
from pathlib import Path
from typing import Dict, List, Tuple

import gymnasium as gym
import hydra
import numpy as np
import pandas as pd
import torch
import yaml
from omegaconf import DictConfig, OmegaConf
from stable_baselines3 import PPO
from stable_baselines3.common.callbacks import EvalCallback
from stable_baselines3.common.vec_env import DummyVecEnv

# ãƒ‘ã‚¹è¨­å®š
sys.path.append(str(Path(__file__).resolve().parents[1] / "src"))

from kazoo.features.feature_extractor import FeatureExtractor


class SimpleTaskAssignmentEnv(gym.Env):
    """
    ã‚·ãƒ³ãƒ—ãƒ«ãªã‚¿ã‚¹ã‚¯å‰²ã‚Šå½“ã¦ç’°å¢ƒ
    - Dictã§ã¯ãªãå˜ç´”ãªBoxè¦³æ¸¬ç©ºé–“ã‚’ä½¿ç”¨
    - IRLé‡ã¿ãƒ™ãƒ¼ã‚¹ã®å ±é…¬è¨ˆç®—
    """
    
    def __init__(self, cfg, backlog_data, dev_profiles_data):
        super().__init__()
        
        self.cfg = cfg
        self.backlog_data = backlog_data
        self.dev_profiles_data = dev_profiles_data
        
        # ç‰¹å¾´é‡æŠ½å‡ºå™¨
        self.feature_extractor = FeatureExtractor(cfg)
        
        # IRLé‡ã¿ã‚’èª­ã¿è¾¼ã¿
        self.irl_weights = self._load_irl_weights()
        
        # ç’°å¢ƒè¨­å®š
        self.num_developers = min(len(dev_profiles_data), 
                                cfg.optimization.get('max_developers', 50))
        self.max_tasks = min(len(backlog_data), 
                           cfg.optimization.get('max_tasks', 200))
        self.max_steps = cfg.env.get('max_steps', 100)
        
        # è¡Œå‹•ãƒ»è¦³æ¸¬ç©ºé–“
        self.action_space = gym.spaces.Discrete(self.num_developers)
        
        feature_dim = len(self.feature_extractor.feature_names)
        self.observation_space = gym.spaces.Box(
            low=-np.inf, high=np.inf, 
            shape=(feature_dim,), 
            dtype=np.float32
        )
        
        # é–‹ç™ºè€…ãƒ»ã‚¿ã‚¹ã‚¯ãƒªã‚¹ãƒˆã‚’ä½œæˆ
        self.developers = list(dev_profiles_data.keys())[:self.num_developers]
        
        # ã‚¿ã‚¹ã‚¯ã‚’Taskã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã«å¤‰æ›
        from kazoo.envs.task import Task
        self.Task = Task  # ã‚¯ãƒ©ã‚¹å‚ç…§ã‚’ä¿å­˜
        self.tasks = []
        conversion_errors = 0
        
        for i, task_dict in enumerate(backlog_data[:self.max_tasks]):
            try:
                task_obj = Task(task_dict)
                # ãƒ‡ãƒãƒƒã‚°: updated_atãŒæ­£ã—ãè¨­å®šã•ã‚Œã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
                if task_obj.updated_at is None:
                    print(f"âš ï¸ Task {i} has None updated_at: {task_dict.get('updated_at')}")
                self.tasks.append(task_obj)
            except Exception as e:
                print(f"âš ï¸ Failed to create Task object for task {i}: {e}")
                print(f"   Task data keys: {list(task_dict.keys()) if isinstance(task_dict, dict) else 'Not a dict'}")
                conversion_errors += 1
                # ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸå ´åˆã¯ã€ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã§ Task ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ä½œæˆ
                default_task = {
                    'id': task_dict.get('id', i),
                    'number': task_dict.get('number', i),
                    'title': task_dict.get('title', 'Unknown Task'),
                    'body': task_dict.get('body', ''),
                    'state': task_dict.get('state', 'open'),
                    'created_at': task_dict.get('created_at', '2022-01-01T00:00:00Z'),
                    'updated_at': task_dict.get('updated_at', '2022-01-01T00:00:00Z'),
                    'labels': task_dict.get('labels', []),
                    'user': task_dict.get('user', {'login': 'unknown'}),
                    'comments': task_dict.get('comments', 0)
                }
                self.tasks.append(Task(default_task))
        
        if conversion_errors > 0:
            print(f"âš ï¸ Task conversion errors: {conversion_errors}/{self.max_tasks}")
        
        print(f"âœ… All tasks converted to Task objects: {len(self.tasks)}")
        
        # çŠ¶æ…‹ç®¡ç†
        self.current_task_idx = 0
        self.step_count = 0
        self.assignments = {}
        
        print(f"ğŸ® ã‚·ãƒ³ãƒ—ãƒ«ç’°å¢ƒåˆæœŸåŒ–å®Œäº†")
        print(f"   é–‹ç™ºè€…æ•°: {self.num_developers}")
        print(f"   ã‚¿ã‚¹ã‚¯æ•°: {len(self.tasks)}")
        print(f"   ç‰¹å¾´é‡æ¬¡å…ƒ: {feature_dim}")
        print(f"   æœ€å¤§ã‚¹ãƒ†ãƒƒãƒ—æ•°: {self.max_steps}")
        
    def _load_irl_weights(self):
        """IRLå­¦ç¿’æ¸ˆã¿é‡ã¿ã‚’èª­ã¿è¾¼ã¿"""
        weights_path = self.cfg.irl.get('output_weights_path')
        
        if weights_path and Path(weights_path).exists():
            try:
                weights = np.load(weights_path)
                print(f"âœ… IRLé‡ã¿ã‚’èª­ã¿è¾¼ã¿: {weights_path} ({weights.shape})")
                return torch.tensor(weights, dtype=torch.float32)
            except Exception as e:
                print(f"âš ï¸ IRLé‡ã¿èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
        
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ãƒ©ãƒ³ãƒ€ãƒ é‡ã¿
        feature_dim = len(self.feature_extractor.feature_names)
        weights = torch.randn(feature_dim, dtype=torch.float32)
        print(f"âš ï¸ ãƒ©ãƒ³ãƒ€ãƒ é‡ã¿ã‚’ä½¿ç”¨: {weights.shape}")
        return weights
    
    def reset(self, seed=None, options=None):
        """ç’°å¢ƒã‚’ãƒªã‚»ãƒƒãƒˆ"""
        super().reset(seed=seed)
        
        self.current_task_idx = 0
        self.step_count = 0
        self.assignments = {}
        
        # æœ€åˆã®è¦³æ¸¬ã‚’å–å¾—
        obs = self._get_observation()
        return obs, {}
    
    def step(self, action):
        """ã‚¹ãƒ†ãƒƒãƒ—å®Ÿè¡Œ"""
        if (self.current_task_idx >= len(self.tasks) or 
            action >= self.num_developers):
            # çµ‚äº†æ¡ä»¶
            obs = np.zeros(self.observation_space.shape, dtype=np.float32)
            return obs, 0.0, True, False, {}
        
        # ç¾åœ¨ã®ã‚¿ã‚¹ã‚¯ã¨é¸æŠã•ã‚ŒãŸé–‹ç™ºè€…
        current_task = self.tasks[self.current_task_idx]
        
        # ã‚¿ã‚¹ã‚¯ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®å‹ã‚’ãƒã‚§ãƒƒã‚¯
        if not hasattr(current_task, 'updated_at'):
            print(f"âš ï¸ Task {self.current_task_idx} is not a Task object in step: {type(current_task)}")
            # è¾æ›¸ã®å ´åˆã¯ Task ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã«å¤‰æ›
            if isinstance(current_task, dict):
                current_task = self.Task(current_task)
                self.tasks[self.current_task_idx] = current_task
        
        developer_name = self.developers[action]
        developer_profile = self.dev_profiles_data.get(developer_name, {})
        developer_obj = {"name": developer_name, "profile": developer_profile}
        
        # å ±é…¬è¨ˆç®—
        reward = self._calculate_reward(current_task, developer_obj)
        
        # å‰²ã‚Šå½“ã¦ã‚’è¨˜éŒ²
        task_id = current_task.id if hasattr(current_task, 'id') else current_task.get('id', self.current_task_idx)
        self.assignments[task_id] = developer_name
        
        # æ¬¡ã®ã‚¿ã‚¹ã‚¯ã«ç§»è¡Œ
        self.current_task_idx += 1
        self.step_count += 1
        
        # çµ‚äº†æ¡ä»¶ã‚’ãƒã‚§ãƒƒã‚¯
        terminated = (self.current_task_idx >= len(self.tasks) or 
                     self.step_count >= self.max_steps)
        
        # æ¬¡ã®è¦³æ¸¬
        if terminated:
            obs = np.zeros(self.observation_space.shape, dtype=np.float32)
        else:
            obs = self._get_observation()
        
        info = {
            'task_id': task_id,
            'developer': developer_name,
            'step': self.step_count,
            'assignments_count': len(self.assignments)
        }
        
        return obs, reward, terminated, False, info
    
    def _get_observation(self):
        """ç¾åœ¨ã®è¦³æ¸¬ã‚’å–å¾—"""
        if self.current_task_idx >= len(self.tasks):
            return np.zeros(self.observation_space.shape, dtype=np.float32)
        
        current_task = self.tasks[self.current_task_idx]
        
        # ã‚¿ã‚¹ã‚¯ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®å‹ã‚’ãƒã‚§ãƒƒã‚¯
        if not hasattr(current_task, 'updated_at'):
            print(f"âš ï¸ Task {self.current_task_idx} is not a Task object: {type(current_task)}")
            # è¾æ›¸ã®å ´åˆã¯ Task ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã«å¤‰æ›
            if isinstance(current_task, dict):
                current_task = self.Task(current_task)
                self.tasks[self.current_task_idx] = current_task
        
        # æœ€åˆã®é–‹ç™ºè€…ã®ç‰¹å¾´é‡ã‚’ä»£è¡¨ã¨ã—ã¦ä½¿ç”¨
        try:
            developer_name = self.developers[0]
            developer_profile = self.dev_profiles_data.get(developer_name, {})
            developer_obj = {"name": developer_name, "profile": developer_profile}
            
            # ãƒ€ãƒŸãƒ¼ç’°å¢ƒã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ä½œæˆ
            dummy_env = type('DummyEnv', (), {
                'backlog': [t if hasattr(t, 'updated_at') else self.Task(t) for t in self.backlog_data[:self.max_tasks]],
                'dev_profiles': self.dev_profiles_data,
                'assignments': {},
                'dev_action_history': {}  # é–‹ç™ºè€…ã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³å±¥æ­´
            })()
            
            features = self.feature_extractor.get_features(
                current_task, developer_obj, dummy_env
            )
            
            return features.astype(np.float32)
            
        except Exception as e:
            print(f"âš ï¸ è¦³æ¸¬å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
            print(f"   Task type: {type(current_task)}")
            if hasattr(current_task, 'updated_at'):
                print(f"   Task updated_at: {current_task.updated_at}")
            return np.zeros(self.observation_space.shape, dtype=np.float32)
    
    def _calculate_reward(self, task, developer):
        """IRLé‡ã¿ã‚’ä½¿ç”¨ã—ãŸå ±é…¬è¨ˆç®—"""
        try:
            # ã‚¿ã‚¹ã‚¯ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®å‹ã‚’ãƒã‚§ãƒƒã‚¯
            if not hasattr(task, 'updated_at'):
                print(f"âš ï¸ Task is not a Task object in reward calculation: {type(task)}")
                # è¾æ›¸ã®å ´åˆã¯ Task ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã«å¤‰æ›
                if isinstance(task, dict):
                    task = self.Task(task)
            
            # ãƒ€ãƒŸãƒ¼ç’°å¢ƒã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ä½œæˆ
            dummy_env = type('DummyEnv', (), {
                'backlog': [t if hasattr(t, 'updated_at') else self.Task(t) for t in self.backlog_data[:self.max_tasks]],
                'dev_profiles': self.dev_profiles_data,
                'assignments': {},
                'dev_action_history': {}  # é–‹ç™ºè€…ã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³å±¥æ­´
            })()
            
            # ç‰¹å¾´é‡ã‚’æŠ½å‡º
            features = self.feature_extractor.get_features(task, developer, dummy_env)
            features_tensor = torch.tensor(features, dtype=torch.float32)
            
            # IRLé‡ã¿ã¨ã®å†…ç©ã§å ±é…¬ã‚’è¨ˆç®—
            reward = torch.dot(self.irl_weights, features_tensor).item()
            
            # å ±é…¬ã‚’æ­£è¦åŒ–
            reward = np.clip(reward, -10.0, 10.0)
            
            return reward
            
        except Exception as e:
            print(f"âš ï¸ å ±é…¬è¨ˆç®—ã‚¨ãƒ©ãƒ¼: {e}")
            print(f"   Task type: {type(task)}")
            if hasattr(task, 'updated_at'):
                print(f"   Task updated_at: {task.updated_at}")
            return 0.0


@hydra.main(config_path="../configs", config_name="unified_rl", version_base=None)
def main(cfg: DictConfig):
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    
    print("ğŸš€ ã‚·ãƒ³ãƒ—ãƒ«çµ±åˆå¼·åŒ–å­¦ç¿’ã‚·ã‚¹ãƒ†ãƒ é–‹å§‹")
    print("=" * 60)
    
    # 1. ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    print("1. ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿...")
    with open(cfg.env.backlog_path, "r", encoding="utf-8") as f:
        backlog_data = json.load(f)
    with open(cfg.env.dev_profiles_path, "r", encoding="utf-8") as f:
        dev_profiles_data = yaml.safe_load(f)
    
    print(f"   ãƒãƒƒã‚¯ãƒ­ã‚°: {len(backlog_data)} ã‚¿ã‚¹ã‚¯")
    print(f"   é–‹ç™ºè€…: {len(dev_profiles_data)} äºº")
    
    # 2. ç’°å¢ƒã®ä½œæˆ
    print("2. ç’°å¢ƒåˆæœŸåŒ–...")
    def make_env():
        return SimpleTaskAssignmentEnv(cfg, backlog_data, dev_profiles_data)
    
    env = DummyVecEnv([make_env])
    eval_env = DummyVecEnv([make_env])
    
    # 3. PPOãƒ¢ãƒ‡ãƒ«ä½œæˆ
    print("3. PPOãƒ¢ãƒ‡ãƒ«ä½œæˆ...")
    
    # å­¦ç¿’ç‡ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒªãƒ³ã‚°é–¢æ•°
    def linear_schedule(initial_value: float):
        """
        Linear learning rate schedule.
        åˆæœŸå€¤ã‹ã‚‰æœ€çµ‚çš„ã«åˆæœŸå€¤ã®10%ã¾ã§ç·šå½¢æ¸›è¡°
        """
        def func(progress_remaining: float) -> float:
            # progress_remaining ã¯ 1.0 (é–‹å§‹) ã‹ã‚‰ 0.0 (çµ‚äº†) ã¸
            return progress_remaining * initial_value + (1 - progress_remaining) * 0.1 * initial_value
        return func
    
    # å­¦ç¿’ç‡ã‚’å‹•çš„ã«è¨­å®š
    learning_rate = cfg.rl.get('learning_rate', 3e-4)
    use_lr_schedule = cfg.rl.get('use_lr_schedule', True)
    
    model = PPO(
        "MlpPolicy",
        env,
        verbose=1,
        learning_rate=linear_schedule(learning_rate) if use_lr_schedule else learning_rate,
        n_steps=cfg.rl.get('n_steps', 2048),
        batch_size=cfg.rl.get('batch_size', 64),
        n_epochs=cfg.rl.get('n_epochs', 10),
        gamma=cfg.rl.get('gamma', 0.99),
        gae_lambda=cfg.rl.get('gae_lambda', 0.95),
        clip_range=cfg.rl.get('clip_range', 0.2),
        clip_range_vf=cfg.rl.get('clip_range_vf', None),
        ent_coef=cfg.rl.get('ent_coef', 0.0),
        vf_coef=cfg.rl.get('vf_coef', 0.5),
        max_grad_norm=cfg.rl.get('max_grad_norm', 0.5),
        device="auto",
        seed=cfg.rl.get('seed', None)
    )
    
    # 4. è¨“ç·´å®Ÿè¡Œ
    total_timesteps = cfg.rl.get('total_timesteps', 500000)
    print("4. è¨“ç·´é–‹å§‹...")
    print(f"   ç·ã‚¹ãƒ†ãƒƒãƒ—æ•°: {total_timesteps:,}")
    print(f"   æ¨å®šå®Ÿè¡Œæ™‚é–“: {total_timesteps / 1000:.1f}åˆ†")
    
    # è©•ä¾¡ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯
    eval_callback = EvalCallback(
        eval_env,
        best_model_save_path="./models/simple_unified_best/",
        log_path="./logs/simple_unified_eval/",
        eval_freq=cfg.rl.get('eval_freq', 5000),
        deterministic=True,
        render=False,
        verbose=1
    )
    
    # è¨“ç·´
    model.learn(
        total_timesteps=total_timesteps,
        callback=eval_callback,
        progress_bar=False  # ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼ã‚’ç„¡åŠ¹åŒ–
    )
    
    # 5. ãƒ¢ãƒ‡ãƒ«ä¿å­˜
    print("5. ãƒ¢ãƒ‡ãƒ«ä¿å­˜...")
    
    # ãƒ¡ã‚¤ãƒ³ãƒ¢ãƒ‡ãƒ«ä¿å­˜
    model_path = "models/simple_unified_rl_agent.zip"
    model.save(model_path)
    print(f"âœ… ãƒ¢ãƒ‡ãƒ«ä¿å­˜: {model_path}")
    
    # è¨“ç·´çµ±è¨ˆä¿å­˜
    timestamp = pd.Timestamp.now().strftime("%Y%m%d_%H%M%S")
    training_info = {
        'timestamp': timestamp,
        'total_timesteps': total_timesteps,
        'final_mean_reward': 'TBD',  # è©•ä¾¡ã§æ›´æ–°
        'config': dict(cfg.rl)
    }
    
    # JSONå½¢å¼ã§è¨“ç·´æƒ…å ±ã‚’ä¿å­˜
    info_path = f"models/training_info_{timestamp}.json"
    with open(info_path, 'w') as f:
        json.dump(training_info, f, indent=2, default=str)
    print(f"âœ… è¨“ç·´æƒ…å ±ä¿å­˜: {info_path}")
    
    # 6. è©•ä¾¡
    print("6. è©•ä¾¡å®Ÿè¡Œ...")
    evaluate_simple_model(model, make_env(), cfg)
    
    print("âœ… ã‚·ãƒ³ãƒ—ãƒ«çµ±åˆå¼·åŒ–å­¦ç¿’ã‚·ã‚¹ãƒ†ãƒ å®Œäº†")


def evaluate_simple_model(model, env, cfg):
    """ã‚·ãƒ³ãƒ—ãƒ«ãƒ¢ãƒ‡ãƒ«ã®è©•ä¾¡ - æ¨è–¦ã‚·ã‚¹ãƒ†ãƒ ã¨ã—ã¦ã®æ€§èƒ½è©•ä¾¡ã‚‚å«ã‚€"""
    print("ğŸ“Š æ€§èƒ½è©•ä¾¡...")
    
    num_episodes = cfg.rl.get('eval_episodes', 10)
    rewards = []
    assignment_counts = []
    
    # æ¨è–¦æ€§èƒ½è©•ä¾¡ã®ãŸã‚
    all_predictions = {}
    all_confidences = []
    developer_distribution = Counter()
    
    for episode in range(num_episodes):
        obs, _ = env.reset()
        episode_reward = 0
        assignments = 0
        episode_predictions = {}
        
        while True:
            action, _ = model.predict(obs, deterministic=True)
            
            # äºˆæ¸¬ç¢ºç‡åˆ†å¸ƒã‚‚å–å¾—
            obs_tensor = torch.tensor(obs).unsqueeze(0).float()
            with torch.no_grad():
                # ãƒ¢ãƒ‡ãƒ«ã®æ”¿ç­–ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‹ã‚‰ç¢ºç‡åˆ†å¸ƒã‚’å–å¾—
                features = model.policy.features_extractor(obs_tensor)
                logits = model.policy.mlp_extractor.policy_net(features)
                probs = torch.softmax(logits, dim=-1).numpy()[0]
            
            # ç¾åœ¨ã®ã‚¿ã‚¹ã‚¯æƒ…å ±ã‚’è¨˜éŒ²
            if env.current_task_idx < len(env.tasks):
                task = env.tasks[env.current_task_idx]
                task_id = task.id if hasattr(task, 'id') else task.get('id', env.current_task_idx)
                predicted_dev = env.developers[action] if action < len(env.developers) else 'unknown'
                
                episode_predictions[task_id] = {
                    'developer': predicted_dev,
                    'confidence': float(probs[action]),
                    'top_3_probs': sorted(probs, reverse=True)[:3],
                    'episode': episode + 1
                }
                
                developer_distribution[predicted_dev] += 1
                all_confidences.append(probs[action])
            
            obs, reward, terminated, truncated, info = env.step(action)
            episode_reward += reward
            assignments += 1
            
            if terminated or truncated:
                break
        
        rewards.append(episode_reward)
        assignment_counts.append(assignments)
        all_predictions.update(episode_predictions)
        
        print(f"   Episode {episode + 1}: å ±é…¬={episode_reward:.4f}, å‰²ã‚Šå½“ã¦æ•°={assignments}")
    
    # åŸºæœ¬çµ±è¨ˆè¨ˆç®—
    avg_reward = np.mean(rewards)
    std_reward = np.std(rewards)
    avg_assignments = np.mean(assignment_counts)
    
    # æ¨è–¦ã‚·ã‚¹ãƒ†ãƒ è©•ä¾¡æŒ‡æ¨™
    recommendation_metrics = calculate_recommendation_metrics(
        all_predictions, all_confidences, developer_distribution, env.developers
    )
    
    print(f"\nğŸ¯ åŸºæœ¬è©•ä¾¡çµæœ:")
    print(f"   å¹³å‡å ±é…¬: {avg_reward:.4f} Â± {std_reward:.4f}")
    print(f"   å¹³å‡å‰²ã‚Šå½“ã¦æ•°: {avg_assignments:.1f}")
    print(f"   æœ€å¤§å ±é…¬: {max(rewards):.4f}")
    print(f"   æœ€å°å ±é…¬: {min(rewards):.4f}")
    
    print(f"\nğŸ¯ æ¨è–¦ã‚·ã‚¹ãƒ†ãƒ è©•ä¾¡:")
    for metric, value in recommendation_metrics.items():
        if isinstance(value, float):
            print(f"   {metric}: {value:.4f}")
        else:
            print(f"   {metric}: {value}")
    
    # çµæœä¿å­˜
    timestamp = pd.Timestamp.now().strftime("%Y%m%d_%H%M%S")
    
    # åŸºæœ¬è©•ä¾¡çµæœ
    results_df = pd.DataFrame({
        'episode': range(1, num_episodes + 1),
        'reward': rewards,
        'assignments': assignment_counts
    })
    
    csv_path = f"outputs/simple_unified_evaluation_{timestamp}.csv"
    results_df.to_csv(csv_path, index=False)
    print(f"âœ… è©•ä¾¡çµæœä¿å­˜: {csv_path}")
    
    # æ¨è–¦è©³ç´°ãƒ‡ãƒ¼ã‚¿ä¿å­˜
    predictions_df = pd.DataFrame([
        {
            'task_id': task_id,
            'predicted_developer': pred['developer'],
            'confidence': pred['confidence'],
            'top_3_avg_prob': np.mean(pred['top_3_probs']),
            'episode': pred['episode']
        }
        for task_id, pred in all_predictions.items()
    ])
    
    predictions_path = f"outputs/simple_unified_predictions_{timestamp}.csv"
    predictions_df.to_csv(predictions_path, index=False)
    print(f"âœ… äºˆæ¸¬è©³ç´°ä¿å­˜: {predictions_path}")
    
    # çµ±åˆã‚µãƒãƒªãƒ¼ä¿å­˜
    all_metrics = {
        'avg_reward': avg_reward,
        'std_reward': std_reward, 
        'avg_assignments': avg_assignments,
        'max_reward': max(rewards),
        'min_reward': min(rewards),
        **recommendation_metrics
    }
    
    summary_df = pd.DataFrame([
        {'metric': k, 'value': v} for k, v in all_metrics.items()
    ])
    
    summary_path = f"outputs/simple_unified_summary_{timestamp}.csv"
    summary_df.to_csv(summary_path, index=False)
    print(f"âœ… çµ±åˆã‚µãƒãƒªãƒ¼ä¿å­˜: {summary_path}")


def calculate_recommendation_metrics(predictions, confidences, dev_distribution, available_developers):
    """æ¨è–¦ã‚·ã‚¹ãƒ†ãƒ ã¨ã—ã¦ã®è©•ä¾¡æŒ‡æ¨™ã‚’è¨ˆç®—"""
    metrics = {}
    
    if not predictions:
        return metrics
    
    # 1. äºˆæ¸¬ä¿¡é ¼åº¦ã®çµ±è¨ˆ
    if confidences:
        metrics['avg_confidence'] = np.mean(confidences)
        metrics['confidence_std'] = np.std(confidences)
        metrics['min_confidence'] = np.min(confidences)
        metrics['max_confidence'] = np.max(confidences)
    
    # 2. é–‹ç™ºè€…æ¨è–¦ã®å¤šæ§˜æ€§
    total_predictions = len(predictions)
    unique_developers = len(dev_distribution)
    
    metrics['unique_recommended_developers'] = unique_developers
    metrics['total_available_developers'] = len(available_developers)
    metrics['recommendation_coverage'] = unique_developers / len(available_developers) if available_developers else 0
    
    # 3. æ¨è–¦é›†ä¸­åº¦ï¼ˆä¸Šä½é–‹ç™ºè€…ã¸ã®é›†ä¸­åº¦ï¼‰
    if dev_distribution:
        max_assignments = max(dev_distribution.values())
        metrics['max_assignments_ratio'] = max_assignments / total_predictions
        
        # ã‚¸ãƒ‹ä¿‚æ•°ï¼ˆæ¨è–¦ã®åã‚Šï¼‰
        counts = list(dev_distribution.values())
        counts.sort()
        n = len(counts)
        if n > 1:
            gini = (2 * sum((i + 1) * x for i, x in enumerate(counts))) / (n * sum(counts)) - (n + 1) / n
            metrics['recommendation_gini'] = gini
            metrics['recommendation_diversity'] = 1 - gini
    
    # 4. é«˜ä¿¡é ¼åº¦äºˆæ¸¬ã®å‰²åˆ
    if confidences:
        high_confidence_threshold = 0.7
        medium_confidence_threshold = 0.5
        
        high_confidence_count = sum(1 for c in confidences if c >= high_confidence_threshold)
        medium_confidence_count = sum(1 for c in confidences if c >= medium_confidence_threshold)
        
        metrics['high_confidence_ratio'] = high_confidence_count / len(confidences)
        metrics['medium_confidence_ratio'] = medium_confidence_count / len(confidences)
    
    return metrics


if __name__ == "__main__":
    main()
