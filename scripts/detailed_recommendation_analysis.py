import sys
from pathlib import Path

sys.path.append(str(Path(__file__).resolve().parents[1] / "src"))

import json
from collections import Counter, defaultdict

import torch
import torch.nn.functional as F

from kazoo.gnn.gnn_model import GNNModel


def analyze_recommendations_with_metadata():
    """æ¨è–¦çµæœã¨ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’çµ„ã¿åˆã‚ã›ãŸè©³ç´°åˆ†æ"""

    # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    root = Path(__file__).resolve().parents[1]
    graph_path = root / "data/graph.pt"
    model_path = root / "data/gnn_model.pt"

    data = torch.load(graph_path, weights_only=False)
    model = GNNModel(in_channels_dict={"dev": 8, "task": 9}, out_channels=32)
    model.load_state_dict(torch.load(model_path, weights_only=True))
    model.eval()

    # åŸ‹ã‚è¾¼ã¿ç”Ÿæˆ
    with torch.no_grad():
        embeddings = model(data.x_dict, data.edge_index_dict)

    # ãƒãƒ¼ãƒ‰IDã‚’å–å¾—
    if hasattr(data["dev"], "node_id") and isinstance(data["dev"].node_id, list):
        dev_ids = data["dev"].node_id
    else:
        dev_ids = list(range(embeddings["dev"].size(0)))

    if hasattr(data["task"], "node_id") and isinstance(data["task"].node_id, list):
        task_ids = data["task"].node_id
    else:
        task_ids = list(range(embeddings["task"].size(0)))

    print("ğŸ” æ¨è–¦ã‚·ã‚¹ãƒ†ãƒ ã®è©³ç´°åˆ†æ")
    print(f"é–‹ç™ºè€…æ•°: {len(dev_ids)}, ã‚¿ã‚¹ã‚¯æ•°: {len(task_ids)}")

    # 1. é–‹ç™ºè€…IDåˆ¥ã®æ¨è–¦ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ
    print("\nğŸ“‹ é–‹ç™ºè€…IDåˆ¥æ¨è–¦ãƒ‘ã‚¿ãƒ¼ãƒ³")

    recommendation_patterns = defaultdict(list)
    for dev_idx in range(min(20, embeddings["dev"].size(0))):
        dev_vec = embeddings["dev"][dev_idx : dev_idx + 1]
        similarities = F.cosine_similarity(dev_vec, embeddings["task"])
        top_5 = torch.topk(similarities, k=5)

        dev_id = dev_ids[dev_idx] if dev_idx < len(dev_ids) else f"dev_{dev_idx}"
        pattern = []

        for sim, task_idx in zip(top_5.values, top_5.indices):
            task_id = (
                task_ids[task_idx] if task_idx < len(task_ids) else f"task_{task_idx}"
            )
            pattern.append((task_id, sim.item()))

        recommendation_patterns[str(dev_id)] = pattern

    # ãƒ‘ã‚¿ãƒ¼ãƒ³ã®é¡ä¼¼æ€§ã‚’åˆ†æ
    print("æ¨è–¦ãƒ‘ã‚¿ãƒ¼ãƒ³ã®å¤šæ§˜æ€§:")
    unique_patterns = {}
    for dev_id, pattern in recommendation_patterns.items():
        # ã‚¿ã‚¹ã‚¯IDã ã‘ã‚’å–å¾—ã—ã¦ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ä½œæˆ
        task_pattern = tuple([task_id for task_id, _ in pattern])
        if task_pattern not in unique_patterns:
            unique_patterns[task_pattern] = []
        unique_patterns[task_pattern].append(dev_id)

    print(f"  ãƒ¦ãƒ‹ãƒ¼ã‚¯ãªãƒ‘ã‚¿ãƒ¼ãƒ³æ•°: {len(unique_patterns)}")
    for i, (pattern, devs) in enumerate(unique_patterns.items()):
        if i < 5:  # ä¸Šä½5ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’è¡¨ç¤º
            print(f"  ãƒ‘ã‚¿ãƒ¼ãƒ³{i+1}: {len(devs)}åã®é–‹ç™ºè€…")
            print(f"    ã‚¿ã‚¹ã‚¯: {pattern[:3]}...")  # æœ€åˆã®3ã‚¿ã‚¹ã‚¯ã‚’è¡¨ç¤º
            print(f"    é–‹ç™ºè€…: {devs[:5]}")  # æœ€åˆã®5åã‚’è¡¨ç¤º

    # 2. ã‚¿ã‚¹ã‚¯IDåˆ†æ
    print("\nğŸ¯ æ¨è–¦ã•ã‚Œã‚„ã™ã„ã‚¿ã‚¹ã‚¯ã®åˆ†æ")

    # å…¨é–‹ç™ºè€…ã«å¯¾ã™ã‚‹å„ã‚¿ã‚¹ã‚¯ã®æ¨è–¦é »åº¦ã‚’è¨ˆç®—
    task_recommendation_count = Counter()

    for dev_idx in range(
        min(50, embeddings["dev"].size(0))
    ):  # 50åã®é–‹ç™ºè€…ã§ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
        dev_vec = embeddings["dev"][dev_idx : dev_idx + 1]
        similarities = F.cosine_similarity(dev_vec, embeddings["task"])
        top_10 = torch.topk(similarities, k=10).indices  # ãƒˆãƒƒãƒ—10ã‚’è€ƒæ…®

        for task_idx in top_10:
            task_id = (
                task_ids[task_idx] if task_idx < len(task_ids) else f"task_{task_idx}"
            )
            task_recommendation_count[task_id] += 1

    print("æœ€ã‚‚æ¨è–¦ã•ã‚Œã‚„ã™ã„ã‚¿ã‚¹ã‚¯ï¼ˆãƒˆãƒƒãƒ—10ï¼‰:")
    for i, (task_id, count) in enumerate(task_recommendation_count.most_common(10)):
        print(f"  {i+1}. {task_id}: {count}å›æ¨è–¦")

    # 3. é¡ä¼¼åº¦å€¤ã®åˆ†å¸ƒè©³ç´°åˆ†æ
    print("\nğŸ“Š é¡ä¼¼åº¦å€¤ã®è©³ç´°åˆ†å¸ƒ")

    all_similarities = []
    for dev_idx in range(min(20, embeddings["dev"].size(0))):
        dev_vec = embeddings["dev"][dev_idx : dev_idx + 1]
        similarities = F.cosine_similarity(dev_vec, embeddings["task"])
        all_similarities.extend(similarities.tolist())

    import numpy as np

    all_similarities = np.array(all_similarities)

    print(f"é¡ä¼¼åº¦çµ±è¨ˆ:")
    print(f"  ã‚µãƒ³ãƒ—ãƒ«æ•°: {len(all_similarities)}")
    print(f"  å¹³å‡å€¤: {np.mean(all_similarities):.4f}")
    print(f"  ä¸­å¤®å€¤: {np.median(all_similarities):.4f}")
    print(f"  æ¨™æº–åå·®: {np.std(all_similarities):.4f}")
    print(f"  æœ€å°å€¤: {np.min(all_similarities):.4f}")
    print(f"  æœ€å¤§å€¤: {np.max(all_similarities):.4f}")

    # ãƒ‘ãƒ¼ã‚»ãƒ³ã‚¿ã‚¤ãƒ«åˆ†æ
    percentiles = [10, 25, 50, 75, 90, 95, 99]
    print("ãƒ‘ãƒ¼ã‚»ãƒ³ã‚¿ã‚¤ãƒ«åˆ†æ:")
    for p in percentiles:
        value = np.percentile(all_similarities, p)
        print(f"  {p}%tile: {value:.4f}")

    # 4. å®Ÿéš›ã®ã‚¨ãƒƒã‚¸é–¢ä¿‚ã¨ã®æ¯”è¼ƒ
    print("\nğŸ”— å®Ÿéš›ã®ã‚¨ãƒƒã‚¸é–¢ä¿‚ã¨ã®æ¯”è¼ƒ")

    # å®Ÿéš›ã®ã‚¨ãƒƒã‚¸ã‚’å–å¾—
    writes_edges = data[("dev", "writes", "task")].edge_index
    actual_relationships = set()

    for i in range(writes_edges.size(1)):
        dev_idx = writes_edges[0, i].item()
        task_idx = writes_edges[1, i].item()
        if dev_idx < len(dev_ids) and task_idx < len(task_ids):
            dev_id = dev_ids[dev_idx]
            task_id = task_ids[task_idx]
            actual_relationships.add((dev_id, task_id))

    print(f"å®Ÿéš›ã®é–¢ä¿‚æ•°: {len(actual_relationships)}")

    # æ¨è–¦çµæœã¨å®Ÿéš›ã®é–¢ä¿‚ã®ä¸€è‡´åº¦ã‚’ç¢ºèª
    match_count = 0
    total_recommendations = 0

    for dev_id, recommendations in list(recommendation_patterns.items())[:10]:
        for task_id, _ in recommendations:
            total_recommendations += 1
            if (dev_id, task_id) in actual_relationships:
                match_count += 1

    if total_recommendations > 0:
        match_rate = match_count / total_recommendations
        print(
            f"æ¨è–¦ã¨å®Ÿéš›ã®é–¢ä¿‚ã®ä¸€è‡´ç‡: {match_rate:.2%} ({match_count}/{total_recommendations})"
        )

    return recommendation_patterns, task_recommendation_count


if __name__ == "__main__":
    analyze_recommendations_with_metadata()
