#!/usr/bin/env python3

"""
ã‚ˆã‚Šæœ€è¿‘ã®ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ã—ã¦GNNã‚’å†è¨“ç·´ã™ã‚‹ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
- 2020-2022å¹´ã®GitHubã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨
- ç¾åœ¨ã®ãƒãƒƒã‚¯ãƒ­ã‚°ã¨é–‹ç™ºè€…ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ã«åŸºã¥ã„ã¦ã‚°ãƒ©ãƒ•ã‚’ç”Ÿæˆ
- GNNãƒ¢ãƒ‡ãƒ«ã‚’å†è¨“ç·´
"""

import json
import sys
from datetime import datetime
from pathlib import Path

import numpy as np
import pandas as pd
import yaml

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’Pythonãƒ‘ã‚¹ã«è¿½åŠ 
project_root = Path(__file__).resolve().parents[1]
sys.path.insert(0, str(project_root / "src"))


def load_recent_github_data():
    """æœ€è¿‘ã®GitHubã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿"""
    print("ğŸ“š æœ€è¿‘ã®GitHubãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ä¸­...")

    data_files = [
        "data/gharchive_docker_compose_events_2020-08.jsonl",
        "data/gharchive_docker_compose_events_2021-01.jsonl",
        "data/gharchive_docker_compose_events_2021-10.jsonl",
        "data/gharchive_docker_compose_events_2022-02.jsonl",
    ]

    all_events = []
    for file_path in data_files:
        if Path(file_path).exists():
            print(f"  ğŸ“„ èª­ã¿è¾¼ã¿: {file_path}")
            with open(file_path, "r") as f:
                count = 0
                for line in f:
                    try:
                        event = json.loads(line.strip())
                        all_events.append(event)
                        count += 1
                    except json.JSONDecodeError:
                        continue
                print(f"    ã‚¤ãƒ™ãƒ³ãƒˆæ•°: {count}")
        else:
            print(f"  âš ï¸  ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {file_path}")

    print(f"ğŸ“Š ç·ã‚¤ãƒ™ãƒ³ãƒˆæ•°: {len(all_events)}")
    return all_events


def extract_developer_task_interactions(events):
    """é–‹ç™ºè€…ã¨ã‚¿ã‚¹ã‚¯ã®ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ã‚·ãƒ§ãƒ³ã‚’æŠ½å‡º"""
    print("ğŸ” é–‹ç™ºè€…-ã‚¿ã‚¹ã‚¯ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ã‚·ãƒ§ãƒ³ã‚’æŠ½å‡ºä¸­...")

    interactions = []
    task_info = {}
    dev_info = {}

    for event in events:
        # ã‚¤ãƒ™ãƒ³ãƒˆã‚¿ã‚¤ãƒ—ã‚’ç¢ºèª
        event_type = event.get("type", "")

        if event_type in ["PullRequestEvent", "IssuesEvent", "PullRequestReviewEvent"]:
            # é–‹ç™ºè€…æƒ…å ±ã‚’å–å¾—
            actor = event.get("actor", {})
            dev_name = actor.get("login", "")

            # ãƒœãƒƒãƒˆé™¤å¤–
            if "bot" in dev_name.lower() or not dev_name:
                continue

            # ã‚¿ã‚¹ã‚¯/ãƒ—ãƒ«ãƒªã‚¯æƒ…å ±ã‚’å–å¾—
            payload = event.get("payload", {})

            if event_type == "PullRequestEvent":
                pr = payload.get("pull_request", {})
                task_id = f"pr_{pr.get('id', '')}"
                task_title = pr.get("title", "")
                task_body = pr.get("body", "") or ""
                task_labels = [label.get("name", "") for label in pr.get("labels", [])]

            elif event_type == "IssuesEvent":
                issue = payload.get("issue", {})
                task_id = f"issue_{issue.get('id', '')}"
                task_title = issue.get("title", "")
                task_body = issue.get("body", "") or ""
                task_labels = [
                    label.get("name", "") for label in issue.get("labels", [])
                ]

            elif event_type == "PullRequestReviewEvent":
                pr = payload.get("pull_request", {})
                task_id = f"pr_{pr.get('id', '')}"
                task_title = pr.get("title", "")
                task_body = pr.get("body", "") or ""
                task_labels = [label.get("name", "") for label in pr.get("labels", [])]

            if not task_id or not dev_name:
                continue

            # ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ã‚·ãƒ§ãƒ³è¨˜éŒ²
            action = payload.get("action", "")
            created_at = event.get("created_at", "")

            interactions.append(
                {
                    "dev_name": dev_name,
                    "task_id": task_id,
                    "event_type": event_type,
                    "action": action,
                    "created_at": created_at,
                }
            )

            # ã‚¿ã‚¹ã‚¯æƒ…å ±ã‚’è“„ç©
            if task_id not in task_info:
                task_info[task_id] = {
                    "title": task_title,
                    "body": task_body,
                    "labels": task_labels,
                    "first_seen": created_at,
                }

            # é–‹ç™ºè€…æƒ…å ±ã‚’è“„ç©
            if dev_name not in dev_info:
                dev_info[dev_name] = {
                    "interactions": 0,
                    "tasks": set(),
                    "labels": set(),
                }

            dev_info[dev_name]["interactions"] += 1
            dev_info[dev_name]["tasks"].add(task_id)
            dev_info[dev_name]["labels"].update(task_labels)

    print(f"ğŸ“Š æŠ½å‡ºçµæœ:")
    print(f"  ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ã‚·ãƒ§ãƒ³æ•°: {len(interactions)}")
    print(f"  ãƒ¦ãƒ‹ãƒ¼ã‚¯ã‚¿ã‚¹ã‚¯æ•°: {len(task_info)}")
    print(f"  ãƒ¦ãƒ‹ãƒ¼ã‚¯é–‹ç™ºè€…æ•°: {len(dev_info)}")

    return interactions, task_info, dev_info


def create_modern_graph_data(interactions, task_info, dev_info):
    """æœ€æ–°ãƒ‡ãƒ¼ã‚¿ã«åŸºã¥ã„ã¦ã‚°ãƒ©ãƒ•ãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆ"""
    print("ğŸ•¸ï¸  æœ€æ–°ã‚°ãƒ©ãƒ•ãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆä¸­...")

    # ç¾åœ¨ã®é–‹ç™ºè€…ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿
    with open("configs/dev_profiles.yaml", "r") as f:
        existing_profiles = yaml.safe_load(f)

    # ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°: ååˆ†ãªã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ã‚·ãƒ§ãƒ³ãŒã‚ã‚‹é–‹ç™ºè€…ãƒ»ã‚¿ã‚¹ã‚¯ã®ã¿
    min_interactions = 2
    active_devs = {
        name: info
        for name, info in dev_info.items()
        if info["interactions"] >= min_interactions
    }

    min_dev_interactions = 1
    active_tasks = {
        tid: info
        for tid, info in task_info.items()
        if sum(1 for i in interactions if i["task_id"] == tid) >= min_dev_interactions
    }

    print(f"  ã‚¢ã‚¯ãƒ†ã‚£ãƒ–é–‹ç™ºè€…: {len(active_devs)}")
    print(f"  ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ã‚¿ã‚¹ã‚¯: {len(active_tasks)}")

    # ã‚°ãƒ©ãƒ•ãƒãƒ¼ãƒ‰ã¨ã‚¨ãƒƒã‚¸ã‚’ä½œæˆ
    import torch
    from torch_geometric.data import HeteroData

    # é–‹ç™ºè€…ãƒãƒ¼ãƒ‰ç‰¹å¾´é‡ã‚’ä½œæˆ
    dev_features = []
    dev_node_ids = []

    for dev_name in active_devs.keys():
        dev_info_item = active_devs[dev_name]

        # æ—¢å­˜ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚ã‚Œã°ä½¿ç”¨ã€ãªã‘ã‚Œã°ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
        profile = existing_profiles.get(
            dev_name, {"skills": ["general"], "touched_files": [], "label_affinity": {}}
        )

        # ç‰¹å¾´é‡è¨ˆç®—
        feature_vector = [
            len(profile.get("skills", [])),  # ã‚¹ã‚­ãƒ«æ•°
            len(profile.get("touched_files", [])),  # çµŒé¨“ãƒ•ã‚¡ã‚¤ãƒ«æ•°
            dev_info_item["interactions"],  # ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ã‚·ãƒ§ãƒ³æ•°
            len(dev_info_item["tasks"]),  # å‚åŠ ã‚¿ã‚¹ã‚¯æ•°
            len(dev_info_item["labels"]),  # æ‰±ã£ãŸãƒ©ãƒ™ãƒ«æ•°
            profile.get("label_affinity", {}).get("bug", 0.0),  # ãƒã‚°è¦ªå’Œæ€§
            profile.get("label_affinity", {}).get("enhancement", 0.0),  # æ©Ÿèƒ½å¼·åŒ–è¦ªå’Œæ€§
            profile.get("label_affinity", {}).get(
                "documentation", 0.0
            ),  # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆè¦ªå’Œæ€§
        ]

        dev_features.append(feature_vector)
        dev_node_ids.append(dev_name)

    # ã‚¿ã‚¹ã‚¯ãƒãƒ¼ãƒ‰ç‰¹å¾´é‡ã‚’ä½œæˆ
    task_features = []
    task_node_ids = []

    for task_id in active_tasks.keys():
        task_info_item = active_tasks[task_id]

        # ç‰¹å¾´é‡è¨ˆç®—
        labels = task_info_item["labels"]
        feature_vector = [
            len(task_info_item["title"]),  # ã‚¿ã‚¤ãƒˆãƒ«é•·
            len(task_info_item["body"]),  # æœ¬æ–‡é•·
            len(labels),  # ãƒ©ãƒ™ãƒ«æ•°
            1 if "bug" in labels else 0,  # ãƒã‚°ãƒ•ãƒ©ã‚°
            1 if "enhancement" in labels else 0,  # æ©Ÿèƒ½å¼·åŒ–ãƒ•ãƒ©ã‚°
            1 if "documentation" in labels else 0,  # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãƒ•ãƒ©ã‚°
            sum(
                1 for i in interactions if i["task_id"] == task_id
            ),  # ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ã‚·ãƒ§ãƒ³æ•°
            len(
                set(i["dev_name"] for i in interactions if i["task_id"] == task_id)
            ),  # é–¢ä¸é–‹ç™ºè€…æ•°
            task_info_item["body"].count("```") // 2,  # ã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯æ•°
        ]

        task_features.append(feature_vector)
        task_node_ids.append(task_id)

    # ã‚¨ãƒƒã‚¸ã‚’ä½œæˆ (é–‹ç™ºè€… â†’ ã‚¿ã‚¹ã‚¯ã®ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ã‚·ãƒ§ãƒ³)
    edge_indices = []

    for interaction in interactions:
        dev_name = interaction["dev_name"]
        task_id = interaction["task_id"]

        if dev_name in active_devs and task_id in active_tasks:
            dev_idx = dev_node_ids.index(dev_name)
            task_idx = task_node_ids.index(task_id)
            edge_indices.append([dev_idx, task_idx])

    # HeteroDataã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ä½œæˆ
    data = HeteroData()

    # ãƒãƒ¼ãƒ‰ç‰¹å¾´é‡ã¨IDã‚’è¨­å®š
    data["dev"].x = torch.tensor(dev_features, dtype=torch.float)
    data["dev"].node_id = dev_node_ids

    data["task"].x = torch.tensor(task_features, dtype=torch.float)
    data["task"].node_id = task_node_ids

    # ã‚¨ãƒƒã‚¸ã‚’è¨­å®š
    if edge_indices:
        edge_tensor = torch.tensor(edge_indices, dtype=torch.long).t().contiguous()
        data["dev", "writes", "task"].edge_index = edge_tensor
        # é€†æ–¹å‘ã‚¨ãƒƒã‚¸ã‚‚è¿½åŠ 
        data["task", "written_by", "dev"].edge_index = edge_tensor.flip(0)
    else:
        # ã‚¨ãƒƒã‚¸ãŒãªã„å ´åˆã¯ç©ºã®ãƒ†ãƒ³ã‚½ãƒ«
        data["dev", "writes", "task"].edge_index = torch.empty((2, 0), dtype=torch.long)
        data["task", "written_by", "dev"].edge_index = torch.empty(
            (2, 0), dtype=torch.long
        )

    print(f"âœ… ã‚°ãƒ©ãƒ•ãƒ‡ãƒ¼ã‚¿ä½œæˆå®Œäº†:")
    print(f"  é–‹ç™ºè€…ãƒãƒ¼ãƒ‰: {data['dev'].x.shape}")
    print(f"  ã‚¿ã‚¹ã‚¯ãƒãƒ¼ãƒ‰: {data['task'].x.shape}")
    print(f"  ã‚¨ãƒƒã‚¸æ•°: {data['dev', 'writes', 'task'].edge_index.shape[1]}")

    return data


def retrain_gnn_model(graph_data):
    """æ–°ã—ã„ã‚°ãƒ©ãƒ•ãƒ‡ãƒ¼ã‚¿ã§GNNãƒ¢ãƒ‡ãƒ«ã‚’å†è¨“ç·´"""
    print("ğŸ¤– GNNãƒ¢ãƒ‡ãƒ«ã‚’å†è¨“ç·´ä¸­...")

    # GNNãƒ¢ãƒ‡ãƒ«ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
    try:
        from kazoo.gnn.gnn_model import GNNModel
    except ImportError:
        print("âŒ GNNãƒ¢ãƒ‡ãƒ«ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆã§ãã¾ã›ã‚“")
        return None

    import torch
    import torch.nn.functional as F
    from torch.optim import Adam

    # ãƒ‡ãƒã‚¤ã‚¹è¨­å®š
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"ä½¿ç”¨ãƒ‡ãƒã‚¤ã‚¹: {device}")

    # ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–
    model = GNNModel(
        in_channels_dict={
            "dev": graph_data["dev"].x.shape[1],  # å®Ÿéš›ã®é–‹ç™ºè€…ç‰¹å¾´é‡æ¬¡å…ƒ
            "task": graph_data["task"].x.shape[1],  # å®Ÿéš›ã®ã‚¿ã‚¹ã‚¯ç‰¹å¾´é‡æ¬¡å…ƒ
        },
        out_channels=32,
    )
    model = model.to(device)
    graph_data = graph_data.to(device)

    # ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ãƒ¼è¨­å®š
    optimizer = Adam(model.parameters(), lr=0.01)

    # è‡ªå·±æ•™å¸«ã‚ã‚Šå­¦ç¿’ã§ã‚°ãƒ©ãƒ•å†æ§‹ç¯‰ã‚’å­¦ç¿’
    print("å­¦ç¿’é–‹å§‹...")

    model.train()
    for epoch in range(200):
        optimizer.zero_grad()

        # ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰ãƒ‘ã‚¹
        embeddings = model(graph_data.x_dict, graph_data.edge_index_dict)

        # æå¤±è¨ˆç®—ï¼ˆã‚³ãƒ³ãƒˆãƒ©ã‚¹ãƒˆå­¦ç¿’é¢¨ï¼‰
        dev_emb = embeddings["dev"]
        task_emb = embeddings["task"]

        # æ­£ã®ãƒšã‚¢ï¼ˆå®Ÿéš›ã®ã‚¨ãƒƒã‚¸ï¼‰
        edge_index = graph_data["dev", "writes", "task"].edge_index
        if edge_index.shape[1] > 0:
            pos_dev_emb = dev_emb[edge_index[0]]
            pos_task_emb = task_emb[edge_index[1]]
            pos_score = F.cosine_similarity(pos_dev_emb, pos_task_emb, dim=1)

            # è² ã®ãƒšã‚¢ï¼ˆãƒ©ãƒ³ãƒ€ãƒ ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼‰
            num_neg = min(edge_index.shape[1], 100)
            neg_dev_idx = torch.randint(0, dev_emb.shape[0], (num_neg,)).to(device)
            neg_task_idx = torch.randint(0, task_emb.shape[0], (num_neg,)).to(device)
            neg_dev_emb = dev_emb[neg_dev_idx]
            neg_task_emb = task_emb[neg_task_idx]
            neg_score = F.cosine_similarity(neg_dev_emb, neg_task_emb, dim=1)

            # ã‚³ãƒ³ãƒˆãƒ©ã‚¹ãƒˆæå¤±
            pos_loss = -torch.log(torch.sigmoid(pos_score)).mean()
            neg_loss = -torch.log(torch.sigmoid(-neg_score)).mean()
            loss = pos_loss + neg_loss
        else:
            # ã‚¨ãƒƒã‚¸ãŒãªã„å ´åˆã¯åŸ‹ã‚è¾¼ã¿ã®æ­£å‰‡åŒ–ã®ã¿
            loss = torch.norm(dev_emb) + torch.norm(task_emb)

        loss.backward()
        optimizer.step()

        if epoch % 50 == 0:
            print(f"  ã‚¨ãƒãƒƒã‚¯ {epoch}: æå¤± = {loss.item():.4f}")

    print("âœ… å­¦ç¿’å®Œäº†")

    # ãƒ¢ãƒ‡ãƒ«ã¨ã‚°ãƒ©ãƒ•ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜
    model.eval()
    model_save_path = "data/gnn_model_retrained.pt"
    graph_save_path = "data/graph_retrained.pt"

    torch.save(model.state_dict(), model_save_path)
    torch.save(graph_data.cpu(), graph_save_path)

    print(f"ğŸ’¾ å†è¨“ç·´æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ä¿å­˜: {model_save_path}")
    print(f"ğŸ’¾ æ–°ã—ã„ã‚°ãƒ©ãƒ•ãƒ‡ãƒ¼ã‚¿ä¿å­˜: {graph_save_path}")

    return model


def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    print("ğŸš€ GNNå†è¨“ç·´ãƒ—ãƒ­ã‚»ã‚¹é–‹å§‹")
    print("=" * 50)

    # 1. æœ€è¿‘ã®ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿
    events = load_recent_github_data()

    if not events:
        print("âŒ ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        return

    # 2. ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ã‚·ãƒ§ãƒ³ã‚’æŠ½å‡º
    interactions, task_info, dev_info = extract_developer_task_interactions(events)

    # 3. ã‚°ãƒ©ãƒ•ãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆ
    graph_data = create_modern_graph_data(interactions, task_info, dev_info)

    # 4. GNNã‚’å†è¨“ç·´
    model = retrain_gnn_model(graph_data)

    if model:
        print("\nğŸ‰ GNNå†è¨“ç·´ãŒæ­£å¸¸ã«å®Œäº†ã—ã¾ã—ãŸï¼")
        print("æ–°ã—ã„ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«:")
        print("  - data/gnn_model_retrained.pt")
        print("  - data/graph_retrained.pt")
        print("\nè¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ›´æ–°ã—ã¦æ–°ã—ã„ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„ã€‚")
    else:
        print("âŒ GNNå†è¨“ç·´ã«å¤±æ•—ã—ã¾ã—ãŸ")


if __name__ == "__main__":
    main()
