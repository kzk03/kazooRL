#!/usr/bin/env python3
"""
Kazoo çµ±åˆå­¦ç¿’ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³

ã“ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã¯ä»¥ä¸‹ã‚’é †æ¬¡å®Ÿè¡Œã—ã¾ã™ï¼š
1. å”åŠ›ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯å¯¾å¿œGNNãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´
2. é€†å¼·åŒ–å­¦ç¿’ï¼ˆIRLï¼‰ã«ã‚ˆã‚‹å ±é…¬é‡ã¿å­¦ç¿’
3. å¼·åŒ–å­¦ç¿’ï¼ˆRLï¼‰ã«ã‚ˆã‚‹æœ€çµ‚çš„ãªã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆè¨“ç·´

Usage:
    python scripts/full_training_pipeline.py [OPTIONS]

Options:
    --config PATH    è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ (default: configs/base.yaml)
    --skip-gnn       GNNè¨“ç·´ã‚’ã‚¹ã‚­ãƒƒãƒ—
    --skip-irl       IRLè¨“ç·´ã‚’ã‚¹ã‚­ãƒƒãƒ—
    --skip-rl        RLè¨“ç·´ã‚’ã‚¹ã‚­ãƒƒãƒ—
    --production     ãƒ—ãƒ­ãƒ€ã‚¯ã‚·ãƒ§ãƒ³è¨­å®šã‚’ä½¿ç”¨
    --quiet          è©³ç´°ãƒ­ã‚°ã‚’ç„¡åŠ¹åŒ–
    --help           ãƒ˜ãƒ«ãƒ—è¡¨ç¤º

Examples:
    python scripts/full_training_pipeline.py                    # ãƒ•ãƒ«å®Ÿè¡Œ
    python scripts/full_training_pipeline.py --production       # ãƒ—ãƒ­ãƒ€ã‚¯ã‚·ãƒ§ãƒ³è¨­å®š
    python scripts/full_training_pipeline.py --skip-gnn         # GNNã‚¹ã‚­ãƒƒãƒ—
    python scripts/full_training_pipeline.py --skip-gnn --skip-irl  # RLã®ã¿
"""
import argparse
import json
import pickle
import subprocess
import sys
import time
import traceback
from datetime import datetime
from pathlib import Path

import numpy as np
import torch
import torch.optim as optim
import yaml
from omegaconf import OmegaConf

# Kazooãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
sys.path.append(str(Path(__file__).resolve().parents[1]))
from kazoo.envs.oss_simple import OSSSimpleEnv
from kazoo.envs.task import Task
from kazoo.features.feature_extractor import FeatureExtractor


class FullTrainingPipeline:
    """çµ±åˆå­¦ç¿’ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®å®Ÿè¡Œã‚¯ãƒ©ã‚¹"""

    def __init__(self, config_path="configs/base.yaml", quiet=False):
        self.config_path = config_path
        self.cfg = OmegaConf.load(config_path)
        self.start_time = datetime.now()
        self.quiet = quiet

        # ãƒ­ã‚°ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
        log_dir = Path("logs")
        log_dir.mkdir(exist_ok=True)

        # ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«è¨­å®š
        self.log_file = (
            log_dir / f"kazoo_training_{self.start_time.strftime('%Y%m%d_%H%M%S')}.log"
        )

        # åˆæœŸãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
        self.log(f"ğŸš€ Kazooçµ±åˆå­¦ç¿’ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³é–‹å§‹: {self.start_time}")
        self.log(f"ğŸ“ è¨­å®š: {config_path}")
        self.log(f"ğŸ“‹ ãƒ­ã‚°: {self.log_file}")

    def log(self, message):
        """ãƒ­ã‚°ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®å‡ºåŠ›"""
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        log_message = f"[{timestamp}] {message}"

        if not self.quiet:
            print(log_message)

        # ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã«ã‚‚è¨˜éŒ²
        with open(self.log_file, "a", encoding="utf-8") as f:
            f.write(log_message + "\n")

    def check_prerequisites(self):
        """å‰ææ¡ä»¶ã®ç¢ºèª"""
        self.log("ğŸ” å‰ææ¡ä»¶ã®ç¢ºèªä¸­...")

        required_files = [
            self.cfg.env.backlog_path,
            self.cfg.env.dev_profiles_path,
            self.cfg.irl.expert_path,
        ]

        missing_files = []
        for file_path in required_files:
            if not Path(file_path).exists():
                missing_files.append(file_path)

        if missing_files:
            self.log(f"âŒ å¿…è¦ãªãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {missing_files}")
            return False

        # ãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ç¢ºèª
        Path("data").mkdir(exist_ok=True)
        Path("models").mkdir(exist_ok=True)
        Path("outputs").mkdir(exist_ok=True)

        self.log("âœ… å‰ææ¡ä»¶ãƒã‚§ãƒƒã‚¯å®Œäº†")
        return True

    def train_collaborative_gnn(self):
        """å”åŠ›ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯å¯¾å¿œGNNãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´"""
        self.log("ğŸ§  å”åŠ›ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯å¯¾å¿œGNNè¨“ç·´é–‹å§‹...")

        try:
            # å”åŠ›ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ§‹ç¯‰ï¼ˆå¿…è¦ã«å¿œã˜ã¦ï¼‰
            if not Path("data/developer_collaboration_network.pt").exists():
                self.log("ğŸ”— é–‹ç™ºè€…å”åŠ›ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’æ§‹ç¯‰ä¸­...")
                result = subprocess.run(
                    [
                        sys.executable,
                        "tools/data_processing/build_developer_network.py",
                    ],
                    capture_output=True,
                    text=True,
                    cwd=Path.cwd(),
                )

                if result.returncode != 0:
                    self.log(f"âŒ é–‹ç™ºè€…ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ§‹ç¯‰ã‚¨ãƒ©ãƒ¼: {result.stderr}")
                    return False

                self.log("âœ… é–‹ç™ºè€…å”åŠ›ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ§‹ç¯‰å®Œäº†")

            # GNNãƒ¢ãƒ‡ãƒ«è¨“ç·´
            self.log("ğŸ‹ï¸ GNNãƒ¢ãƒ‡ãƒ«è¨“ç·´å®Ÿè¡Œä¸­...")
            start_time = time.time()

            result = subprocess.run(
                [sys.executable, "scripts/train_collaborative_gat.py"],
                capture_output=True,
                text=True,
                cwd=Path.cwd(),
            )

            duration = time.time() - start_time

            if result.returncode != 0:
                self.log(f"âŒ GNNè¨“ç·´ã‚¨ãƒ©ãƒ¼: {result.stderr}")
                return False

            self.log(f"âœ… GNNè¨“ç·´å®Œäº† (æ‰€è¦æ™‚é–“: {duration:.1f}ç§’)")

            # ç”Ÿæˆã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã®ç¢ºèª
            required_models = [
                "data/gnn_model_collaborative.pt",
                "data/graph_collaborative.pt",
            ]

            for model_path in required_models:
                if not Path(model_path).exists():
                    self.log(
                        f"âŒ ç”Ÿæˆã•ã‚Œã‚‹ã¹ããƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {model_path}"
                    )
                    return False

            return True

        except Exception as e:
            self.log(f"âŒ GNNè¨“ç·´ä¸­ã®äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼: {e}")
            self.log(traceback.format_exc())
            return False

    def train_irl(self):
        """é€†å¼·åŒ–å­¦ç¿’ï¼ˆIRLï¼‰ã«ã‚ˆã‚‹å ±é…¬é‡ã¿å­¦ç¿’"""
        self.log("ğŸ¯ é€†å¼·åŒ–å­¦ç¿’ï¼ˆIRLï¼‰é–‹å§‹...")

        try:
            start_time = time.time()

            # IRLã®å®Ÿè¡Œ
            self.log(
                f"ğŸ“Š IRLè¨­å®š: ã‚¨ãƒãƒƒã‚¯æ•°={self.cfg.irl.epochs}, å­¦ç¿’ç‡={self.cfg.irl.learning_rate}"
            )

            # train_irl.pyã®å†…å®¹ã‚’ç›´æ¥å®Ÿè¡Œ
            self._run_irl_training()

            duration = time.time() - start_time
            self.log(f"âœ… IRLè¨“ç·´å®Œäº† (æ‰€è¦æ™‚é–“: {duration:.1f}ç§’)")

            # å­¦ç¿’æ¸ˆã¿é‡ã¿ã®ç¢ºèª
            if not Path(self.cfg.irl.output_weights_path).exists():
                self.log(
                    f"âŒ å­¦ç¿’æ¸ˆã¿é‡ã¿ãƒ•ã‚¡ã‚¤ãƒ«ãŒç”Ÿæˆã•ã‚Œã¾ã›ã‚“ã§ã—ãŸ: {self.cfg.irl.output_weights_path}"
                )
                return False

            # é‡ã¿ã®åˆ†æ
            weights = np.load(self.cfg.irl.output_weights_path)
            self.log(
                f"ğŸ“ˆ å­¦ç¿’æ¸ˆã¿é‡ã¿çµ±è¨ˆ: å¹³å‡={weights.mean():.4f}, æ¨™æº–åå·®={weights.std():.4f}"
            )
            self.log(f"ğŸ“ˆ é‡ã¿ç¯„å›²: æœ€å°={weights.min():.4f}, æœ€å¤§={weights.max():.4f}")

            return True

        except Exception as e:
            self.log(f"âŒ IRLè¨“ç·´ä¸­ã®ã‚¨ãƒ©ãƒ¼: {e}")
            self.log(traceback.format_exc())
            return False

    def _run_irl_training(self):
        """IRLè¨“ç·´ã®å®Ÿéš›ã®å®Ÿè¡Œ"""
        self.log("ğŸ“š ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆè»Œè·¡ã¨ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿...")

        # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
        try:
            with open(self.cfg.irl.expert_path, "rb") as f:
                trajectories = pickle.load(f)
                expert_trajectory_steps = trajectories[0] if trajectories else []
        except Exception as e:
            raise Exception(f"ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆè»Œè·¡ã®èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")

        if not expert_trajectory_steps:
            raise Exception("ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆè»Œè·¡ã«ã‚¹ãƒ†ãƒƒãƒ—ãŒå«ã¾ã‚Œã¦ã„ã¾ã›ã‚“")

        with open(self.cfg.env.backlog_path, "r", encoding="utf-8") as f:
            backlog_data = json.load(f)
        with open(self.cfg.env.dev_profiles_path, "r", encoding="utf-8") as f:
            dev_profiles_data = yaml.safe_load(f)

        # ç’°å¢ƒã¨ç‰¹å¾´é‡æŠ½å‡ºå™¨ã®åˆæœŸåŒ–
        env = OSSSimpleEnv(self.cfg, backlog_data, dev_profiles_data)
        feature_extractor = FeatureExtractor(self.cfg)
        feature_dim = len(feature_extractor.feature_names)

        all_tasks_db = {task.id: task for task in env.backlog}

        self.log(f"ğŸ”§ IRLãƒ¢ãƒ‡ãƒ«è¨­å®š: ç‰¹å¾´é‡æ¬¡å…ƒ={feature_dim}")

        # ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–
        reward_weights = torch.randn(feature_dim, requires_grad=True)
        optimizer = optim.Adam([reward_weights], lr=self.cfg.irl.learning_rate)

        self.log(
            f"ğŸ”„ è¨“ç·´ãƒ«ãƒ¼ãƒ—é–‹å§‹: {len(expert_trajectory_steps)} ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã‚¹ãƒ†ãƒƒãƒ—"
        )

        # è¨“ç·´ãƒ«ãƒ¼ãƒ—
        for epoch in range(self.cfg.irl.epochs):
            total_loss = 0
            valid_steps = 0

            for step_data in expert_trajectory_steps:
                try:
                    optimizer.zero_grad()

                    # è»Œè·¡ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰çŠ¶æ…‹ã¨è¡Œå‹•ã‚’å–å¾—
                    state = step_data["state"]
                    action_details = step_data["action_details"]

                    developer_id = action_details.get("developer")
                    expert_task_id = action_details.get("task_id")
                    event_timestamp = datetime.fromisoformat(
                        action_details.get("timestamp").replace("Z", "+00:00")
                    )

                    # ãƒ‡ãƒ¼ã‚¿æœ‰åŠ¹æ€§ç¢ºèª
                    developer_profile = dev_profiles_data.get(developer_id)
                    expert_task = all_tasks_db.get(expert_task_id)
                    if not developer_profile or not expert_task:
                        continue

                    developer_obj = {"name": developer_id, "profile": developer_profile}
                    env.current_time = event_timestamp

                    # ç‰¹å¾´é‡è¨ˆç®—
                    expert_features = feature_extractor.get_features(
                        expert_task, developer_obj, env
                    )
                    expert_features = torch.from_numpy(expert_features).float()

                    # ä»–ã®å¯èƒ½ãªè¡Œå‹•ã®ç‰¹å¾´é‡
                    other_features_list = []
                    for other_task_id in state["open_task_ids"]:
                        if other_task_id != expert_task_id:
                            other_task = all_tasks_db.get(other_task_id)
                            if other_task:
                                features = feature_extractor.get_features(
                                    other_task, developer_obj, env
                                )
                                other_features_list.append(
                                    torch.from_numpy(features).float()
                                )

                    if not other_features_list:
                        continue

                    # æå¤±è¨ˆç®—
                    expert_reward = torch.dot(reward_weights, expert_features)
                    other_rewards = torch.stack(
                        [torch.dot(reward_weights, f) for f in other_features_list]
                    )
                    log_sum_exp_other_rewards = torch.logsumexp(other_rewards, dim=0)

                    loss = -(expert_reward - log_sum_exp_other_rewards)
                    total_loss += loss.item()
                    valid_steps += 1

                    loss.backward()
                    optimizer.step()

                except Exception as e:
                    self.log(f"âš ï¸ ã‚¹ãƒ†ãƒƒãƒ—å‡¦ç†ã‚¨ãƒ©ãƒ¼ (ã‚¹ã‚­ãƒƒãƒ—): {e}")
                    continue

            if valid_steps > 0:
                avg_loss = total_loss / valid_steps
                if (epoch + 1) % 100 == 0 or epoch == 0:
                    self.log(
                        f"ğŸ“ˆ ã‚¨ãƒãƒƒã‚¯ {epoch + 1}/{self.cfg.irl.epochs}, å¹³å‡æå¤±: {avg_loss:.6f}, æœ‰åŠ¹ã‚¹ãƒ†ãƒƒãƒ—: {valid_steps}"
                    )

        # é‡ã¿ã®ä¿å­˜
        self.log("ğŸ’¾ å­¦ç¿’æ¸ˆã¿å ±é…¬é‡ã¿ã®ä¿å­˜...")
        np.save(self.cfg.irl.output_weights_path, reward_weights.detach().numpy())

    def train_rl(self):
        """å¼·åŒ–å­¦ç¿’ï¼ˆRLï¼‰ã«ã‚ˆã‚‹æœ€çµ‚ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆè¨“ç·´"""
        self.log("ğŸ¤– å¼·åŒ–å­¦ç¿’ï¼ˆRLï¼‰é–‹å§‹...")

        try:
            start_time = time.time()

            self.log(
                f"ğŸ® RLè¨­å®š: ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—={self.cfg.rl.total_timesteps}, å­¦ç¿’ç‡={self.cfg.rl.learning_rate}"
            )

            # RLã®å®Ÿè¡Œ
            result = subprocess.run(
                [sys.executable, "scripts/train_oss.py"],
                capture_output=True,
                text=True,
                cwd=Path.cwd(),
            )

            duration = time.time() - start_time

            if result.returncode != 0:
                self.log(f"âŒ RLè¨“ç·´ã‚¨ãƒ©ãƒ¼: {result.stderr}")
                return False

            self.log(f"âœ… RLè¨“ç·´å®Œäº† (æ‰€è¦æ™‚é–“: {duration:.1f}ç§’)")

            # å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®ç¢ºèª
            model_paths = ["models/ppo_agent.pt"]
            for model_path in model_paths:
                if Path(model_path).exists():
                    self.log(f"âœ… å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ç¢ºèª: {model_path}")
                else:
                    self.log(f"âš ï¸ å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {model_path}")

            return True

        except Exception as e:
            self.log(f"âŒ RLè¨“ç·´ä¸­ã®ã‚¨ãƒ©ãƒ¼: {e}")
            self.log(traceback.format_exc())
            return False

    def generate_summary_report(self):
        """æœ€çµ‚çš„ãªã‚µãƒãƒªãƒ¼ãƒ¬ãƒãƒ¼ãƒˆã®ç”Ÿæˆ"""
        self.log("ğŸ“Š ã‚µãƒãƒªãƒ¼ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆä¸­...")

        end_time = datetime.now()
        total_duration = end_time - self.start_time

        report = {
            "kazoo_training_summary": {
                "start_time": self.start_time.isoformat(),
                "end_time": end_time.isoformat(),
                "total_duration_seconds": total_duration.total_seconds(),
                "config_used": self.config_path,
            },
            "generated_files": {
                "gnn_model": "data/gnn_model_collaborative.pt",
                "gnn_graph": "data/graph_collaborative.pt",
                "irl_weights": self.cfg.irl.output_weights_path,
                "rl_model": "models/ppo_agent.pt",
                "collaboration_network": "data/developer_collaboration_network.pt",
            },
            "file_status": {},
        }

        # ãƒ•ã‚¡ã‚¤ãƒ«å­˜åœ¨ç¢ºèª
        for name, path in report["generated_files"].items():
            exists = Path(path).exists()
            size = Path(path).stat().st_size if exists else 0
            report["file_status"][name] = {
                "exists": exists,
                "path": path,
                "size_bytes": size,
            }

        # ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜
        report_path = (
            self.log_file.parent
            / f"kazoo_report_{self.start_time.strftime('%Y%m%d_%H%M%S')}.json"
        )
        with open(report_path, "w", encoding="utf-8") as f:
            json.dump(report, f, indent=2, ensure_ascii=False)

        self.log(f"ğŸ“‹ ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜: {report_path}")
        self.log(f"â±ï¸ ç·å®Ÿè¡Œæ™‚é–“: {total_duration}")

        return report_path

    def run_full_pipeline(self, skip_gnn=False, skip_irl=False, skip_rl=False):
        """çµ±åˆãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®å®Ÿè¡Œ"""
        try:
            self.log("=" * 60)
            self.log("ğŸš€ Kazooçµ±åˆå­¦ç¿’ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè¡Œé–‹å§‹")
            self.log("=" * 60)

            # å‰ææ¡ä»¶ãƒã‚§ãƒƒã‚¯
            if not self.check_prerequisites():
                self.log("âŒ å‰ææ¡ä»¶ãƒã‚§ãƒƒã‚¯å¤±æ•—ã€‚å®Ÿè¡Œã‚’ä¸­æ­¢ã—ã¾ã™ã€‚")
                return False

            success_steps = []

            # ã‚¹ãƒ†ãƒƒãƒ—1: GNNè¨“ç·´
            if not skip_gnn:
                self.log("\n" + "=" * 40)
                self.log("ã‚¹ãƒ†ãƒƒãƒ—1: å”åŠ›ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯å¯¾å¿œGNNè¨“ç·´")
                self.log("=" * 40)
                if self.train_collaborative_gnn():
                    success_steps.append("GNN")
                else:
                    self.log("âŒ GNNè¨“ç·´ã«å¤±æ•—ã—ã¾ã—ãŸã€‚")
                    return False
            else:
                self.log("â­ï¸ GNNè¨“ç·´ã‚’ã‚¹ã‚­ãƒƒãƒ—")
                success_steps.append("GNN (ã‚¹ã‚­ãƒƒãƒ—)")

            # ã‚¹ãƒ†ãƒƒãƒ—2: IRLè¨“ç·´
            if not skip_irl:
                self.log("\n" + "=" * 40)
                self.log("ã‚¹ãƒ†ãƒƒãƒ—2: é€†å¼·åŒ–å­¦ç¿’ï¼ˆIRLï¼‰")
                self.log("=" * 40)
                if self.train_irl():
                    success_steps.append("IRL")
                else:
                    self.log("âŒ IRLè¨“ç·´ã«å¤±æ•—ã—ã¾ã—ãŸã€‚")
                    return False
            else:
                self.log("â­ï¸ IRLè¨“ç·´ã‚’ã‚¹ã‚­ãƒƒãƒ—")
                success_steps.append("IRL (ã‚¹ã‚­ãƒƒãƒ—)")

            # ã‚¹ãƒ†ãƒƒãƒ—3: RLè¨“ç·´
            if not skip_rl:
                self.log("\n" + "=" * 40)
                self.log("ã‚¹ãƒ†ãƒƒãƒ—3: å¼·åŒ–å­¦ç¿’ï¼ˆRLï¼‰")
                self.log("=" * 40)
                if self.train_rl():
                    success_steps.append("RL")
                else:
                    self.log("âŒ RLè¨“ç·´ã«å¤±æ•—ã—ã¾ã—ãŸã€‚")
                    return False
            else:
                self.log("â­ï¸ RLè¨“ç·´ã‚’ã‚¹ã‚­ãƒƒãƒ—")
                success_steps.append("RL (ã‚¹ã‚­ãƒƒãƒ—)")

            # æœ€çµ‚ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
            self.log("\n" + "=" * 40)
            self.log("ã‚¹ãƒ†ãƒƒãƒ—4: ã‚µãƒãƒªãƒ¼ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ")
            self.log("=" * 40)
            report_path = self.generate_summary_report()

            self.log("\n" + "=" * 60)
            self.log("ğŸ‰ Kazooçµ±åˆå­¦ç¿’ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Œäº†!")
            self.log(f"âœ… æˆåŠŸã—ãŸã‚¹ãƒ†ãƒƒãƒ—: {', '.join(success_steps)}")
            self.log(f"ğŸ“‹ è©³ç´°ãƒ¬ãƒãƒ¼ãƒˆ: {report_path}")
            self.log("=" * 60)

            return True

        except Exception as e:
            self.log(f"âŒ ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè¡Œä¸­ã®äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼: {e}")
            self.log(traceback.format_exc())
            return False


def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    parser = argparse.ArgumentParser(
        description="Kazooçµ±åˆå­¦ç¿’ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ä¾‹:
  python scripts/full_training_pipeline.py                    # ãƒ•ãƒ«å®Ÿè¡Œ
  python scripts/full_training_pipeline.py --production       # ãƒ—ãƒ­ãƒ€ã‚¯ã‚·ãƒ§ãƒ³è¨­å®š
  python scripts/full_training_pipeline.py --skip-gnn         # GNNã‚¹ã‚­ãƒƒãƒ—
  python scripts/full_training_pipeline.py --skip-gnn --skip-irl  # RLã®ã¿
  python scripts/full_training_pipeline.py --quiet            # é™ç²›ãƒ¢ãƒ¼ãƒ‰

å®Ÿè¡Œæ™‚é–“ã®ç›®å®‰:
  GNNè¨“ç·´: 30åˆ†ã€œ1æ™‚é–“
  IRLå­¦ç¿’: 2ã€œ4æ™‚é–“  
  RLå­¦ç¿’: 8ã€œ12æ™‚é–“
  åˆè¨ˆ: 10ã€œ17æ™‚é–“ï¼ˆãƒ•ãƒ«å®Ÿè¡Œï¼‰
        """,
    )
    parser.add_argument(
        "--config",
        default="configs/base.yaml",
        help="è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ (default: configs/base.yaml)",
    )
    parser.add_argument(
        "--production",
        action="store_true",
        help="ãƒ—ãƒ­ãƒ€ã‚¯ã‚·ãƒ§ãƒ³è¨­å®šã‚’ä½¿ç”¨ (configs/production.yaml)",
    )
    parser.add_argument("--skip-gnn", action="store_true", help="GNNè¨“ç·´ã‚’ã‚¹ã‚­ãƒƒãƒ—")
    parser.add_argument("--skip-irl", action="store_true", help="IRLè¨“ç·´ã‚’ã‚¹ã‚­ãƒƒãƒ—")
    parser.add_argument("--skip-rl", action="store_true", help="RLè¨“ç·´ã‚’ã‚¹ã‚­ãƒƒãƒ—")
    parser.add_argument("--quiet", action="store_true", help="è©³ç´°ãƒ­ã‚°ã‚’ç„¡åŠ¹åŒ–")

    args = parser.parse_args()

    # ãƒ—ãƒ­ãƒ€ã‚¯ã‚·ãƒ§ãƒ³è¨­å®šã®å‡¦ç†
    config_path = "configs/production.yaml" if args.production else args.config

    # ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè¡Œ
    pipeline = FullTrainingPipeline(config_path, quiet=args.quiet)

    if not args.quiet:
        print(f"\nğŸš€ Kazooçµ±åˆå­¦ç¿’ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³")
        print(f"ğŸ“ è¨­å®š: {config_path}")
        skip_list = []
        if args.skip_gnn:
            skip_list.append("GNN")
        if args.skip_irl:
            skip_list.append("IRL")
        if args.skip_rl:
            skip_list.append("RL")
        if skip_list:
            print(f"â­ï¸ ã‚¹ã‚­ãƒƒãƒ—: {', '.join(skip_list)}")
        else:
            print(f"ğŸ”„ å®Ÿè¡Œ: å…¨ã‚¹ãƒ†ãƒƒãƒ—ï¼ˆGNN â†’ IRL â†’ RLï¼‰")
        print()

    success = pipeline.run_full_pipeline(
        skip_gnn=args.skip_gnn, skip_irl=args.skip_irl, skip_rl=args.skip_rl
    )

    if not args.quiet:
        if success:
            print(f"\nâœ… å‡¦ç†ãŒæ­£å¸¸ã«å®Œäº†ã—ã¾ã—ãŸï¼")
            print(f"ğŸ“‹ ãƒ­ã‚°: {pipeline.log_file}")
        else:
            print(f"\nâŒ å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ")
            print(f"ğŸ“‹ ãƒ­ã‚°: {pipeline.log_file}")

    sys.exit(0 if success else 1)


if __name__ == "__main__":
    main()
