#!/usr/bin/env python3
"""
å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’2022å¹´ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã§è©•ä¾¡ã™ã‚‹ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
"""

import argparse
import json
import os
import sys
from collections import defaultdict
from pathlib import Path

import numpy as np
import yaml
from tqdm import tqdm

# ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã®ãƒ‘ã‚¹ã‚’è¿½åŠ 
sys.path.append(str(Path(__file__).parent.parent / "src"))

from kazoo.envs.oss_simple import OSSSimpleEnv
from kazoo.features.feature_extractor import FeatureExtractor


class SimpleConfig:
    """è¾æ›¸ã‚’ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®ã‚ˆã†ã«æ‰±ã†ãŸã‚ã®ã‚¯ãƒ©ã‚¹"""
    def __init__(self, config_dict):
        self._dict = config_dict
        for key, value in config_dict.items():
            if isinstance(value, dict):
                setattr(self, key, SimpleConfig(value))
            else:
                setattr(self, key, value)
    
    def get(self, key, default=None):
        """è¾æ›¸ã®getãƒ¡ã‚½ãƒƒãƒ‰ã¨åŒæ§˜ã®å‹•ä½œ"""
        return self._dict.get(key, default)


def load_config(config_path):
    """è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€"""
    with open(config_path, 'r', encoding='utf-8') as f:
        config_dict = yaml.safe_load(f)
    return SimpleConfig(config_dict)


def evaluate_recommendations(backlog_data, dev_profiles_data, feature_extractor, learned_weights, num_recommendations=5):
    """
    å­¦ç¿’æ¸ˆã¿é‡ã¿ã‚’ä½¿ã£ã¦æ¨è–¦ã‚·ã‚¹ãƒ†ãƒ ã‚’è©•ä¾¡ã™ã‚‹ï¼ˆç”Ÿã®JSONãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ï¼‰
    
    Returns:
        dict: è©•ä¾¡çµæœï¼ˆaccuracy, precision, recall, etc.ï¼‰
    """
    # ã‚¿ã‚¹ã‚¯ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®ç°¡æ˜“ç‰ˆã‚’ä½œæˆ
    class MockTask:
        def __init__(self, task_data):
            self.id = task_data.get('id')
            self.title = task_data.get('title', '')
            self.body = task_data.get('body', '')
            self.labels = [label.get('name') for label in task_data.get('labels', [])]
            self.comments = task_data.get('comments', 0)
            self.updated_at = task_data.get('updated_at', '2022-01-01T00:00:00Z')
            self.user = task_data.get('user', {})
            self.assignees = task_data.get('assignees', [])
            
            # æ—¥ä»˜æ–‡å­—åˆ—ã‚’datetimeã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã«å¤‰æ›
            from datetime import datetime
            if isinstance(self.updated_at, str):
                try:
                    if self.updated_at.endswith('Z'):
                        self.updated_at = self.updated_at[:-1] + '+00:00'
                    self.updated_at = datetime.fromisoformat(self.updated_at)
                except:
                    self.updated_at = datetime(2022, 1, 1)

    # ä»®ã®ç’°å¢ƒã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ä½œæˆï¼ˆç‰¹å¾´é‡æŠ½å‡ºã«å¿…è¦ï¼‰
    class MockEnv:
        def __init__(self, dev_profiles, backlog_data):
            self.dev_profiles = dev_profiles
            self.assignments = defaultdict(set)
            self.dev_action_history = defaultdict(list)
            self.backlog = [MockTask(task_data) for task_data in backlog_data]
    
    results = {
        'total_tasks': 0,
        'tasks_with_assignees': 0,
        'correct_recommendations': 0,
        'top_k_hits': defaultdict(int),
        'recommendation_details': []
    }
    
    print(f"ğŸ” è©•ä¾¡é–‹å§‹: {len(backlog_data)} ã‚¿ã‚¹ã‚¯ã§è©•ä¾¡")
    
    # æ‹…å½“è€…æƒ…å ±ãŒã‚ã‚‹ã‚¿ã‚¹ã‚¯ã®ã¿ã‚’æŠ½å‡º
    tasks_with_assignees = []
    for task in backlog_data:
        if task.get('assignees') and len(task['assignees']) > 0:
            # æ‹…å½“è€…ãŒdev_profiles_dataã«å«ã¾ã‚Œã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
            assignees = [a.get('login') for a in task['assignees'] if a.get('login')]
            if any(assignee in dev_profiles_data for assignee in assignees):
                tasks_with_assignees.append(task)
    
    print(f"ğŸ“Š æ‹…å½“è€…æƒ…å ±ãŒã‚ã‚‹ã‚¿ã‚¹ã‚¯: {len(tasks_with_assignees)}/{len(backlog_data)}")
    
    # æœ¬æ ¼çš„ãªè©•ä¾¡ã®ãŸã‚å…¨ã‚¿ã‚¹ã‚¯ã‚’ä½¿ç”¨ï¼ˆé«˜é€ŸåŒ–ã®ãŸã‚ã®åˆ¶é™ã‚’å‰Šé™¤ï¼‰
    eval_tasks = tasks_with_assignees  # å…¨ã¦ã®ã‚¿ã‚¹ã‚¯ã§è©•ä¾¡
    
    print(f"ğŸ¯ æœ¬æ ¼è©•ä¾¡: {len(eval_tasks)} ã‚¿ã‚¹ã‚¯ã§è©•ä¾¡å®Ÿè¡Œ")
    
    # ç’°å¢ƒã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ä½œæˆ
    mock_env = MockEnv(dev_profiles_data, backlog_data)
    
    # è©•ä¾¡ã‚¿ã‚¹ã‚¯ã®é€²æ—ãƒãƒ¼
    task_progress = tqdm(
        enumerate(eval_tasks),
        total=len(eval_tasks),
        desc="ğŸ“Š è©•ä¾¡é€²è¡Œ",
        unit="task",
        colour='green',
        leave=True
    )
    
    for task_idx, task in task_progress:
        # ã‚¿ã‚¹ã‚¯ã®å®Ÿéš›ã®æ‹…å½“è€…ã‚’å–å¾—ï¼ˆGround Truthï¼‰
        actual_assignees = [assignee.get('login') for assignee in task['assignees'] 
                          if assignee.get('login')]
        
        if not actual_assignees:
            task_progress.set_postfix({"Status": "æ‹…å½“è€…ãªã— (ã‚¹ã‚­ãƒƒãƒ—)"})
            continue
            
        # å„é–‹ç™ºè€…ã«å¯¾ã™ã‚‹ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—ï¼ˆæœ¬æ ¼è©•ä¾¡ã®ãŸã‚å…¨é–‹ç™ºè€…ã‚’å¯¾è±¡ï¼‰
        developer_scores = []
        available_developers = list(dev_profiles_data.keys())  # å…¨é–‹ç™ºè€…ã§è©•ä¾¡
        
        mock_task = MockTask(task)
        
        # é–‹ç™ºè€…è©•ä¾¡ã®é€²æ—ãƒãƒ¼ï¼ˆå†…éƒ¨ï¼‰
        dev_progress = tqdm(
            available_developers,
            desc=f"Task {task_idx+1:2d}",
            unit="dev",
            leave=False,
            colour='blue'
        )
        
        for dev_name in dev_progress:
            try:
                # é–‹ç™ºè€…ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ã‚’è¾æ›¸å½¢å¼ã§å–å¾—
                dev_profile = dev_profiles_data[dev_name]
                developer = {'name': dev_name, 'profile': dev_profile}
                
                # ç‰¹å¾´é‡ã‚’æŠ½å‡º
                features = feature_extractor.get_features(mock_task, developer, mock_env)
                
                # å­¦ç¿’æ¸ˆã¿é‡ã¿ã§é‡ã¿ä»˜ã‘ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—
                score = np.dot(features, learned_weights)
                developer_scores.append((dev_name, score))
                
            except Exception as e:
                # ç‰¹å¾´é‡æŠ½å‡ºã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸå ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
                if task_idx == 0:  # æœ€åˆã®ã‚¿ã‚¹ã‚¯ã§ã®ã¿ã‚¨ãƒ©ãƒ¼ã‚’è¡¨ç¤º
                    print(f"âš ï¸ é–‹ç™ºè€… {dev_name} ã®ç‰¹å¾´é‡æŠ½å‡ºã§ã‚¨ãƒ©ãƒ¼: {e}")
                continue
        
        if not developer_scores:
            task_progress.set_postfix({"Status": "ã‚¹ã‚³ã‚¢è¨ˆç®—å¤±æ•—"})
            continue
            
        # ã‚¹ã‚³ã‚¢é †ã«ã‚½ãƒ¼ãƒˆï¼ˆé™é †ï¼‰
        developer_scores.sort(key=lambda x: x[1], reverse=True)
        
        # ä¸Šä½Näººã®æ¨è–¦ãƒªã‚¹ãƒˆã‚’ä½œæˆ
        recommendations = [dev_name for dev_name, score in developer_scores[:num_recommendations]]
        
        # æ­£è§£ç‡ã‚’è¨ˆç®—
        correct_in_top_k = []
        for k in [1, 3, 5]:
            top_k_recs = recommendations[:k]
            hit = any(assignee in top_k_recs for assignee in actual_assignees)
            if hit:
                results['top_k_hits'][f'top_{k}'] += 1
            correct_in_top_k.append(hit)
        
        # è©³ç´°çµæœã‚’è¨˜éŒ²
        results['recommendation_details'].append({
            'task_id': task.get('id'),
            'task_title': task.get('title', 'Unknown')[:50],
            'actual_assignees': actual_assignees,
            'recommendations': recommendations,
            'top_scores': [(dev, float(score)) for dev, score in developer_scores[:5]],
            'correct_in_top_1': correct_in_top_k[0],
            'correct_in_top_3': correct_in_top_k[1],
            'correct_in_top_5': correct_in_top_k[2]
        })
        
        results['total_tasks'] += 1
        results['tasks_with_assignees'] += 1
        
        # é€²æ—ãƒãƒ¼ã®æƒ…å ±æ›´æ–°
        if results['total_tasks'] > 0:
            top1_acc = results['top_k_hits']['top_1'] / results['total_tasks']
            top3_acc = results['top_k_hits']['top_3'] / results['total_tasks']
            task_progress.set_postfix({
                "Top-1": f"{top1_acc:.3f}",
                "Top-3": f"{top3_acc:.3f}",
                "å®Œäº†": f"{results['total_tasks']}/{len(eval_tasks)}"
            })
    
    return results


def main():
    parser = argparse.ArgumentParser(description='å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’2022å¹´ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã§è©•ä¾¡')
    parser.add_argument('--config', required=True, help='è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹')
    parser.add_argument('--learned-weights', required=True, help='å­¦ç¿’æ¸ˆã¿é‡ã¿ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹')
    parser.add_argument('--output', default='evaluation_results_2022.json', help='çµæœå‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«')
    
    args = parser.parse_args()
    
    print("ğŸš€ 2022å¹´ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã§ã®è©•ä¾¡é–‹å§‹")
    print(f"ğŸ“ è¨­å®š: {args.config}")
    print(f"ğŸ¯ å­¦ç¿’æ¸ˆã¿é‡ã¿: {args.learned_weights}")
    
    # è¨­å®šèª­ã¿è¾¼ã¿
    config = load_config(args.config)
    
    # å­¦ç¿’æ¸ˆã¿é‡ã¿èª­ã¿è¾¼ã¿
    learned_weights = np.load(args.learned_weights)
    print(f"ğŸ’ª å­¦ç¿’æ¸ˆã¿é‡ã¿å½¢çŠ¶: {learned_weights.shape}")
    
    # ãƒãƒƒã‚¯ãƒ­ã‚°ã¨ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    print("ğŸ“š ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ä¸­...")
    with open(config.env.backlog_path, 'r', encoding='utf-8') as f:
        backlog_data = json.load(f)
    with open(config.env.dev_profiles_path, 'r', encoding='utf-8') as f:
        dev_profiles_data = yaml.safe_load(f)
    
    # ç‰¹å¾´é‡æŠ½å‡ºå™¨åˆæœŸåŒ–
    print("ğŸ”§ ç‰¹å¾´é‡æŠ½å‡ºå™¨åˆæœŸåŒ–ä¸­...")
    feature_extractor = FeatureExtractor(config)
    
    print(f"ğŸ“Š ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿: {len(backlog_data)} ã‚¿ã‚¹ã‚¯, {len(dev_profiles_data)} é–‹ç™ºè€…")
    
    # è©•ä¾¡å®Ÿè¡Œ
    print("ğŸ¯ æ¨è–¦è©•ä¾¡å®Ÿè¡Œä¸­...")
    results = evaluate_recommendations(backlog_data, dev_profiles_data, feature_extractor, learned_weights)
    
    # çµæœè¨ˆç®—
    total_tasks = results['total_tasks']
    if total_tasks > 0:
        accuracy_top_1 = results['top_k_hits']['top_1'] / total_tasks
        accuracy_top_3 = results['top_k_hits']['top_3'] / total_tasks
        accuracy_top_5 = results['top_k_hits']['top_5'] / total_tasks
        
        print("\n" + "="*60)
        print("ğŸ“ˆ è©•ä¾¡çµæœ")
        print("="*60)
        print(f"è©•ä¾¡ã‚¿ã‚¹ã‚¯æ•°: {total_tasks}")
        print(f"Top-1 Accuracy: {accuracy_top_1:.3f} ({results['top_k_hits']['top_1']}/{total_tasks})")
        print(f"Top-3 Accuracy: {accuracy_top_3:.3f} ({results['top_k_hits']['top_3']}/{total_tasks})")
        print(f"Top-5 Accuracy: {accuracy_top_5:.3f} ({results['top_k_hits']['top_5']}/{total_tasks})")
        print("="*60)
        
        # çµæœã‚’ã¾ã¨ã‚
        final_results = {
            'evaluation_config': args.config,
            'learned_weights_path': args.learned_weights,
            'total_tasks_evaluated': total_tasks,
            'tasks_with_assignees': results['tasks_with_assignees'],
            'top_1_accuracy': float(accuracy_top_1),
            'top_3_accuracy': float(accuracy_top_3),
            'top_5_accuracy': float(accuracy_top_5),
            'detailed_results': results['recommendation_details']
        }
        
        # çµæœä¿å­˜
        output_path = Path(args.output)
        output_path.parent.mkdir(parents=True, exist_ok=True)
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(final_results, f, indent=2, ensure_ascii=False)
        
        print(f"ğŸ’¾ è©³ç´°çµæœã‚’ä¿å­˜: {output_path}")
        
        # ã‚µãƒ³ãƒ—ãƒ«çµæœè¡¨ç¤º
        print("\nğŸ“‹ ã‚µãƒ³ãƒ—ãƒ«æ¨è–¦çµæœ:")
        for i, detail in enumerate(results['recommendation_details'][:3]):
            print(f"\nã‚¿ã‚¹ã‚¯ {i+1}: {detail['task_title']}")
            print(f"  å®Ÿéš›ã®æ‹…å½“è€…: {detail['actual_assignees']}")
            print(f"  æ¨è–¦Top-5: {detail['recommendations']}")
            print(f"  Top-1æ­£è§£: {'âœ…' if detail['correct_in_top_1'] else 'âŒ'}")
            print(f"  Top-3æ­£è§£: {'âœ…' if detail['correct_in_top_3'] else 'âŒ'}")
    
    else:
        print("âš ï¸ è©•ä¾¡ã§ãã‚‹ã‚¿ã‚¹ã‚¯ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")


if __name__ == "__main__":
    main()
