#!/usr/bin/env python3
"""
å¼·åŒ–å­¦ç¿’ã§å­¦ç¿’ã—ãŸãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ãŸæ¨è–¦ã‚·ã‚¹ãƒ†ãƒ ã®ç²¾åº¦è©•ä¾¡

ã“ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã¯ï¼š
1. å­¦ç¿’æ¸ˆã¿PPOãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿
2. ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã«å¯¾ã—ã¦æ¨è–¦ã‚’å®Ÿè¡Œ
3. æ­£è§£ãƒ‡ãƒ¼ã‚¿ï¼ˆã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆãƒˆãƒ©ã‚¸ã‚§ã‚¯ãƒˆãƒªï¼‰ã¨æ¯”è¼ƒ
4. æ¨è–¦ç²¾åº¦ï¼ˆTop-K accuracyï¼‰ã‚’è¨ˆç®—
"""

import os
import pickle
from pathlib import Path
from typing import Dict, List, Tuple

import hydra
import numpy as np
import torch
from omegaconf import DictConfig

from kazoo.envs.oss_simple import OSSSimpleEnv
from kazoo.learners.independent_ppo_controller import IndependentPPOController


class RecommendationEvaluator:
    """æ¨è–¦ã‚·ã‚¹ãƒ†ãƒ ã®ç²¾åº¦è©•ä¾¡ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, cfg: DictConfig):
        self.cfg = cfg
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        
        # ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿
        import json
        import yaml
        
        with open(cfg.env.backlog_path, "r", encoding="utf-8") as f:
            backlog = json.load(f)
        with open(cfg.env.dev_profiles_path, "r", encoding="utf-8") as f:
            dev_profiles = yaml.safe_load(f)
        
        # ç’°å¢ƒã¨ã‚³ãƒ³ãƒˆãƒ­ãƒ¼ãƒ©ãƒ¼ã®åˆæœŸåŒ–
        self.env = OSSSimpleEnv(cfg, backlog, dev_profiles)
        self.controller = IndependentPPOController(self.env, cfg)
        
        # å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿
        self.load_trained_models()
        
        # ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ï¼ˆæ­£è§£ãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦ä½¿ç”¨ï¼‰
        self.expert_trajectories = self.load_expert_data()
    
    def load_trained_models(self):
        """å­¦ç¿’æ¸ˆã¿PPOãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã‚€"""
        model_dir = Path(self.cfg.rl.output_model_dir)
        
        if not model_dir.exists():
            raise FileNotFoundError(f"Model directory not found: {model_dir}")
        
        print(f"Loading trained models from: {model_dir}")
        
        loaded_count = 0
        for agent_id in self.controller.agent_ids:
            model_path = model_dir / f"ppo_agent_{agent_id}.pth"
            if model_path.exists():
                try:
                    self.controller.agents[agent_id].load(str(model_path))
                    loaded_count += 1
                    print(f"âœ… Loaded model for {agent_id}")
                except Exception as e:
                    print(f"âŒ Failed to load model for {agent_id}: {e}")
            else:
                print(f"âš ï¸  Model not found for {agent_id}: {model_path}")
        
        print(f"Successfully loaded {loaded_count}/{len(self.controller.agent_ids)} models")
    
    def load_expert_data(self) -> List[Dict]:
        """ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆè»Œè·¡ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã‚€ï¼ˆæ­£è§£ãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦ä½¿ç”¨ï¼‰"""
        expert_path = Path(self.cfg.irl.expert_path)
        
        if not expert_path.exists():
            print(f"âš ï¸  Expert data not found: {expert_path}")
            return []
        
        with open(expert_path, 'rb') as f:
            expert_data = pickle.load(f)
        
        print(f"Loaded {len(expert_data)} expert trajectories")
        return expert_data
    
    def get_model_recommendations(self, num_tasks: int = 50) -> List[Tuple[str, str]]:
        """
        å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ã¦ã‚¿ã‚¹ã‚¯æ¨è–¦ã‚’ç”Ÿæˆ
        
        Returns:
            List of (task_id, recommended_developer) tuples
        """
        recommendations = []
        
        # ç’°å¢ƒã‚’ãƒªã‚»ãƒƒãƒˆ
        observations = self.env.reset()
        
        for step in range(num_tasks):
            if not self.env.backlog:
                break
            
            # ç¾åœ¨ã®ã‚¿ã‚¹ã‚¯ã‚’å–å¾—
            current_task = self.env.backlog[0]  # æœ€åˆã®ã‚¿ã‚¹ã‚¯ã‚’è©•ä¾¡å¯¾è±¡ã¨ã™ã‚‹
            
            # å„é–‹ç™ºè€…ã«å¯¾ã™ã‚‹è¡Œå‹•ç¢ºç‡ã‚’è¨ˆç®—
            developer_scores = {}
            
            for agent_id in self.controller.agent_ids:
                if agent_id in observations:
                    obs = observations[agent_id]
                    
                    # ãƒ¢ãƒ‡ãƒ«ã‹ã‚‰è¡Œå‹•ç¢ºç‡ã‚’å–å¾—
                    with torch.no_grad():
                        obs_tensor = torch.FloatTensor(obs).to(self.controller.agents[agent_id].device)
                        action_probs = self.controller.agents[agent_id].policy.actor(obs_tensor).cpu().numpy()
                    
                    # ã‚¢ã‚¯ã‚·ãƒ§ãƒ³2ï¼ˆã‚¿ã‚¹ã‚¯ã‚’å—ã‘å…¥ã‚Œã‚‹ï¼‰ã®ç¢ºç‡ã‚’ä½¿ç”¨
                    accept_prob = action_probs[2].item() if len(action_probs) > 2 else 0.0
                    developer_scores[agent_id] = accept_prob
            
            # æœ€ã‚‚ç¢ºç‡ã®é«˜ã„é–‹ç™ºè€…ã‚’æ¨è–¦
            recommended_dev = None
            if developer_scores:
                recommended_dev = max(developer_scores, key=developer_scores.get)
                recommendations.append((current_task.id, recommended_dev))
            
            # æ¨è–¦ã‚’å®Ÿè¡Œï¼ˆã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ï¼‰
            actions = {agent_id: 0 for agent_id in self.controller.agent_ids}  # å…¨å“¡ãŒå¾…æ©Ÿ
            if recommended_dev and recommended_dev in actions:
                actions[recommended_dev] = 2  # æ¨è–¦ã•ã‚ŒãŸé–‹ç™ºè€…ãŒã‚¿ã‚¹ã‚¯ã‚’å—ã‘å…¥ã‚Œ
            
            observations, rewards, terminateds, truncateds, infos = self.env.step(actions)
            
            # çµ‚äº†æ¡ä»¶ãƒã‚§ãƒƒã‚¯
            if all(terminateds.values()) or all(truncateds.values()):
                break
        
        return recommendations
    
    def calculate_accuracy(self, recommendations: List[Tuple[str, str]], k_values: List[int] = [1, 3, 5]) -> Dict[str, float]:
        """
        æ¨è–¦ç²¾åº¦ã‚’è¨ˆç®—
        
        Args:
            recommendations: ãƒ¢ãƒ‡ãƒ«ã®æ¨è–¦çµæœ [(task_id, recommended_dev), ...]
            k_values: Top-Kç²¾åº¦ã‚’è¨ˆç®—ã™ã‚‹Kã®å€¤ã®ãƒªã‚¹ãƒˆ
        
        Returns:
            å„Kå€¤ã«å¯¾ã™ã‚‹ç²¾åº¦ã®è¾æ›¸
        """
        if not self.expert_trajectories:
            print("âŒ No expert data available for accuracy calculation")
            return {}
        
        # ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆãƒ‡ãƒ¼ã‚¿ã‹ã‚‰æ­£è§£ã‚’ä½œæˆ
        expert_assignments = {}
        for trajectory_episode in self.expert_trajectories:  # å¤–å´ã®ãƒªã‚¹ãƒˆï¼ˆã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ï¼‰
            for step in trajectory_episode:  # å„ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰å†…ã®ã‚¹ãƒ†ãƒƒãƒ—
                if isinstance(step, dict) and 'action_details' in step:
                    action_details = step['action_details']
                    task_id = action_details.get('task_id')
                    assigned_dev = action_details.get('developer')
                    if task_id and assigned_dev:
                        expert_assignments[task_id] = assigned_dev
        
        print(f"Expert assignments available for {len(expert_assignments)} tasks")
        
        # ç²¾åº¦è¨ˆç®—
        accuracies = {}
        valid_recommendations = 0
        
        for k in k_values:
            correct_predictions = 0
            
            for task_id, recommended_dev in recommendations:
                if task_id in expert_assignments:
                    valid_recommendations += 1
                    expert_dev = expert_assignments[task_id]
                    
                    # Top-Kç²¾åº¦ã®å ´åˆã€ã“ã“ã§ã¯å˜ç´”ã«Top-1ã¨ã—ã¦è¨ˆç®—
                    # ã‚ˆã‚Šè¤‡é›‘ãªTop-Kè¨ˆç®—ãŒå¿…è¦ãªå ´åˆã¯ã€è¤‡æ•°ã®æ¨è–¦ã‚’ç”Ÿæˆã™ã‚‹å¿…è¦ãŒã‚ã‚‹
                    if recommended_dev == expert_dev:
                        correct_predictions += 1
            
            if valid_recommendations > 0:
                accuracy = correct_predictions / valid_recommendations
                accuracies[f"top_{k}_accuracy"] = accuracy
            else:
                accuracies[f"top_{k}_accuracy"] = 0.0
        
        return accuracies
    
    def evaluate(self, num_tasks: int = 50):
        """æ¨è–¦ã‚·ã‚¹ãƒ†ãƒ ã®ç·åˆè©•ä¾¡ã‚’å®Ÿè¡Œ"""
        print("ğŸ¯ Starting recommendation accuracy evaluation...")
        print(f"Evaluating {num_tasks} tasks")
        
        # æ¨è–¦ã®ç”Ÿæˆ
        print("\nğŸ“Š Generating recommendations...")
        recommendations = self.get_model_recommendations(num_tasks)
        
        print(f"Generated {len(recommendations)} recommendations")
        
        # ã‚µãƒ³ãƒ—ãƒ«æ¨è–¦ã®è¡¨ç¤º
        print("\nğŸ“‹ Sample recommendations:")
        for i, (task_id, dev) in enumerate(recommendations[:5]):
            print(f"  {i+1}. Task {task_id} â†’ {dev}")
        
        # ç²¾åº¦è¨ˆç®—
        print("\nğŸ¯ Calculating accuracy...")
        accuracies = self.calculate_accuracy(recommendations)
        
        # çµæœè¡¨ç¤º
        print("\nğŸ“ˆ Recommendation Accuracy Results:")
        print("=" * 50)
        
        if accuracies:
            for metric, value in accuracies.items():
                print(f"{metric:20s}: {value:.3f} ({value*100:.1f}%)")
        else:
            print("âŒ Could not calculate accuracy (no expert data or valid recommendations)")
        
        # è©³ç´°çµ±è¨ˆ
        print(f"\nTotal recommendations: {len(recommendations)}")
        print(f"Expert data points: {len(self.expert_trajectories)}")
        
        return accuracies


@hydra.main(version_base="1.3", config_path="../configs", config_name="base")
def main(cfg: DictConfig):
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    
    print("ğŸš€ Recommendation Accuracy Evaluation")
    print("=" * 50)
    
    try:
        evaluator = RecommendationEvaluator(cfg)
        results = evaluator.evaluate(num_tasks=30)  # 30ã‚¿ã‚¹ã‚¯ã§è©•ä¾¡
        
        print("\nâœ… Evaluation completed successfully!")
        
        return results
        
    except Exception as e:
        print(f"âŒ Evaluation failed: {e}")
        raise


if __name__ == "__main__":
    main()
