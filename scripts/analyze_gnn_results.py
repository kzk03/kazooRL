import sys
from pathlib import Path

sys.path.append(str(Path(__file__).resolve().parents[1] / "src"))

import json
from collections import defaultdict

import numpy as np
import torch
import torch.nn.functional as F

from kazoo.gnn.gnn_model import GNNModel


def load_metadata():
    """é–‹ç™ºè€…ã¨ã‚¿ã‚¹ã‚¯ã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿"""
    
    # é–‹ç™ºè€…ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿
    profile_path = Path("configs/dev_profiles.yaml")
    if profile_path.exists():
        import yaml
        with open(profile_path, 'r', encoding='utf-8') as f:
            profiles = yaml.safe_load(f)
    else:
        profiles = None
    
    # ã‚¿ã‚¹ã‚¯ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ï¼ˆJSONå½¢å¼ï¼‰
    status_dir = Path("data/status/")
    task_metadata = []
    
    if status_dir.exists():
        for json_file in status_dir.glob("*.json"):
            try:
                with open(json_file, 'r', encoding='utf-8') as f:
                    for line in f:
                        line = line.strip()
                        if line:
                            task_data = json.loads(line)
                            task_metadata.append(task_data)
            except Exception as e:
                print(f"Warning: Could not read {json_file}: {e}")
    
    return profiles, task_metadata

def analyze_gnn_results():
    """GNNçµæœã®è©³ç´°åˆ†æ"""
    
    # ãƒ‘ã‚¹è¨­å®š
    root = Path(__file__).resolve().parents[1]
    graph_path = root / "data/graph.pt"
    model_path = root / "data/gnn_model.pt"
    
    # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    print("Loading data...")
    data = torch.load(graph_path, weights_only=False)
    
    # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    profiles, task_metadata = load_metadata()
    
    # ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿
    print("Loading model...")
    model = GNNModel(in_channels_dict={"dev": 8, "task": 9}, out_channels=32)
    model.load_state_dict(torch.load(model_path, weights_only=True))
    model.eval()
    
    # äºˆæ¸¬å®Ÿè¡Œ
    print("Running analysis...")
    with torch.no_grad():
        embeddings = model(data.x_dict, data.edge_index_dict)
    
    dev_emb = embeddings['dev']
    task_emb = embeddings['task']
    
    # â–¼â–¼â–¼ã€ä¿®æ­£ç®‡æ‰€ã€‘node_idã®é©åˆ‡ãªå‡¦ç†â–¼â–¼â–¼
    # é–‹ç™ºè€…IDã¨ã‚¿ã‚¹ã‚¯IDã‚’å–å¾—
    if hasattr(data['dev'], 'node_id'):
        if isinstance(data['dev'].node_id, torch.Tensor):
            dev_ids = data['dev'].node_id.tolist()
        elif isinstance(data['dev'].node_id, list):
            dev_ids = data['dev'].node_id
        else:
            dev_ids = list(range(dev_emb.size(0)))
    else:
        dev_ids = list(range(dev_emb.size(0)))
    
    if hasattr(data['task'], 'node_id'):
        if isinstance(data['task'].node_id, torch.Tensor):
            task_ids = data['task'].node_id.tolist()
        elif isinstance(data['task'].node_id, list):
            task_ids = data['task'].node_id
        else:
            task_ids = list(range(task_emb.size(0)))
    else:
        task_ids = list(range(task_emb.size(0)))
    # â–²â–²â–²ã€ä¿®æ­£ç®‡æ‰€ã“ã“ã¾ã§ã€‘â–²â–²â–²
    
    print(f"\n=== è©³ç´°åˆ†æçµæœ ===")
    print(f"é–‹ç™ºè€…æ•°: {len(dev_ids)}, ã‚¿ã‚¹ã‚¯æ•°: {len(task_ids)}")
    
    # 1. é¡ä¼¼åº¦åˆ†å¸ƒã®åˆ†æ
    print("\nğŸ“Š é¡ä¼¼åº¦åˆ†å¸ƒã®åˆ†æ")
    dev_0 = dev_emb[0:1]
    all_similarities = F.cosine_similarity(dev_0, task_emb)
    unique_values, counts = torch.unique(all_similarities.round(decimals=4), return_counts=True)
    
    print(f"é¡ä¼¼åº¦ã®å€¤ã®ç¨®é¡: {len(unique_values)}")
    for val, count in zip(unique_values[:10], counts[:10]):  # ä¸Šä½10å€‹ã‚’è¡¨ç¤º
        print(f"  {val.item():.4f}: {count.item()}ä»¶")
    
    # 2. é–‹ç™ºè€…åˆ¥ã®æ¨è–¦åˆ†æ
    print(f"\nğŸ¯ é–‹ç™ºè€…åˆ¥æ¨è–¦åˆ†æï¼ˆä¸Šä½5åï¼‰")
    for dev_idx in range(min(5, dev_emb.size(0))):
        dev_vec = dev_emb[dev_idx:dev_idx+1]
        similarities = F.cosine_similarity(dev_vec, task_emb)
        top_5 = torch.topk(similarities, k=5)
        
        dev_id = dev_ids[dev_idx] if dev_idx < len(dev_ids) else 'Unknown'
        print(f"\né–‹ç™ºè€… {dev_idx} (ID: {dev_id}):")
        for rank, (sim, task_idx) in enumerate(zip(top_5.values, top_5.indices), 1):
            task_id = task_ids[task_idx] if task_idx < len(task_ids) else 'Unknown'
            print(f"  {rank}. Task {task_idx.item()} (ID: {task_id}): {sim.item():.4f}")
    
    # 3. ã‚¿ã‚¹ã‚¯ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°åˆ†æ
    print(f"\nğŸ” ã‚¿ã‚¹ã‚¯ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°åˆ†æ")
    # ã‚¿ã‚¹ã‚¯åŸ‹ã‚è¾¼ã¿ã®é¡ä¼¼åº¦è¡Œåˆ—ã‚’è¨ˆç®—ï¼ˆè¨ˆç®—é‡ã‚’è€ƒæ…®ã—ã¦ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼‰
    sample_size = min(100, task_emb.size(0))
    sample_indices = torch.randperm(task_emb.size(0))[:sample_size]
    sample_task_emb = task_emb[sample_indices]
    
    # é¡ä¼¼åº¦è¡Œåˆ—è¨ˆç®—
    task_similarity_matrix = F.cosine_similarity(
        sample_task_emb.unsqueeze(1), 
        sample_task_emb.unsqueeze(0), 
        dim=2
    )
    
    # é«˜ã„é¡ä¼¼åº¦ã‚’æŒã¤ã‚¿ã‚¹ã‚¯ãƒšã‚¢ã‚’æ¢ç´¢
    upper_triangle = torch.triu(task_similarity_matrix, diagonal=1)
    high_sim_indices = torch.where(upper_triangle > 0.8)
    
    print(f"é«˜ã„é¡ä¼¼åº¦ï¼ˆ>0.8ï¼‰ã‚’æŒã¤ã‚¿ã‚¹ã‚¯ãƒšã‚¢: {len(high_sim_indices[0])}çµ„")
    for i in range(min(5, len(high_sim_indices[0]))):
        idx1, idx2 = high_sim_indices[0][i], high_sim_indices[1][i]
        sim = task_similarity_matrix[idx1, idx2].item()
        task1_idx = sample_indices[idx1].item()
        task2_idx = sample_indices[idx2].item()
        task1_id = task_ids[task1_idx] if task1_idx < len(task_ids) else 'Unknown'
        task2_id = task_ids[task2_idx] if task2_idx < len(task_ids) else 'Unknown'
        print(f"  Task {task1_idx} (ID: {task1_id}) - Task {task2_idx} (ID: {task2_id}): {sim:.4f}")
    
    # 4. é–‹ç™ºè€…ã®å°‚é–€æ€§åˆ†æ
    print(f"\nğŸ‘¥ é–‹ç™ºè€…ã®å°‚é–€æ€§åˆ†æ")
    # å„é–‹ç™ºè€…ã«ã¤ã„ã¦ã€æœ€ã‚‚é¡ä¼¼åº¦ã®é«˜ã„ã‚¿ã‚¹ã‚¯ã®ç‰¹å¾´ã‚’åˆ†æ
    for dev_idx in range(min(3, dev_emb.size(0))):
        dev_vec = dev_emb[dev_idx:dev_idx+1]
        similarities = F.cosine_similarity(dev_vec, task_emb)
        
        # ä¸Šä½20%ã®ã‚¿ã‚¹ã‚¯ã‚’å°‚é–€åˆ†é‡ã¨ã—ã¦å®šç¾©
        top_k = max(1, int(0.2 * task_emb.size(0)))
        top_tasks = torch.topk(similarities, k=top_k).indices
        
        mean_sim = similarities[top_tasks].mean().item()
        std_sim = similarities[top_tasks].std().item()
        
        dev_id = dev_ids[dev_idx] if dev_idx < len(dev_ids) else 'Unknown'
        print(f"é–‹ç™ºè€… {dev_idx} (ID: {dev_id}):")
        print(f"  å°‚é–€åˆ†é‡ã‚¿ã‚¹ã‚¯æ•°: {top_k}")
        print(f"  å¹³å‡é¡ä¼¼åº¦: {mean_sim:.4f}")
        print(f"  é¡ä¼¼åº¦æ¨™æº–åå·®: {std_sim:.4f}")
    
    # 5. å…¨ä½“ã®çµ±è¨ˆæƒ…å ±
    print(f"\nğŸ“ˆ å…¨ä½“çµ±è¨ˆ")
    # å…¨ãƒšã‚¢ã®é¡ä¼¼åº¦çµ±è¨ˆ
    all_dev_similarities = []
    for dev_idx in range(min(10, dev_emb.size(0))):  # è¨ˆç®—é‡å‰Šæ¸›ã®ãŸã‚10åã¾ã§
        dev_vec = dev_emb[dev_idx:dev_idx+1]
        similarities = F.cosine_similarity(dev_vec, task_emb)
        all_dev_similarities.append(similarities)
    
    if all_dev_similarities:
        all_sims = torch.cat(all_dev_similarities)
        print(f"é¡ä¼¼åº¦çµ±è¨ˆï¼ˆã‚µãƒ³ãƒ—ãƒ«10ååˆ†ï¼‰:")
        print(f"  å¹³å‡: {all_sims.mean().item():.4f}")
        print(f"  æ¨™æº–åå·®: {all_sims.std().item():.4f}")
        print(f"  æœ€å°å€¤: {all_sims.min().item():.4f}")
        print(f"  æœ€å¤§å€¤: {all_sims.max().item():.4f}")

if __name__ == "__main__":
    analyze_gnn_results()