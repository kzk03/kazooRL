#!/usr/bin/env python3
"""
2023å¹´ã®GHArchiveãƒ‡ãƒ¼ã‚¿ã‹ã‚‰backlogå½¢å¼ã«å¤‰æ›
"""

import glob
import json
from collections import defaultdict
from datetime import datetime
from pathlib import Path


def process_github_event(event):
    """GitHubã‚¤ãƒ™ãƒ³ãƒˆã‹ã‚‰issue/PRãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º"""
    if event['type'] not in ['IssuesEvent', 'PullRequestEvent']:
        return None
    
    payload = event['payload']
    
    # IssuesEventã®å ´åˆ
    if event['type'] == 'IssuesEvent':
        if 'issue' not in payload:
            return None
        
        issue = payload['issue']
        
        # åŸºæœ¬æƒ…å ±
        task_data = {
            'id': issue['id'],
            'number': issue['number'],
            'title': issue['title'],
            'body': issue.get('body', ''),
            'state': issue['state'],
            'created_at': issue['created_at'],
            'updated_at': issue['updated_at'],
            'closed_at': issue.get('closed_at'),
            'author': {
                'login': issue['user']['login'],
                'id': issue['user']['id']
            },
            'assignees': issue.get('assignees', []),
            'labels': [label['name'] for label in issue.get('labels', [])],
            'comments_count': issue.get('comments', 0),
            'repo': {
                'name': event['repo']['name'],
                'id': event['repo']['id']
            },
            'type': 'issue'
        }
        
        return task_data
    
    # PullRequestEventã®å ´åˆ
    elif event['type'] == 'PullRequestEvent':
        if 'pull_request' not in payload:
            return None
            
        pr = payload['pull_request']
        
        # åŸºæœ¬æƒ…å ±
        task_data = {
            'id': pr['id'],
            'number': pr['number'],
            'title': pr['title'],
            'body': pr.get('body', ''),
            'state': pr['state'],
            'created_at': pr['created_at'],
            'updated_at': pr['updated_at'],
            'closed_at': pr.get('closed_at'),
            'merged_at': pr.get('merged_at'),
            'author': {
                'login': pr['user']['login'],
                'id': pr['user']['id']
            },
            'assignees': pr.get('assignees', []),
            'labels': [label['name'] for label in pr.get('labels', [])],
            'comments_count': pr.get('comments', 0),
            'repo': {
                'name': event['repo']['name'],
                'id': event['repo']['id']
            },
            'type': 'pull_request'
        }
        
        return task_data
    
    return None


def main():
    print("ğŸ”„ 2023å¹´ãƒ‡ãƒ¼ã‚¿å¤‰æ›é–‹å§‹")
    print("=" * 60)
    
    # 2023å¹´ã®JSONLãƒ•ã‚¡ã‚¤ãƒ«ã‚’å–å¾—
    data_dir = Path("/Users/kazuki-h/rl/kazoo/data/status/2023")
    jsonl_files = list(data_dir.glob("*.jsonl"))
    
    print(f"ğŸ“‚ å¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«: {len(jsonl_files)} å€‹")
    for file in sorted(jsonl_files):
        print(f"   {file.name}")
    
    # ãƒ‡ãƒ¼ã‚¿çµ±åˆ
    all_tasks = []
    issue_count = 0
    pr_count = 0
    other_count = 0
    
    for jsonl_file in sorted(jsonl_files):
        print(f"\nğŸ“– å‡¦ç†ä¸­: {jsonl_file.name}")
        
        with open(jsonl_file, 'r', encoding='utf-8') as f:
            for line_num, line in enumerate(f, 1):
                try:
                    event = json.loads(line.strip())
                    
                    task_data = process_github_event(event)
                    if task_data:
                        all_tasks.append(task_data)
                        
                        if task_data['type'] == 'issue':
                            issue_count += 1
                        elif task_data['type'] == 'pull_request':
                            pr_count += 1
                    else:
                        other_count += 1
                        
                except json.JSONDecodeError as e:
                    print(f"   âš ï¸ JSONè§£æã‚¨ãƒ©ãƒ¼ (è¡Œ {line_num}): {e}")
                    continue
                except Exception as e:
                    print(f"   âš ï¸ å‡¦ç†ã‚¨ãƒ©ãƒ¼ (è¡Œ {line_num}): {e}")
                    continue
        
        print(f"   å‡¦ç†æ¸ˆã¿: Issues={issue_count}, PRs={pr_count}, ãã®ä»–={other_count}")
    
    print(f"\nğŸ“Š æŠ½å‡ºçµæœ:")
    print(f"   Issues: {issue_count:,} ä»¶")
    print(f"   Pull Requests: {pr_count:,} ä»¶")
    print(f"   ç·è¨ˆ: {len(all_tasks):,} ä»¶")
    
    if not all_tasks:
        print("âŒ æŠ½å‡ºã•ã‚ŒãŸã‚¿ã‚¹ã‚¯ãŒã‚ã‚Šã¾ã›ã‚“")
        return
    
    # æ—¥ä»˜ã§ã‚½ãƒ¼ãƒˆ
    all_tasks.sort(key=lambda x: x['created_at'])
    
    # çµ±è¨ˆæƒ…å ±
    print(f"\nğŸ“… æœŸé–“æƒ…å ±:")
    print(f"   æœ€å¤: {all_tasks[0]['created_at']}")
    print(f"   æœ€æ–°: {all_tasks[-1]['created_at']}")
    
    # æœˆåˆ¥çµ±è¨ˆ
    monthly_stats = defaultdict(int)
    for task in all_tasks:
        month = task['created_at'][:7]  # YYYY-MM
        monthly_stats[month] += 1
    
    print(f"\nğŸ“ˆ æœˆåˆ¥çµ±è¨ˆ:")
    for month in sorted(monthly_stats.keys()):
        print(f"   {month}: {monthly_stats[month]:,} ä»¶")
    
    # backlog_test_2023.json ã¨ã—ã¦ä¿å­˜
    output_path = Path("/Users/kazuki-h/rl/kazoo/data/backlog_test_2023.json")
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(all_tasks, f, indent=2, ensure_ascii=False)
    
    print(f"\nâœ… ä¿å­˜å®Œäº†: {output_path}")
    print(f"   ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: {output_path.stat().st_size / 1024 / 1024:.1f} MB")
    
    # æ—¢å­˜ã®backlog.jsonã¨çµ±åˆ
    main_backlog_path = Path("/Users/kazuki-h/rl/kazoo/data/backlog.json")
    if main_backlog_path.exists():
        print(f"\nğŸ”— æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã¨ã®çµ±åˆ...")
        
        with open(main_backlog_path, 'r', encoding='utf-8') as f:
            existing_data = json.load(f)
        
        print(f"   æ—¢å­˜ãƒ‡ãƒ¼ã‚¿: {len(existing_data):,} ä»¶")
        
        # é‡è¤‡ãƒã‚§ãƒƒã‚¯ï¼ˆIDãƒ™ãƒ¼ã‚¹ï¼‰
        existing_ids = set(item.get('id') for item in existing_data if item.get('id'))
        new_tasks = [task for task in all_tasks if task['id'] not in existing_ids]
        
        print(f"   æ–°è¦ãƒ‡ãƒ¼ã‚¿: {len(new_tasks):,} ä»¶")
        print(f"   é‡è¤‡é™¤å¤–: {len(all_tasks) - len(new_tasks):,} ä»¶")
        
        # çµ±åˆ
        combined_data = existing_data + new_tasks
        combined_data.sort(key=lambda x: x.get('created_at', ''))
        
        # ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—
        backup_path = main_backlog_path.with_suffix('.bak')
        main_backlog_path.rename(backup_path)
        print(f"   ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—: {backup_path}")
        
        # æ–°ã—ã„backlog.jsonä¿å­˜
        with open(main_backlog_path, 'w', encoding='utf-8') as f:
            json.dump(combined_data, f, indent=2, ensure_ascii=False)
        
        print(f"âœ… çµ±åˆå®Œäº†: {len(combined_data):,} ä»¶")
        
        # å¹´æ¬¡åˆ†å¸ƒã®å†è¡¨ç¤º
        years = defaultdict(int)
        for item in combined_data:
            year = item.get('created_at', '')[:4]
            if year:
                years[year] += 1
        
        print(f"\nğŸ“Š çµ±åˆå¾Œã®å¹´æ¬¡åˆ†å¸ƒ:")
        for year in sorted(years.keys()):
            print(f"   {year}å¹´: {years[year]:,} ä»¶")
    
    print(f"\nâš ï¸ æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—:")
    print(f"   1. python results/scripts/create_temporal_split.py ã‚’å®Ÿè¡Œ")
    print(f"   2. 2019-2021: IRL, 2022: RLè¨“ç·´, 2023: ãƒ†ã‚¹ãƒˆç”¨ã«åˆ†å‰²")
    print(f"   3. evaluation/behavioral_comparison.py ã§ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ")


if __name__ == "__main__":
    main()
