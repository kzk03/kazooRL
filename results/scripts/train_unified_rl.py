#!/usr/bin/env python3
"""
çµ±åˆå¼·åŒ–å­¦ç¿’ã‚·ã‚¹ãƒ†ãƒ  - IRLé‡ã¿ã‚’ä½¿ç”¨ã—ãŸRLè¨“ç·´
train_oss.pyã¨train_rl_agent.pyã®çµ±åˆæ”¹è‰¯ç‰ˆ

ç‰¹å¾´:
- Hydraè¨­å®šç®¡ç† + IRLé‡ã¿çµ±åˆ
- æ—¢å­˜ã®OSSSimpleEnvã‚’æ´»ç”¨ã—ã¤ã¤ã‚«ã‚¹ã‚¿ãƒ å ±é…¬é–¢æ•°ã‚’è¿½åŠ 
- æ€§èƒ½è©•ä¾¡ã¨CSVå‡ºåŠ›æ©Ÿèƒ½
- è‡ªå‹•åŒ–ã•ã‚ŒãŸãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè¡Œ
"""

import json
import os
import sys
from pathlib import Path
from typing import Dict, List, Tuple

import hydra
import numpy as np
import pandas as pd
import torch
import yaml
from omegaconf import DictConfig, OmegaConf
from stable_baselines3 import PPO
from stable_baselines3.common.callbacks import EvalCallback
from stable_baselines3.common.vec_env import DummyVecEnv

# ãƒ‘ã‚¹è¨­å®š
sys.path.append(str(Path(__file__).resolve().parents[1] / "src"))

from kazoo.envs.oss_simple import OSSSimpleEnv
from kazoo.features.feature_extractor import FeatureExtractor
from kazoo.learners.independent_ppo_controller import IndependentPPOController


class UnifiedTaskAssignmentEnv(OSSSimpleEnv):
    """
    IRLé‡ã¿ã‚’çµ±åˆã—ãŸçµ±ä¸€ã‚¿ã‚¹ã‚¯å‰²ã‚Šå½“ã¦ç’°å¢ƒ
    æ—¢å­˜ã®OSSSimpleEnvã‚’ç¶™æ‰¿ã—ã€IRLé‡ã¿ãƒ™ãƒ¼ã‚¹ã®å ±é…¬ã‚’è¿½åŠ 
    """
    
    def __init__(self, config, backlog, dev_profiles, irl_weights_path=None):
        super().__init__(config, backlog, dev_profiles)
        
        self.feature_extractor = FeatureExtractor(config)
        self.irl_weights = self._load_irl_weights(irl_weights_path)
        
        # ãƒ‡ãƒãƒƒã‚°æƒ…å ±
        print(f"ğŸ® çµ±åˆç’°å¢ƒåˆæœŸåŒ–å®Œäº†")
        print(f"   é–‹ç™ºè€…æ•°: {len(dev_profiles)}")
        print(f"   ã‚¿ã‚¹ã‚¯æ•°: {len(backlog)}")
        print(f"   ç‰¹å¾´é‡æ¬¡å…ƒ: {len(self.feature_extractor.feature_names)}")
        print(f"   IRLé‡ã¿å½¢çŠ¶: {self.irl_weights.shape}")
    
    def _load_irl_weights(self, weights_path):
        """IRLå­¦ç¿’æ¸ˆã¿é‡ã¿ã‚’èª­ã¿è¾¼ã¿"""
        if weights_path and Path(weights_path).exists():
            try:
                weights = np.load(weights_path)
                print(f"âœ… IRLé‡ã¿ã‚’èª­ã¿è¾¼ã¿: {weights_path} ({weights.shape})")
                return torch.tensor(weights, dtype=torch.float32)
            except Exception as e:
                print(f"âš ï¸ IRLé‡ã¿èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
        
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ãƒ©ãƒ³ãƒ€ãƒ é‡ã¿
        feature_dim = len(self.feature_extractor.feature_names)
        weights = torch.randn(feature_dim, dtype=torch.float32)
        print(f"âš ï¸ ãƒ©ãƒ³ãƒ€ãƒ é‡ã¿ã‚’ä½¿ç”¨: {weights.shape}")
        return weights
    
    def calculate_irl_reward(self, task, developer) -> float:
        """IRLé‡ã¿ã‚’ä½¿ç”¨ã—ãŸå ±é…¬è¨ˆç®—"""
        try:
            # ç‰¹å¾´é‡ã‚’æŠ½å‡º
            features = self.feature_extractor.get_features(task, developer, self)
            features_tensor = torch.tensor(features, dtype=torch.float32)
            
            # IRLé‡ã¿ã¨ã®å†…ç©ã§å ±é…¬ã‚’è¨ˆç®—
            irl_reward = torch.dot(self.irl_weights, features_tensor).item()
            
            # å ±é…¬ã‚’æ­£è¦åŒ–
            irl_reward = np.clip(irl_reward, -10.0, 10.0)
            
            return irl_reward
            
        except Exception as e:
            print(f"âš ï¸ IRLå ±é…¬è¨ˆç®—ã‚¨ãƒ©ãƒ¼: {e}")
            return 0.0
    
    def step(self, action):
        """ã‚¹ãƒ†ãƒƒãƒ—å®Ÿè¡Œæ™‚ã«IRLå ±é…¬ã‚’è¿½åŠ """
        # å…ƒã®ç’°å¢ƒã®ã‚¹ãƒ†ãƒƒãƒ—ã‚’å®Ÿè¡Œ
        obs, original_reward, terminated, truncated, info = super().step(action)
        
        # IRLå ±é…¬ã‚’è¨ˆç®—ã—ã¦è¿½åŠ 
        if hasattr(self, '_last_assignment'):
            task, developer = self._last_assignment
            irl_reward = self.calculate_irl_reward(task, developer)
            
            # å…ƒã®å ±é…¬ã¨IRLå ±é…¬ã‚’çµ„ã¿åˆã‚ã›
            combined_reward = 0.5 * original_reward + 0.5 * irl_reward
            
            info['original_reward'] = original_reward
            info['irl_reward'] = irl_reward
            info['combined_reward'] = combined_reward
            
            return obs, combined_reward, terminated, truncated, info
        
        return obs, original_reward, terminated, truncated, info


@hydra.main(config_path="../configs", config_name="base_training", version_base=None)
def main(cfg: DictConfig):
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    
    print("ğŸš€ çµ±åˆå¼·åŒ–å­¦ç¿’ã‚·ã‚¹ãƒ†ãƒ é–‹å§‹")
    print("=" * 60)
    print(OmegaConf.to_yaml(cfg))
    
    # 1. ãƒ•ã‚¡ã‚¤ãƒ«å­˜åœ¨ç¢ºèª
    print("1. ãƒ•ã‚¡ã‚¤ãƒ«å­˜åœ¨ç¢ºèª...")
    required_files = [
        cfg.env.backlog_path,
        cfg.env.dev_profiles_path,
    ]
    
    for file_path in required_files:
        if not os.path.exists(file_path):
            print(f"âŒ å¿…è¦ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {file_path}")
            return
    
    print("âœ… å…¨ã¦ã®å¿…è¦ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç¢ºèª")
    
    # 2. ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    print("2. ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿...")
    with open(cfg.env.backlog_path, "r", encoding="utf-8") as f:
        backlog = json.load(f)
    with open(cfg.env.dev_profiles_path, "r", encoding="utf-8") as f:
        dev_profiles = yaml.safe_load(f)
    
    print(f"   ãƒãƒƒã‚¯ãƒ­ã‚°: {len(backlog)} ã‚¿ã‚¹ã‚¯")
    print(f"   é–‹ç™ºè€…: {len(dev_profiles)} äºº")
    
    # 3. çµ±åˆç’°å¢ƒã®åˆæœŸåŒ–
    print("3. çµ±åˆç’°å¢ƒåˆæœŸåŒ–...")
    irl_weights_path = getattr(cfg.irl, 'output_weights_path', None)
    
    env = UnifiedTaskAssignmentEnv(
        config=cfg,
        backlog=backlog,
        dev_profiles=dev_profiles,
        irl_weights_path=irl_weights_path
    )
    
    # 4. è¨“ç·´æ–¹æ³•ã®é¸æŠ
    training_method = cfg.get('training_method', 'unified')
    
    if training_method == 'original':
        # å…ƒã®OSSSimpleEnv + IndependentPPOControllerã‚’ä½¿ç”¨
        print("4. å…ƒã®ã‚·ã‚¹ãƒ†ãƒ ã§è¨“ç·´...")
        train_original_system(cfg, env)
        
    elif training_method == 'stable_baselines':
        # Stable-Baselines3ã‚’ç›´æ¥ä½¿ç”¨
        print("4. Stable-Baselines3ã§è¨“ç·´...")
        train_with_stable_baselines(cfg, env)
        
    else:
        # çµ±åˆã‚·ã‚¹ãƒ†ãƒ ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼‰
        print("4. çµ±åˆã‚·ã‚¹ãƒ†ãƒ ã§è¨“ç·´...")
        train_unified_system(cfg, env)
    
    # 5. è©•ä¾¡ã¨ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
    print("5. è©•ä¾¡ã¨ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ...")
    generate_evaluation_report(cfg, env)
    
    print("âœ… çµ±åˆå¼·åŒ–å­¦ç¿’ã‚·ã‚¹ãƒ†ãƒ å®Œäº†")


def train_original_system(cfg: DictConfig, env: UnifiedTaskAssignmentEnv):
    """å…ƒã®ã‚·ã‚¹ãƒ†ãƒ ã§ã®è¨“ç·´"""
    controller = IndependentPPOController(env=env, config=cfg)
    
    total_timesteps = cfg.rl.get('total_timesteps', 50000)
    controller.learn(total_timesteps=total_timesteps)
    
    # ãƒ¢ãƒ‡ãƒ«ä¿å­˜
    output_dir = cfg.rl.get('output_model_dir', 'models/original_rl')
    controller.save_models(output_dir)
    print(f"âœ… å…ƒã‚·ã‚¹ãƒ†ãƒ ãƒ¢ãƒ‡ãƒ«ä¿å­˜: {output_dir}")


def train_with_stable_baselines(cfg: DictConfig, env: UnifiedTaskAssignmentEnv):
    """Stable-Baselines3ã§ã®ç›´æ¥è¨“ç·´"""
    
    # ç’°å¢ƒã‚’Vectorizedã«å¤‰æ›
    def make_env():
        return env
    
    vec_env = DummyVecEnv([make_env])
    eval_env = DummyVecEnv([make_env])
    
    # PPOãƒ¢ãƒ‡ãƒ«ä½œæˆ
    model = PPO(
        "MlpPolicy",
        vec_env,
        verbose=1,
        learning_rate=cfg.rl.get('learning_rate', 3e-4),
        n_steps=cfg.rl.get('n_steps', 2048),
        batch_size=cfg.rl.get('batch_size', 64),
        n_epochs=cfg.rl.get('n_epochs', 10),
        gamma=cfg.rl.get('gamma', 0.99),
        device="auto"
    )
    
    # è©•ä¾¡ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯
    eval_callback = EvalCallback(
        eval_env,
        best_model_save_path="./models/unified_best/",
        log_path="./logs/unified_eval/",
        eval_freq=1000,
        deterministic=True,
        render=False
    )
    
    # è¨“ç·´å®Ÿè¡Œ
    total_timesteps = cfg.rl.get('total_timesteps', 50000)
    model.learn(
        total_timesteps=total_timesteps,
        callback=eval_callback,
        progress_bar=True
    )
    
    # ãƒ¢ãƒ‡ãƒ«ä¿å­˜
    model_path = "models/unified_rl_agent.zip"
    model.save(model_path)
    print(f"âœ… Stable-Baselines3ãƒ¢ãƒ‡ãƒ«ä¿å­˜: {model_path}")


def train_unified_system(cfg: DictConfig, env: UnifiedTaskAssignmentEnv):
    """çµ±åˆã‚·ã‚¹ãƒ†ãƒ ã§ã®è¨“ç·´ï¼ˆä¸¡æ–¹ã®æ‰‹æ³•ã‚’çµ„ã¿åˆã‚ã›ï¼‰"""
    
    print("ğŸ“Š çµ±åˆè¨“ç·´: å…ƒã‚·ã‚¹ãƒ†ãƒ  + Stable-Baselines3")
    
    # 1. å…ƒã‚·ã‚¹ãƒ†ãƒ ã§ã®äº‹å‰è¨“ç·´
    print("   1) å…ƒã‚·ã‚¹ãƒ†ãƒ ã§ã®äº‹å‰è¨“ç·´...")
    train_original_system(cfg, env)
    
    # 2. Stable-Baselines3ã§ã®å¾®èª¿æ•´
    print("   2) Stable-Baselines3ã§ã®å¾®èª¿æ•´...")
    train_with_stable_baselines(cfg, env)
    
    print("âœ… çµ±åˆè¨“ç·´å®Œäº†")


def generate_evaluation_report(cfg: DictConfig, env: UnifiedTaskAssignmentEnv):
    """è©•ä¾¡ãƒ¬ãƒãƒ¼ãƒˆã¨CSVãƒ•ã‚¡ã‚¤ãƒ«ã®ç”Ÿæˆ"""
    
    print("ğŸ“Š è©•ä¾¡ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆä¸­...")
    
    # åˆ©ç”¨å¯èƒ½ãªãƒ¢ãƒ‡ãƒ«ã‚’æ¤œç´¢
    model_paths = [
        "models/unified_rl_agent.zip",
        "models/original_rl/",
        "models/unified_best/"
    ]
    
    results = []
    
    for model_path in model_paths:
        if Path(model_path).exists():
            try:
                if model_path.endswith('.zip'):
                    # Stable-Baselines3ãƒ¢ãƒ‡ãƒ«
                    model = PPO.load(model_path)
                    model_name = Path(model_path).stem
                    
                    # è©•ä¾¡å®Ÿè¡Œ
                    rewards = evaluate_sb3_model(model, env, num_episodes=10)
                    
                else:
                    # å…ƒã‚·ã‚¹ãƒ†ãƒ ãƒ¢ãƒ‡ãƒ«ï¼ˆè©•ä¾¡æ–¹æ³•ã¯åˆ¥é€”å®Ÿè£…ãŒå¿…è¦ï¼‰
                    model_name = Path(model_path).name
                    rewards = [0.0]  # ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼
                
                # çµæœã‚’è¨˜éŒ²
                results.append({
                    'model_name': model_name,
                    'model_path': model_path,
                    'avg_reward': np.mean(rewards),
                    'std_reward': np.std(rewards),
                    'max_reward': np.max(rewards),
                    'min_reward': np.min(rewards),
                    'num_episodes': len(rewards)
                })
                
                print(f"   âœ… {model_name}: å¹³å‡å ±é…¬ {np.mean(rewards):.4f}")
                
            except Exception as e:
                print(f"   âŒ {model_path} è©•ä¾¡ã‚¨ãƒ©ãƒ¼: {e}")
    
    # CSVãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
    if results:
        df = pd.DataFrame(results)
        timestamp = pd.Timestamp.now().strftime("%Y%m%d_%H%M%S")
        csv_path = f"outputs/unified_rl_evaluation_{timestamp}.csv"
        
        df.to_csv(csv_path, index=False)
        print(f"âœ… è©•ä¾¡ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜: {csv_path}")
        
        # ã‚µãƒãƒªãƒ¼è¡¨ç¤º
        print("\nğŸ“ˆ è©•ä¾¡ã‚µãƒãƒªãƒ¼:")
        print(df.to_string(index=False))
    
    # ç‰¹å¾´é‡é‡è¦åº¦åˆ†æ
    analyze_feature_importance(env)


def evaluate_sb3_model(model, env, num_episodes=10) -> List[float]:
    """Stable-Baselines3ãƒ¢ãƒ‡ãƒ«ã®è©•ä¾¡"""
    rewards = []
    
    for episode in range(num_episodes):
        obs, _ = env.reset()
        episode_reward = 0
        
        while True:
            action, _ = model.predict(obs, deterministic=True)
            obs, reward, terminated, truncated, info = env.step(action)
            episode_reward += reward
            
            if terminated or truncated:
                break
        
        rewards.append(episode_reward)
    
    return rewards


def analyze_feature_importance(env: UnifiedTaskAssignmentEnv):
    """ç‰¹å¾´é‡é‡è¦åº¦åˆ†æ"""
    print("ğŸ” ç‰¹å¾´é‡é‡è¦åº¦åˆ†æ...")
    
    feature_names = env.feature_extractor.feature_names
    irl_weights = env.irl_weights.numpy()
    
    # é‡è¦åº¦ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ä½œæˆ
    importance_df = pd.DataFrame({
        'feature_name': feature_names,
        'irl_weight': irl_weights,
        'abs_weight': np.abs(irl_weights),
        'importance_rank': range(1, len(feature_names) + 1)
    })
    
    # é‡è¦åº¦é †ã«ã‚½ãƒ¼ãƒˆ
    importance_df = importance_df.sort_values('abs_weight', ascending=False)
    importance_df['importance_rank'] = range(1, len(importance_df) + 1)
    
    # CSVä¿å­˜
    timestamp = pd.Timestamp.now().strftime("%Y%m%d_%H%M%S")
    csv_path = f"outputs/unified_feature_importance_{timestamp}.csv"
    importance_df.to_csv(csv_path, index=False)
    
    print(f"âœ… ç‰¹å¾´é‡é‡è¦åº¦ä¿å­˜: {csv_path}")
    print("\nğŸ† TOP10 é‡è¦ç‰¹å¾´é‡:")
    print(importance_df.head(10)[['feature_name', 'irl_weight', 'abs_weight']].to_string(index=False))


if __name__ == "__main__":
    main()
