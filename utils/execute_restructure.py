#!/usr/bin/env python3
"""
Kazoo ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ§‹é€ å®Ÿè¡Œã‚¹ã‚¯ãƒªãƒ—ãƒˆ
æ–°ã—ã„éšå±¤æ§‹é€ ã«ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ•´ç†ã™ã‚‹
"""

import os
import shutil
from datetime import datetime
from pathlib import Path


def create_new_directory_structure():
    """æ–°ã—ã„ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ§‹é€ ã‚’ä½œæˆ"""
    print("ğŸ“ æ–°ã—ã„ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ§‹é€ ã‚’ä½œæˆä¸­...")

    directories = [
        "training/gat",
        "training/irl",
        "training/rl",
        "pipelines",
        "analysis/reports",
        "analysis/visualization",
        "evaluation",
        "data_processing",
        "utils",
    ]

    for dir_path in directories:
        Path(dir_path).mkdir(parents=True, exist_ok=True)
        print(f"âœ… {dir_path}")

    # __init__.pyãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆ
    init_dirs = ["training", "analysis"]
    for dir_name in init_dirs:
        init_file = Path(dir_name) / "__init__.py"
        init_file.write_text("# -*- coding: utf-8 -*-\n")


def move_and_rename_files():
    """ãƒ•ã‚¡ã‚¤ãƒ«ã®ç§»å‹•ã¨ãƒªãƒãƒ¼ãƒ ã‚’å®Ÿè¡Œ"""
    print("\nğŸ“¦ ãƒ•ã‚¡ã‚¤ãƒ«ç§»å‹•ãƒ»ãƒªãƒãƒ¼ãƒ ä¸­...")

    # ç§»å‹•ãƒ»ãƒªãƒãƒ¼ãƒ è¨ˆç”»
    moves = [
        # GATé–¢é€£
        ("scripts/train_collaborative_gat.py", "training/gat/train_gat.py"),
        ("scripts/train_gnn.py", "training/gat/train_gat_standalone.py"),
        # IRLé–¢é€£
        ("scripts/train_irl.py", "training/irl/train_irl.py"),
        # RLé–¢é€£
        ("scripts/train_oss.py", "training/rl/train_rl.py"),
        # ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³
        ("run_full_training_from_scratch.py", "pipelines/full_pipeline.py"),
        # åˆ†æ
        ("analyze_training_results.py", "analysis/reports/training_analysis.py"),
        ("generate_summary_report.py", "analysis/reports/summary_report.py"),
        ("analyze_gat_features.py", "analysis/reports/gat_analysis.py"),
        # è©•ä¾¡
        ("scripts/evaluate_2022_test.py", "evaluation/evaluate_models.py"),
        ("test_feature_dimensions.py", "evaluation/test_features.py"),
        # ãƒ‡ãƒ¼ã‚¿å‡¦ç†
        (
            "tools/data_processing/generate_graph.py",
            "data_processing/generate_graph.py",
        ),
        (
            "tools/data_processing/generate_profiles.py",
            "data_processing/generate_profiles.py",
        ),
        (
            "tools/data_processing/generate_backlog.py",
            "data_processing/generate_backlog.py",
        ),
        (
            "tools/data_processing/build_developer_network.py",
            "data_processing/build_network.py",
        ),
        (
            "tools/data_processing/get_github_data.py",
            "data_processing/extract_github_data.py",
        ),
        (
            "tools/data_processing/generate_labels.py",
            "data_processing/generate_labels.py",
        ),
    ]

    for old_path, new_path in moves:
        old_file = Path(old_path)
        new_file = Path(new_path)

        if old_file.exists():
            shutil.copy2(old_file, new_file)
            print(f"âœ… {old_path} -> {new_path}")
        else:
            print(f"âŒ {old_path} - ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")


def create_unified_irl_analysis():
    """çµ±åˆIRLåˆ†æã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’ä½œæˆ"""
    print("\nğŸ”„ çµ±åˆIRLåˆ†æã‚¹ã‚¯ãƒªãƒ—ãƒˆä½œæˆä¸­...")

    unified_script = '''#!/usr/bin/env python3
"""
çµ±åˆIRLåˆ†æã‚¹ã‚¯ãƒªãƒ—ãƒˆ
è¤‡æ•°ã®IRLåˆ†ææ©Ÿèƒ½ã‚’çµ±åˆã—ã€åˆ†ã‹ã‚Šã‚„ã™ã„ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ
"""

import numpy as np
import matplotlib.pyplot as plt
from pathlib import Path
from datetime import datetime
import sys
sys.path.append('src')

class IRLAnalyzer:
    """IRLçµæœã®çµ±åˆåˆ†æã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, weights_path="data/learned_weights_training.npy"):
        self.weights_path = Path(weights_path)
        self.weights = None
        self.feature_names = self._get_feature_names()
        
    def _get_feature_names(self):
        """ç‰¹å¾´é‡åã®å®šç¾©"""
        base_features = [
            "ãƒ­ã‚°ã‚¤ãƒ³åã®é•·ã•", "åå‰ã®æœ‰ç„¡", "åå‰ã®é•·ã•", "ä¼šç¤¾æƒ…å ±ã®æœ‰ç„¡", "ä¼šç¤¾åã®é•·ã•",
            "å ´æ‰€æƒ…å ±ã®æœ‰ç„¡", "å ´æ‰€æƒ…å ±ã®é•·ã•", "ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«æ–‡ã®æœ‰ç„¡", "ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«æ–‡ã®é•·ã•",
            "å…¬é–‹ãƒªãƒã‚¸ãƒˆãƒªæ•°", "å…¬é–‹ãƒªãƒã‚¸ãƒˆãƒªæ•°(å¯¾æ•°)", "ãƒ•ã‚©ãƒ­ãƒ¯ãƒ¼æ•°", "ãƒ•ã‚©ãƒ­ãƒ¯ãƒ¼æ•°(å¯¾æ•°)",
            "ãƒ•ã‚©ãƒ­ãƒ¼æ•°", "ãƒ•ã‚©ãƒ­ãƒ¼æ•°(å¯¾æ•°)", "ã‚¢ã‚«ã‚¦ãƒ³ãƒˆå¹´æ•°(æ—¥)", "ã‚¢ã‚«ã‚¦ãƒ³ãƒˆå¹´æ•°(å¹´)",
            "ãƒ•ã‚©ãƒ­ãƒ¯ãƒ¼/ãƒ•ã‚©ãƒ­ãƒ¼æ¯”", "å¹´é–“ãƒªãƒã‚¸ãƒˆãƒªä½œæˆæ•°", "äººæ°—åº¦ã‚¹ã‚³ã‚¢", "æ´»å‹•åº¦ã‚¹ã‚³ã‚¢",
            "å½±éŸ¿åŠ›ã‚¹ã‚³ã‚¢", "çµŒé¨“å€¤ã‚¹ã‚³ã‚¢", "ç¤¾äº¤æ€§ã‚¹ã‚³ã‚¢", "ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«å®Œæˆåº¦"
        ]
        
        gat_features = [f"GATç‰¹å¾´é‡{i}" for i in range(37)]
        return base_features + gat_features
    
    def load_weights(self):
        """é‡ã¿ã‚’èª­ã¿è¾¼ã¿"""
        if not self.weights_path.exists():
            raise FileNotFoundError(f"é‡ã¿ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {self.weights_path}")
        
        self.weights = np.load(self.weights_path)
        print(f"âœ… IRLé‡ã¿èª­ã¿è¾¼ã¿æˆåŠŸ: {self.weights.shape}")
        return self.weights
    
    def analyze_weights(self):
        """é‡ã¿ã®åŸºæœ¬åˆ†æ"""
        if self.weights is None:
            self.load_weights()
        
        analysis = {
            "ç·ç‰¹å¾´é‡æ•°": len(self.weights),
            "åŸºæœ¬ç‰¹å¾´é‡æ•°": 25,
            "GATç‰¹å¾´é‡æ•°": len(self.weights) - 25,
            "é‡è¦ç‰¹å¾´é‡æ•°": np.sum(np.abs(self.weights) > 0.5),
            "æ­£ã®é‡ã¿æ•°": np.sum(self.weights > 0),
            "è² ã®é‡ã¿æ•°": np.sum(self.weights < 0),
            "å¹³å‡é‡ã¿": self.weights.mean(),
            "æ¨™æº–åå·®": self.weights.std(),
            "æœ€å¤§é‡ã¿": self.weights.max(),
            "æœ€å°é‡ã¿": self.weights.min()
        }
        
        return analysis
    
    def get_important_features(self, top_n=10):
        """é‡è¦ãªç‰¹å¾´é‡ã‚’å–å¾—"""
        if self.weights is None:
            self.load_weights()
        
        # çµ¶å¯¾å€¤ã§é‡è¦åº¦ã‚’ã‚½ãƒ¼ãƒˆ
        importance_indices = np.argsort(np.abs(self.weights))[::-1]
        
        important_features = []
        for i in range(min(top_n, len(importance_indices))):
            idx = importance_indices[i]
            weight = self.weights[idx]
            name = self.feature_names[idx] if idx < len(self.feature_names) else f"ç‰¹å¾´é‡{idx}"
            important_features.append((name, weight, idx))
        
        return important_features
    
    def generate_simple_report(self):
        """åˆ†ã‹ã‚Šã‚„ã™ã„ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ"""
        if self.weights is None:
            self.load_weights()
        
        print("ğŸ¯ IRLå­¦ç¿’çµæœ - åˆ†ã‹ã‚Šã‚„ã™ã„è§£é‡ˆ")
        print("=" * 60)
        
        # åŸºæœ¬çµ±è¨ˆ
        analysis = self.analyze_weights()
        
        print("ğŸ“Š å­¦ç¿’çµæœã‚µãƒãƒªãƒ¼:")
        print(f"  åˆ†æã—ãŸç‰¹å¾´é‡æ•°: {analysis['ç·ç‰¹å¾´é‡æ•°']}")
        print(f"  é‡è¦ãªç‰¹å¾´é‡æ•°: {analysis['é‡è¦ç‰¹å¾´é‡æ•°']}")
        print(f"  æ­£ã®å½±éŸ¿: {analysis['æ­£ã®é‡ã¿æ•°']}å€‹")
        print(f"  è² ã®å½±éŸ¿: {analysis['è² ã®é‡ã¿æ•°']}å€‹")
        
        # å”åŠ›é–¢ä¿‚ vs åŸºæœ¬æƒ…å ±
        base_weights = self.weights[:25]
        gat_weights = self.weights[25:] if len(self.weights) > 25 else np.array([])
        
        base_importance = np.mean(np.abs(base_weights))
        gat_importance = np.mean(np.abs(gat_weights)) if len(gat_weights) > 0 else 0
        
        print(f"\\nğŸ¤ å”åŠ›é–¢ä¿‚ vs åŸºæœ¬æƒ…å ±:")
        print(f"  åŸºæœ¬æƒ…å ±ã®é‡è¦åº¦: {base_importance:.3f}")
        print(f"  å”åŠ›é–¢ä¿‚ã®é‡è¦åº¦: {gat_importance:.3f}")
        
        if gat_importance > base_importance:
            ratio = gat_importance / base_importance
            print(f"  â†’ å”åŠ›é–¢ä¿‚ãŒ {ratio:.1f}å€é‡è¦ï¼")
        
        # é‡è¦ãªç‰¹å¾´é‡
        important_features = self.get_important_features(10)
        
        print(f"\\nâœ… æœ€é‡è¦ç‰¹å¾´é‡ Top 10:")
        for rank, (name, weight, idx) in enumerate(important_features, 1):
            status = "å„ªå…ˆ" if weight > 0 else "å›é¿"
            print(f"  {rank:2d}. {name[:20]:20s} ({status}: {weight:6.3f})")
        
        return analysis, important_features
    
    def create_visualization(self):
        """å¯è¦–åŒ–ã‚°ãƒ©ãƒ•ã‚’ä½œæˆ"""
        if self.weights is None:
            self.load_weights()
        
        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))
        
        # 1. é‡è¦åº¦ãƒ©ãƒ³ã‚­ãƒ³ã‚°
        important_features = self.get_important_features(15)
        names = [f[0][:15] + "..." if len(f[0]) > 15 else f[0] for f in important_features]
        weights = [f[1] for f in important_features]
        colors = ['blue' if w > 0 else 'red' for w in weights]
        
        ax1.barh(range(len(weights)), weights, color=colors, alpha=0.7)
        ax1.set_yticks(range(len(weights)))
        ax1.set_yticklabels(names, fontsize=10)
        ax1.set_xlabel('Weight Value')
        ax1.set_title('Top 15 Most Important Features')
        ax1.grid(True, alpha=0.3)
        
        # 2. åŸºæœ¬ vs GATæ¯”è¼ƒ
        base_weights = self.weights[:25]
        gat_weights = self.weights[25:] if len(self.weights) > 25 else np.array([])
        
        categories = ['Basic Features']
        importances = [np.mean(np.abs(base_weights))]
        
        if len(gat_weights) > 0:
            categories.append('GAT Features')
            importances.append(np.mean(np.abs(gat_weights)))
        
        ax2.bar(categories, importances, color=['skyblue', 'lightcoral'][:len(categories)], alpha=0.8)
        ax2.set_ylabel('Average Importance')
        ax2.set_title('Feature Category Comparison')
        ax2.grid(True, alpha=0.3)
        
        # 3. é‡ã¿åˆ†å¸ƒ
        ax3.hist(self.weights, bins=30, alpha=0.7, edgecolor='black')
        ax3.axvline(0, color='red', linestyle='--', label='Zero')
        ax3.set_xlabel('Weight Value')
        ax3.set_ylabel('Frequency')
        ax3.set_title('Weight Distribution')
        ax3.legend()
        ax3.grid(True, alpha=0.3)
        
        # 4. ç´¯ç©é‡è¦åº¦
        sorted_abs_weights = np.sort(np.abs(self.weights))[::-1]
        cumsum_weights = np.cumsum(sorted_abs_weights)
        cumsum_normalized = cumsum_weights / cumsum_weights[-1] * 100
        
        ax4.plot(range(1, len(self.weights)+1), cumsum_normalized, 'b-', linewidth=2)
        ax4.axhline(80, color='red', linestyle='--', alpha=0.7, label='80%')
        ax4.axhline(95, color='orange', linestyle='--', alpha=0.7, label='95%')
        ax4.set_xlabel('Number of Features')
        ax4.set_ylabel('Cumulative Importance (%)')
        ax4.set_title('Cumulative Feature Importance')
        ax4.legend()
        ax4.grid(True, alpha=0.3)
        
        plt.tight_layout()
        
        # ä¿å­˜
        output_path = Path("outputs") / f"irl_unified_analysis_{datetime.now().strftime('%Y%m%d_%H%M%S')}.png"
        output_path.parent.mkdir(exist_ok=True)
        plt.savefig(output_path, dpi=300, bbox_inches='tight')
        print(f"âœ… åˆ†æã‚°ãƒ©ãƒ•ä¿å­˜: {output_path}")
        plt.close()
        
        return output_path

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ"""
    print("ğŸ” çµ±åˆIRLåˆ†æ")
    print(f"ğŸ“… å®Ÿè¡Œæ—¥æ™‚: {datetime.now()}")
    print("=" * 60)
    
    try:
        analyzer = IRLAnalyzer()
        analysis, important_features = analyzer.generate_simple_report()
        output_path = analyzer.create_visualization()
        
        print(f"\\nğŸ‰ åˆ†æå®Œäº†!")
        print(f"ğŸ“Š å¯è¦–åŒ–: {output_path}")
        
    except Exception as e:
        print(f"âŒ ã‚¨ãƒ©ãƒ¼: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
'''

    unified_file = Path("analysis/reports/irl_analysis.py")
    unified_file.write_text(unified_script)
    print(f"âœ… çµ±åˆIRLåˆ†æã‚¹ã‚¯ãƒªãƒ—ãƒˆä½œæˆ: {unified_file}")


def create_unified_pipeline():
    """çµ±åˆãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’ä½œæˆ"""
    print("\nğŸ”„ çµ±åˆãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ä½œæˆä¸­...")

    pipeline_script = '''#!/usr/bin/env python3
"""
Kazoo çµ±åˆå­¦ç¿’ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³
GAT â†’ IRL â†’ RL ã®å®Œå…¨ãªå­¦ç¿’ãƒ•ãƒ­ãƒ¼ã‚’å®Ÿè¡Œ
"""

import subprocess
import sys
from datetime import datetime
from pathlib import Path

class KazooPipeline:
    """Kazooå­¦ç¿’ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self):
        self.start_time = datetime.now()
        self.project_root = Path(__file__).parent.parent
        
    def run_command(self, cmd, description, working_dir=None):
        """ã‚³ãƒãƒ³ãƒ‰ã‚’å®Ÿè¡Œ"""
        print(f"\\nğŸš€ {description}")
        print(f"å®Ÿè¡Œã‚³ãƒãƒ³ãƒ‰: {cmd}")
        print("=" * 60)
        
        if working_dir:
            original_dir = Path.cwd()
            os.chdir(working_dir)
        
        try:
            result = subprocess.run(cmd, shell=True, capture_output=False, text=True)
            
            if result.returncode == 0:
                print(f"âœ… {description} å®Œäº†")
                return True
            else:
                print(f"âŒ {description} å¤±æ•— (exit code: {result.returncode})")
                return False
        finally:
            if working_dir:
                os.chdir(original_dir)
    
    def check_prerequisites(self):
        """å‰ææ¡ä»¶ã‚’ãƒã‚§ãƒƒã‚¯"""
        print("ğŸ“‹ å‰ææ¡ä»¶ãƒã‚§ãƒƒã‚¯ä¸­...")
        
        required_files = [
            "data/backlog_training.json",
            "data/expert_trajectories.pkl",
            "data/labels.pt",
            "configs/base_training.yaml"
        ]
        
        all_exist = True
        for file_path in required_files:
            if (self.project_root / file_path).exists():
                print(f"âœ… {file_path}")
            else:
                print(f"âŒ {file_path} - è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                all_exist = False
        
        return all_exist
    
    def run_gat_training(self):
        """GATè¨“ç·´ã‚’å®Ÿè¡Œ"""
        return self.run_command(
            "python training/gat/train_gat.py",
            "GAT (Graph Attention Network) è¨“ç·´",
            self.project_root
        )
    
    def run_irl_training(self):
        """IRLè¨“ç·´ã‚’å®Ÿè¡Œ"""
        return self.run_command(
            "python training/irl/train_irl.py", 
            "IRL (Inverse Reinforcement Learning) è¨“ç·´",
            self.project_root
        )
    
    def run_rl_training(self):
        """RLè¨“ç·´ã‚’å®Ÿè¡Œ"""
        return self.run_command(
            "python training/rl/train_rl.py",
            "RL (Reinforcement Learning) è¨“ç·´", 
            self.project_root
        )
    
    def run_analysis(self):
        """çµæœåˆ†æã‚’å®Ÿè¡Œ"""
        return self.run_command(
            "python analysis/reports/irl_analysis.py",
            "çµæœåˆ†æãƒ»ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ",
            self.project_root
        )
    
    def run_evaluation(self):
        """è©•ä¾¡ã‚’å®Ÿè¡Œ"""
        return self.run_command(
            "python evaluation/evaluate_models.py",
            "ãƒ¢ãƒ‡ãƒ«è©•ä¾¡",
            self.project_root
        )
    
    def run_full_pipeline(self):
        """å®Œå…¨ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’å®Ÿè¡Œ"""
        print(f"ğŸ¯ Kazooçµ±åˆå­¦ç¿’ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³é–‹å§‹")
        print(f"ğŸ“… é–‹å§‹æ™‚åˆ»: {self.start_time}")
        print(f"ğŸ“ ä½œæ¥­ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {self.project_root}")
        print("=" * 80)
        
        # å‰ææ¡ä»¶ãƒã‚§ãƒƒã‚¯
        if not self.check_prerequisites():
            print("âŒ å‰ææ¡ä»¶ãŒæº€ãŸã•ã‚Œã¦ã„ã¾ã›ã‚“")
            return False
        
        steps = [
            ("GATè¨“ç·´", self.run_gat_training),
            ("IRLè¨“ç·´", self.run_irl_training), 
            ("RLè¨“ç·´", self.run_rl_training),
            ("çµæœåˆ†æ", self.run_analysis),
            ("ãƒ¢ãƒ‡ãƒ«è©•ä¾¡", self.run_evaluation)
        ]
        
        failed_steps = []
        
        for step_name, step_func in steps:
            if not step_func():
                failed_steps.append(step_name)
                print(f"âš ï¸ {step_name}ã§å¤±æ•—ã—ã¾ã—ãŸãŒã€ç¶™ç¶šã—ã¾ã™")
        
        # å®Œäº†ãƒ¬ãƒãƒ¼ãƒˆ
        end_time = datetime.now()
        elapsed = end_time - self.start_time
        
        print("\\n" + "=" * 80)
        print(f"ğŸ‰ Kazooçµ±åˆå­¦ç¿’ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Œäº†!")
        print(f"â±ï¸ å®Ÿè¡Œæ™‚é–“: {elapsed}")
        print(f"ğŸ“… å®Œäº†æ™‚åˆ»: {end_time}")
        
        if failed_steps:
            print(f"\\nâš ï¸ å¤±æ•—ã—ãŸã‚¹ãƒ†ãƒƒãƒ—: {', '.join(failed_steps)}")
        else:
            print(f"\\nâœ… å…¨ã‚¹ãƒ†ãƒƒãƒ—ãŒæ­£å¸¸ã«å®Œäº†ã—ã¾ã—ãŸ!")
        
        # ç”Ÿæˆã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã®ç¢ºèª
        self.check_generated_files()
        
        return len(failed_steps) == 0
    
    def check_generated_files(self):
        """ç”Ÿæˆã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç¢ºèª"""
        print(f"\\nğŸ“‚ ç”Ÿæˆãƒ•ã‚¡ã‚¤ãƒ«ç¢ºèª:")
        
        expected_files = [
            "data/gnn_model_collaborative.pt",
            "data/graph_collaborative.pt",
            "data/learned_weights_training.npy",
            "models/ppo_agent.pt"
        ]
        
        for file_path in expected_files:
            full_path = self.project_root / file_path
            if full_path.exists():
                size = full_path.stat().st_size
                print(f"âœ… {file_path} ({size:,} bytes)")
            else:
                print(f"âŒ {file_path} - ç”Ÿæˆã•ã‚Œã¾ã›ã‚“ã§ã—ãŸ")

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ"""
    import os
    
    pipeline = KazooPipeline()
    success = pipeline.run_full_pipeline()
    
    sys.exit(0 if success else 1)

if __name__ == "__main__":
    main()
'''

    pipeline_file = Path("pipelines/full_pipeline.py")
    pipeline_file.write_text(pipeline_script)
    print(f"âœ… çµ±åˆãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ä½œæˆ: {pipeline_file}")


def update_import_paths():
    """ã‚¤ãƒ³ãƒãƒ¼ãƒˆãƒ‘ã‚¹ã‚’æ›´æ–°"""
    print("\nğŸ”§ ã‚¤ãƒ³ãƒãƒ¼ãƒˆãƒ‘ã‚¹æ›´æ–°ä¸­...")

    # æ–°ã—ã„ãƒ•ã‚¡ã‚¤ãƒ«ã§srcã¸ã®ãƒ‘ã‚¹ã‚’ä¿®æ­£
    files_to_update = [
        "training/gat/train_gat.py",
        "training/irl/train_irl.py",
        "training/rl/train_rl.py",
        "analysis/reports/irl_analysis.py",
        "evaluation/evaluate_models.py",
    ]

    for file_path in files_to_update:
        file_obj = Path(file_path)
        if file_obj.exists():
            content = file_obj.read_text()

            # srcãƒ‘ã‚¹ã®ä¿®æ­£
            if "sys.path.append('src')" in content:
                # ãƒ•ã‚¡ã‚¤ãƒ«ã®éšå±¤ã«å¿œã˜ã¦ãƒ‘ã‚¹ã‚’èª¿æ•´
                depth = len(file_obj.parts) - 1
                new_path = "../" * depth + "src"
                content = content.replace(
                    "sys.path.append('src')", f"sys.path.append('{new_path}')"
                )
                file_obj.write_text(content)
                print(f"âœ… {file_path} - ã‚¤ãƒ³ãƒãƒ¼ãƒˆãƒ‘ã‚¹æ›´æ–°")


def create_readme():
    """æ–°ã—ã„æ§‹é€ ã®READMEã‚’ä½œæˆ"""
    print("\nğŸ“ READMEä½œæˆä¸­...")

    readme_content = """# Kazoo ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ - æ–°æ§‹é€ 

## ğŸ“ ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ§‹é€ 

```
kazoo/
â”œâ”€â”€ training/           # å­¦ç¿’é–¢é€£ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
â”‚   â”œâ”€â”€ gat/           # GAT (Graph Attention Network) è¨“ç·´
â”‚   â”œâ”€â”€ irl/           # IRL (Inverse Reinforcement Learning) è¨“ç·´  
â”‚   â””â”€â”€ rl/            # RL (Reinforcement Learning) è¨“ç·´
â”œâ”€â”€ pipelines/         # çµ±åˆãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³
â”œâ”€â”€ analysis/          # åˆ†æãƒ»ãƒ¬ãƒãƒ¼ãƒˆ
â”‚   â”œâ”€â”€ reports/       # åˆ†æãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
â”‚   â””â”€â”€ visualization/ # å¯è¦–åŒ–
â”œâ”€â”€ evaluation/        # è©•ä¾¡ãƒ»ãƒ†ã‚¹ãƒˆ
â”œâ”€â”€ data_processing/   # ãƒ‡ãƒ¼ã‚¿å‡¦ç†
â”œâ”€â”€ utils/             # ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£
â”œâ”€â”€ src/              # ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚³ãƒ¼ãƒ‰
â”œâ”€â”€ configs/          # è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«
â”œâ”€â”€ data/             # ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«
â”œâ”€â”€ models/           # å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«
â””â”€â”€ outputs/          # å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«
```

## ğŸš€ ä½¿ç”¨æ–¹æ³•

### å®Œå…¨ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè¡Œ
```bash
python pipelines/full_pipeline.py
```

### å€‹åˆ¥ã‚¹ãƒ†ãƒƒãƒ—å®Ÿè¡Œ

#### GATè¨“ç·´
```bash
python training/gat/train_gat.py
```

#### IRLè¨“ç·´  
```bash
python training/irl/train_irl.py
```

#### RLè¨“ç·´
```bash
python training/rl/train_rl.py
```

### åˆ†æãƒ»ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
```bash
python analysis/reports/irl_analysis.py
```

### è©•ä¾¡
```bash
python evaluation/evaluate_models.py
```

## ğŸ“Š ä¸»è¦ãªæ”¹å–„ç‚¹

- **æ©Ÿèƒ½åˆ¥æ•´ç†**: GATã€IRLã€RLã®è¨“ç·´ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’åˆ†é›¢
- **çµ±åˆåˆ†æ**: è¤‡æ•°ã®åˆ†ææ©Ÿèƒ½ã‚’çµ±åˆã—ã€åˆ†ã‹ã‚Šã‚„ã™ã„ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ
- **ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³åŒ–**: å®Œå…¨ãªå­¦ç¿’ãƒ•ãƒ­ãƒ¼ã‚’è‡ªå‹•å®Ÿè¡Œ
- **ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°**: å„ã‚¹ãƒ†ãƒƒãƒ—ã§ã®ã‚¨ãƒ©ãƒ¼å‡¦ç†ã‚’å¼·åŒ–

## ğŸ”§ è¨­å®š

ä¸»è¦ãªè¨­å®šã¯ `configs/base_training.yaml` ã§ç®¡ç†ã•ã‚Œã¦ã„ã¾ã™ã€‚

## ğŸ“ˆ å‡ºåŠ›

- å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«: `models/`
- åˆ†æçµæœ: `outputs/`
- ãƒ­ã‚°: `logs/`
"""

    readme_file = Path("README_NEW_STRUCTURE.md")
    readme_file.write_text(readme_content)
    print(f"âœ… READMEä½œæˆ: {readme_file}")


def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ"""
    print("ğŸ”„ Kazoo ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ§‹é€ æ•´ç†å®Ÿè¡Œ")
    print(f"ğŸ“… å®Ÿè¡Œæ—¥æ™‚: {datetime.now()}")
    print("=" * 60)

    try:
        create_new_directory_structure()
        move_and_rename_files()
        create_unified_irl_analysis()
        create_unified_pipeline()
        update_import_paths()
        create_readme()

        print("\\nğŸ‰ ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ§‹é€ æ•´ç†å®Œäº†!")
        print("\\nğŸ“‹ ç¢ºèªäº‹é …:")
        print("1. æ–°ã—ã„æ§‹é€ ã§ã®ãƒ•ã‚¡ã‚¤ãƒ«é…ç½®ç¢ºèª")
        print("2. ã‚¤ãƒ³ãƒãƒ¼ãƒˆãƒ‘ã‚¹ã®å‹•ä½œç¢ºèª")
        print("3. ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å‹•ä½œãƒ†ã‚¹ãƒˆ")
        print("\\nğŸ’¡ å¤ã„ãƒ•ã‚¡ã‚¤ãƒ«ã¯æ®‹ã—ã¦ã‚ã‚Šã¾ã™ï¼ˆãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã¨ã—ã¦ï¼‰")

    except Exception as e:
        print(f"âŒ ã‚¨ãƒ©ãƒ¼: {e}")
        import traceback

        traceback.print_exc()


if __name__ == "__main__":
    main()
