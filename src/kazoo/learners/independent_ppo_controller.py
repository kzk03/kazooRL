import os

import numpy as np
import torch
from gymnasium import spaces
from tqdm import tqdm

from kazoo.learners.ppo_agent import PPOAgent


class RolloutStorage:
    """
    PPOã®ãŸã‚ã®çµŒé¨“ãƒ‡ãƒ¼ã‚¿ã‚’ä¸€æ™‚çš„ã«ä¿å­˜ã—ã€ãƒªã‚¿ãƒ¼ãƒ³ã¨ã‚¢ãƒ‰ãƒãƒ³ãƒ†ãƒ¼ã‚¸ã‚’è¨ˆç®—ã™ã‚‹ã€‚
    """

    def __init__(self, num_steps, obs_space, act_space, device):
        self.device = device
        self.num_steps = num_steps

        # è¤‡åˆè¦³æ¸¬ç©ºé–“ã®å ´åˆã¯å…¨ä½“ã®æ¬¡å…ƒã‚’è¨ˆç®—
        if hasattr(obs_space, 'spaces'):  # Dict space
            # å…¨è¦³æ¸¬ç©ºé–“ã®åˆè¨ˆæ¬¡å…ƒã‚’è¨ˆç®—
            total_dim = 0
            for space in obs_space.spaces.values():
                if hasattr(space, 'shape') and space.shape:
                    total_dim += space.shape[0]
            obs_shape = (total_dim,)
        else:
            obs_shape = obs_space.shape
        
        # ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ã‚’(ã‚¹ãƒ†ãƒƒãƒ—æ•°, æ¬¡å…ƒ)ã®å½¢ã§åˆæœŸåŒ–
        self.obs = torch.zeros((num_steps,) + obs_shape).to(device)
        self.actions = torch.zeros(num_steps, 1, dtype=torch.long).to(device)
        self.log_probs = torch.zeros(num_steps, 1).to(device)
        self.rewards = torch.zeros(num_steps, 1).to(device)
        self.dones = torch.zeros(num_steps, 1).to(device)
        self.values = torch.zeros(num_steps, 1).to(device)
        self.step = 0

    def add(self, obs, action, log_prob, reward, done, value):
        """
        ã‚¹ãƒ†ãƒƒãƒ—ã”ã¨ã®ãƒ‡ãƒ¼ã‚¿ã‚’è¿½åŠ ã™ã‚‹ã€‚
        å…¨ã¦ã®ãƒ‡ãƒ¼ã‚¿ã‚’ã€ä¿å­˜å…ˆã®å½¢çŠ¶ã§ã‚ã‚‹[1]ã«åˆã‚ã›ã‚‹ã€‚
        """
        # è¤‡åˆè¦³æ¸¬ã®å ´åˆã¯å¹³å¦åŒ–ã—ã¦çµåˆ
        if isinstance(obs, dict):
            obs_tensor = torch.cat([torch.as_tensor(v, device=self.device).flatten() 
                                  for v in obs.values()], dim=0)
        else:
            obs_tensor = torch.as_tensor(obs, device=self.device)
        
        self.obs[self.step].copy_(obs_tensor)
        self.actions[self.step].copy_(
            torch.as_tensor([action], device=self.device, dtype=torch.long)
        )
        self.log_probs[self.step].copy_(log_prob.view(1))
        self.rewards[self.step].copy_(
            torch.as_tensor([reward], device=self.device, dtype=torch.float32)
        )
        self.dones[self.step].copy_(
            torch.as_tensor([done], device=self.device, dtype=torch.float32)
        )
        self.values[self.step].copy_(value.view(1))
        self.step = (self.step + 1) % self.num_steps

    def compute_returns(self, next_value, gamma, gae_lambda):
        """GAE (Generalized Advantage Estimation) ã‚’ä½¿ã£ã¦ãƒªã‚¿ãƒ¼ãƒ³ã¨ã‚¢ãƒ‰ãƒãƒ³ãƒ†ãƒ¼ã‚¸ã‚’è¨ˆç®—"""
        self.advantages = torch.zeros_like(self.rewards).to(self.device)
        last_gae_lam = 0
        for t in reversed(range(self.num_steps)):
            if t == self.num_steps - 1:
                next_non_terminal = 1.0 - self.dones[t]
                next_values = next_value
            else:
                next_non_terminal = 1.0 - self.dones[t]
                next_values = self.values[t + 1]

            delta = (
                self.rewards[t]
                + gamma * next_values * next_non_terminal
                - self.values[t]
            )
            self.advantages[t] = last_gae_lam = (
                delta + gamma * gae_lambda * next_non_terminal * last_gae_lam
            )
        self.returns = self.advantages + self.values


class IndependentPPOController:
    """è¤‡æ•°ã®PPOAgentã‚’ç®¡ç†ã—ã€ãƒãƒ«ãƒã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆå­¦ç¿’ã‚’å®Ÿè¡Œã™ã‚‹å¸ä»¤å¡”"""

    def __init__(self, env, config):
        self.env = env
        self.agent_ids = env.agent_ids
        self.config = config
        self.num_agents = len(self.agent_ids)
        self.device = torch.device(config.get("device", "cpu"))

        try:
            self.rl_config = config.rl
        except Exception:
            raise ValueError("Configuration file must contain an 'rl' section.")

        self.agents = {}
        self.storages = {}
        for agent_id in self.agent_ids:
            obs_space = self.env.observation_space[agent_id]
            act_space = self.env.action_space[agent_id]
            
            # ãƒ‡ãƒãƒƒã‚°æƒ…å ±ã‚’è¿½åŠ 
            print(f"Agent {agent_id}: obs_space = {obs_space}, act_space = {act_space}")
            
            if obs_space is None:
                raise ValueError(f"Observation space for agent {agent_id} is None")
            if act_space is None:
                raise ValueError(f"Action space for agent {agent_id} is None")
            
            # è¾æ›¸å½¢å¼ã®è¦³æ¸¬ç©ºé–“ã‹ã‚‰æ¬¡å…ƒæ•°ã‚’è¨ˆç®—
            if hasattr(obs_space, 'spaces'):  # Dict space
                total_obs_dim = 0
                for space_name, space in obs_space.spaces.items():
                    if hasattr(space, 'shape'):
                        total_obs_dim += space.shape[0]
                    else:
                        print(f"Warning: Space {space_name} has no shape attribute")
                print(f"Total observation dimension for agent {agent_id}: {total_obs_dim}")
            else:  # Box space
                total_obs_dim = obs_space.shape[0]

            self.agents[agent_id] = PPOAgent(
                obs_dim=total_obs_dim,  # è¦³æ¸¬æ¬¡å…ƒæ•°ã‚’ç›´æ¥æ¸¡ã™
                act_space=act_space,
                lr=self.rl_config.learning_rate,
                gamma=self.rl_config.gamma,
                epochs=self.rl_config.k_epochs,
                eps_clip=self.rl_config.eps_clip,
                device=self.device,
            )
            self.storages[agent_id] = RolloutStorage(
                self.rl_config.rollout_len, obs_space, act_space, self.device
            )

    def learn(self, total_timesteps):
        print("Starting Multi-Agent PPO Training...")
        obs, info = self.env.reset()
        global_step = 0

        # GNNã‚ªãƒ³ãƒ©ã‚¤ãƒ³å­¦ç¿’ã®è¨­å®š
        gnn_update_frequency = getattr(self.config.irl, "gnn_update_frequency", 50)
        online_gnn_learning = getattr(self.config.irl, "online_gnn_learning", False)

        print(
            f"GNN Online Learning: {'Enabled' if online_gnn_learning else 'Disabled'}"
        )
        if online_gnn_learning:
            print(f"GNN Update Frequency: Every {gnn_update_frequency} steps")

        num_updates = (
            int(total_timesteps / self.rl_config.rollout_len / self.num_agents)
            if self.num_agents > 0
            else 0
        )

        # PPOå­¦ç¿’ã®é€²æ—ãƒãƒ¼
        update_progress = tqdm(
            range(1, num_updates + 1),
            desc="ğŸ¤– PPO å­¦ç¿’",
            unit="update",
            colour='magenta',
            leave=True
        )

        for update in update_progress:
            # ãƒ­ãƒ¼ãƒ«ã‚¢ã‚¦ãƒˆåé›†ã®é€²æ—ãƒãƒ¼
            rollout_progress = tqdm(
                range(self.rl_config.rollout_len),
                desc=f"Update {update:4d}/{num_updates}",
                unit="step",
                leave=False,
                colour='yellow'
            )
            
            for step in rollout_progress:
                global_step += self.num_agents
                actions_dict, log_probs_dict, values_dict = {}, {}, {}

                with torch.no_grad():
                    for agent_id, agent_obs in obs.items():
                        action, log_prob, _, value = self.agents[
                            agent_id
                        ].get_action_and_value(agent_obs)
                        actions_dict[agent_id] = action.item()
                        log_probs_dict[agent_id] = log_prob
                        values_dict[agent_id] = value

                next_obs, rewards, terminateds, truncateds, infos = self.env.step(
                    actions_dict
                )
                dones = {
                    agent_id: terminateds[agent_id] or truncateds[agent_id]
                    for agent_id in self.agent_ids
                }

                for agent_id in self.agent_ids:
                    self.storages[agent_id].add(
                        obs[agent_id],
                        actions_dict[agent_id],
                        log_probs_dict[agent_id],
                        rewards[agent_id],
                        dones[agent_id],
                        values_dict[agent_id],
                    )
                obs = next_obs

                # ç¾åœ¨ã®å¹³å‡å ±é…¬ã‚’è¨ˆç®—
                current_rewards = list(rewards.values())
                avg_reward = np.mean(current_rewards) if current_rewards else 0.0
                rollout_progress.set_postfix({
                    "Step": f"{global_step:,}",
                    "Avg_Reward": f"{avg_reward:.3f}"
                })

            with torch.no_grad():
                next_values = {
                    agent_id: self.agents[agent_id].get_action_and_value(obs[agent_id])[
                        3
                    ]
                    for agent_id in self.agent_ids
                }

            for agent_id in self.agent_ids:
                self.storages[agent_id].compute_returns(
                    next_values[agent_id],
                    self.rl_config.gamma,
                    self.rl_config.gae_lambda,
                )
                rollout_data = self.storages[agent_id]
                self.agents[agent_id].update(rollout_data)

            # GNNã®ã‚ªãƒ³ãƒ©ã‚¤ãƒ³å­¦ç¿’æ›´æ–°
            if online_gnn_learning and global_step % gnn_update_frequency == 0:
                print(f"\nğŸ”„ [Global Step {global_step}] GNNæ›´æ–°ãƒã‚§ãƒƒã‚¯ä¸­...")
                self._trigger_gnn_update(global_step)

            # é€²æ—ãƒãƒ¼ã®ãƒ¡ã‚¤ãƒ³æƒ…å ±æ›´æ–°
            if self.storages:
                avg_reward = np.mean(
                    [
                        storage.rewards.mean().item()
                        for storage in self.storages.values()
                    ]
                )
                
                update_progress.set_postfix({
                    "Global_Step": f"{global_step:,}",
                    "Avg_Reward": f"{avg_reward:.4f}",
                    "Agents": self.num_agents
                })
                
                # è©³ç´°ãƒ­ã‚°ã¯å°‘ãªã„é »åº¦ã§å‡ºåŠ›
                if update % 50 == 0:
                    print(
                        f"\nUpdate {update}/{num_updates}, Global Step: {global_step:,}, Avg Reward: {avg_reward:.3f}"
                    )

        print("\nğŸ‰ Training finished.")
        print(f"ğŸ”¢ Total Global Steps: {global_step:,}")
        print(f"ğŸ¤– Total Agents: {self.num_agents}")
        print(f"ğŸ¯ Total Updates: {num_updates}")
        
        # æœ€çµ‚çµ±è¨ˆ
        if self.storages:
            final_avg_reward = np.mean(
                [storage.rewards.mean().item() for storage in self.storages.values()]
            )
            print(f"ğŸ“Š Final Average Reward: {final_avg_reward:.4f}")

    def _trigger_gnn_update(self, global_step):
        """GNNã®ã‚ªãƒ³ãƒ©ã‚¤ãƒ³å­¦ç¿’æ›´æ–°ã‚’ãƒˆãƒªã‚¬ãƒ¼"""
        try:
            # ç’°å¢ƒã®ç‰¹å¾´é‡æŠ½å‡ºå™¨ã«ã‚¢ã‚¯ã‚»ã‚¹
            if hasattr(self.env, "feature_extractor") and hasattr(
                self.env.feature_extractor, "gnn_extractor"
            ):
                gnn_extractor = self.env.feature_extractor.gnn_extractor
                if gnn_extractor and gnn_extractor.online_learning:
                    print(f"\nğŸ”„ [Step {global_step}] GNNã‚ªãƒ³ãƒ©ã‚¤ãƒ³å­¦ç¿’æ›´æ–°ã‚’å®Ÿè¡Œä¸­...")

                    # ãƒãƒƒãƒ•ã‚¡ã®å†…å®¹ã‚’ãƒã‚§ãƒƒã‚¯
                    buffer_size = len(gnn_extractor.interaction_buffer)
                    if buffer_size > 0:
                        print(f"  ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ã‚·ãƒ§ãƒ³ãƒãƒƒãƒ•ã‚¡ã‚µã‚¤ã‚º: {buffer_size}")

                        # GNNæ›´æ–°å®Ÿè¡Œ
                        gnn_extractor._update_gnn_online()

                        # çµ±è¨ˆæƒ…å ±è¡¨ç¤º
                        gnn_extractor.print_statistics()

                        # å®šæœŸçš„ã«ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜ï¼ˆä¾‹ï¼š100ã‚¹ãƒ†ãƒƒãƒ—ã”ã¨ï¼‰
                        if (
                            global_step
                            % (
                                100
                                * getattr(self.config.irl, "gnn_update_frequency", 50)
                            )
                            == 0
                        ):
                            gnn_extractor.save_updated_model(
                                f"data/gnn_model_step_{global_step}.pt"
                            )
                            print(
                                f"  âœ… GNNãƒ¢ãƒ‡ãƒ«ä¿å­˜: gnn_model_step_{global_step}.pt"
                            )
                    else:
                        print("  ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ã‚·ãƒ§ãƒ³ãƒãƒƒãƒ•ã‚¡ãŒç©ºã®ãŸã‚ã€GNNæ›´æ–°ã‚’ã‚¹ã‚­ãƒƒãƒ—")
                else:
                    print(f"  GNNã‚ªãƒ³ãƒ©ã‚¤ãƒ³å­¦ç¿’ãŒç„¡åŠ¹åŒ–ã•ã‚Œã¦ã„ã¾ã™")
        except Exception as e:
            print(f"  âŒ GNNæ›´æ–°ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")

    def save_models(self, directory):
        if not os.path.exists(directory):
            os.makedirs(directory)
        for agent_id, agent in self.agents.items():
            agent.save(os.path.join(directory, f"ppo_agent_{agent_id}.pth"))
