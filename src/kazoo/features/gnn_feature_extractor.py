import sys
from pathlib import Path

import torch
import torch.nn.functional as F

# GNNãƒ¢ãƒ‡ãƒ«ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
sys.path.append(str(Path(__file__).resolve().parents[2]))
from kazoo.GAT.GAT_model import GNNModel


class GNNFeatureExtractor:
    """IRLã®ãŸã‚ã®GNNãƒ™ãƒ¼ã‚¹ã®ç‰¹å¾´é‡æŠ½å‡ºå™¨"""

    def __init__(self, cfg):
        self.cfg = cfg
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

        # çµ±è¨ˆã‚«ã‚¦ãƒ³ã‚¿
        self.stats = {
            "total_requests": 0,
            "full_features": 0,
            "missing_dev": 0,
            "missing_task": 0,
            "missing_both": 0,
            "errors": 0,
            "updates": 0,  # GNNæ›´æ–°å›æ•°ã‚’è¿½åŠ 
        }

        # ã‚ªãƒ³ãƒ©ã‚¤ãƒ³å­¦ç¿’ç”¨ã®è¨­å®š
        self.online_learning = cfg.irl.get("online_gnn_learning", False)
        self.update_frequency = cfg.irl.get("gnn_update_frequency", 100)  # Nå›ã«1å›æ›´æ–°
        self.learning_rate = cfg.irl.get("gnn_learning_rate", 0.001)
        self.time_window_hours = cfg.irl.get(
            "gnn_time_window_hours", 24
        )  # å­¦ç¿’ã«ä½¿ç”¨ã™ã‚‹æ™‚é–“çª“ï¼ˆæ™‚é–“ï¼‰
        self.optimizer = None

        # æ–°ã—ã„ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿ã‚’è“„ç©
        self.interaction_buffer = []
        self.max_buffer_size = cfg.irl.get("gnn_buffer_size", 1000)

        if getattr(cfg.irl, "use_gnn", False):
            self._load_gnn_model()
        else:
            self.model = None
            self.embeddings = None

    def _load_gnn_model(self):
        """GNNãƒ¢ãƒ‡ãƒ«ã¨ã‚°ãƒ©ãƒ•ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿"""
        try:
            # ã‚°ãƒ©ãƒ•ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
            graph_path = Path(getattr(self.cfg.irl, "gnn_graph_path", "data/graph.pt"))
            if not graph_path.exists():
                print(f"Warning: GNN graph file not found: {graph_path}")
                self.model = None
                return

            self.graph_data = torch.load(graph_path, weights_only=False)

            # ğŸ†• é–‹ç™ºè€…å”åŠ›ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚‚èª­ã¿è¾¼ã¿
            network_path = Path("data/developer_collaboration_network.pt")
            if network_path.exists():
                print("Loading developer collaboration network...")
                self.dev_network = torch.load(network_path, weights_only=False)
                print(
                    f"âœ… Developer network loaded: {self.dev_network['num_developers']} devs, {self.dev_network['network_stats']['num_edges']} edges"
                )
            else:
                print(
                    "Warning: Developer collaboration network not found, using basic features only"
                )
                self.dev_network = None

            # ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿
            model_path = Path(
                getattr(self.cfg.irl, "gnn_model_path", "data/gnn_model.pt")
            )
            if not model_path.exists():
                print(f"Warning: GNN model file not found: {model_path}")
                self.model = None
                return

            # Import GNN model here to avoid circular imports
            from kazoo.GAT.GAT_model import GNNModel

            self.model = GNNModel(
                in_channels_dict={"dev": 8, "task": 9}, out_channels=32
            )
            self.model.load_state_dict(torch.load(model_path, weights_only=True))
            self.model.eval()
            self.model.to(self.device)

            # åŸ‹ã‚è¾¼ã¿ã‚’äº‹å‰è¨ˆç®—ï¼ˆå”åŠ›ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’å«ã‚€ï¼‰
            edge_index_dict = {
                ("dev", "writes", "task"): self.graph_data[
                    ("dev", "writes", "task")
                ].edge_index,
                ("task", "written_by", "dev"): self.graph_data[
                    ("dev", "writes", "task")
                ].edge_index.flip([0]),
            }

            # ğŸ†• é–‹ç™ºè€…å”åŠ›é–¢ä¿‚ã‚¨ãƒƒã‚¸ã‚’è¿½åŠ 
            if self.dev_network is not None:
                edge_index_dict[("dev", "collaborates", "dev")] = self.dev_network[
                    "dev_collaboration_edge_index"
                ]

            with torch.no_grad():
                self.embeddings = self.model(self.graph_data.x_dict, edge_index_dict)

            # ãƒãƒ¼ãƒ‰IDãƒãƒƒãƒ”ãƒ³ã‚°ã‚’ä½œæˆ
            self._create_id_mappings()

            # ã‚ªãƒ³ãƒ©ã‚¤ãƒ³å­¦ç¿’ç”¨ã®ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ãƒ¼ã‚’åˆæœŸåŒ–
            if self.online_learning:
                from torch.optim import Adam

                self.optimizer = Adam(self.model.parameters(), lr=self.learning_rate)
                print(
                    f"âœ… Online GNN learning enabled (update every {self.update_frequency} steps)"
                )

            print("âœ… GNN model loaded successfully")

        except Exception as e:
            print(f"Error loading GNN model: {e}")
            self.model = None
            self.embeddings = None

    def _create_id_mappings(self):
        """ãƒãƒ¼ãƒ‰IDã¨åŸ‹ã‚è¾¼ã¿ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®ãƒãƒƒãƒ”ãƒ³ã‚°ã‚’ä½œæˆ"""
        if hasattr(self.graph_data["dev"], "node_id"):
            if isinstance(self.graph_data["dev"].node_id, list):
                dev_ids = self.graph_data["dev"].node_id
            else:
                dev_ids = self.graph_data["dev"].node_id.tolist()
        else:
            dev_ids = list(range(self.embeddings["dev"].shape[0]))

        if hasattr(self.graph_data["task"], "node_id"):
            if isinstance(self.graph_data["task"].node_id, list):
                task_ids = self.graph_data["task"].node_id
            else:
                task_ids = self.graph_data["task"].node_id.tolist()
        else:
            task_ids = list(range(self.embeddings["task"].shape[0]))

        # ID â†’ ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®ãƒãƒƒãƒ”ãƒ³ã‚°
        self.dev_id_to_idx = {str(dev_id): idx for idx, dev_id in enumerate(dev_ids)}
        self.task_id_to_idx = {
            str(task_id): idx for idx, task_id in enumerate(task_ids)
        }

        print(
            f"ID mappings created: {len(self.dev_id_to_idx)} devs, {len(self.task_id_to_idx)} tasks"
        )

    def get_gnn_features(self, task, developer, env):
        """GNNãƒ™ãƒ¼ã‚¹ã®ç‰¹å¾´é‡ã‚’å–å¾—ï¼ˆ3æ¬¡å…ƒã®ã‚¹ã‚³ã‚¢ã®ã¿ï¼‰"""
        self.stats["total_requests"] += 1

        if not self.model or not self.embeddings:
            # å”åŠ›ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ãŒåˆ©ç”¨å¯èƒ½ã‹ã©ã†ã‹ã§ç‰¹å¾´é‡æ•°ã‚’æ±ºå®š
            num_features = 5 if self.dev_network is not None else 3
            return [0.0] * num_features

        try:
            # é–‹ç™ºè€…IDã¨ã‚¿ã‚¹ã‚¯IDã‚’å–å¾—
            dev_id = str(developer.get("name", ""))
            task_id = str(task.id)

            # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’å–å¾—
            dev_idx = self.dev_id_to_idx.get(dev_id)
            task_idx = self.task_id_to_idx.get(task_id)

            # Missing node handling with more informative approach
            missing_dev = dev_idx is None
            missing_task = task_idx is None

            # å”åŠ›ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ãŒåˆ©ç”¨å¯èƒ½ã‹ã©ã†ã‹ã§ç‰¹å¾´é‡æ•°ã‚’æ±ºå®š
            num_features = 5 if self.dev_network is not None else 3

            if missing_dev and missing_task:
                # Both missing - return zero features
                self.stats["missing_both"] += 1
                return [0.0] * num_features
            elif missing_dev:
                # Developer missing - use average developer embedding as fallback
                self.stats["missing_dev"] += 1
                return self._get_fallback_features_missing_dev(task_idx, dev_id)
            elif missing_task:
                # Task missing - use average task embedding as fallback
                self.stats["missing_task"] += 1
                return self._get_fallback_features_missing_task(dev_idx, task_id)
            else:
                # Both nodes exist - compute full features
                self.stats["full_features"] += 1
                return self._get_simplified_gnn_features(dev_idx, task_idx)

        except Exception as e:
            self.stats["errors"] += 1
            print(
                f"Error extracting GNN features for dev={dev_id}, task={task_id}: {e}"
            )
            num_features = 5 if self.dev_network is not None else 3
            return [0.0] * num_features

    def record_interaction(
        self, task, developer, reward, action_taken=None, simulation_time=None
    ):
        """æ–°ã—ã„ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¨˜éŒ²ï¼ˆå¼·åŒ–å­¦ç¿’ã®ã‚¹ãƒ†ãƒƒãƒ—ã”ã¨ã«å‘¼ã³å‡ºã•ã‚Œã‚‹ï¼‰"""
        if not self.online_learning:
            return

        # å®Ÿéš›ã®ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³æ™‚é–“ã‚’è¨˜éŒ²
        from datetime import datetime

        if simulation_time is None:
            timestamp = datetime.now()
        else:
            timestamp = simulation_time

        interaction = {
            "dev_id": str(developer.get("name", "")),
            "task_id": str(task.id),
            "reward": float(reward),
            "timestamp": timestamp,
            "simulation_time": timestamp,
            "step_number": len(self.interaction_buffer),
            "action_taken": action_taken,  # ã‚¢ã‚µã‚¤ãƒ³ã•ã‚ŒãŸã‹ã©ã†ã‹ç­‰
        }

        self.interaction_buffer.append(interaction)

        # ãƒãƒƒãƒ•ã‚¡ã‚µã‚¤ã‚ºã‚’åˆ¶é™
        if len(self.interaction_buffer) > self.max_buffer_size:
            self.interaction_buffer.pop(0)  # å¤ã„ã‚‚ã®ã‹ã‚‰å‰Šé™¤

        # å®šæœŸçš„ã«GNNã‚’æ›´æ–°
        if len(self.interaction_buffer) % self.update_frequency == 0:
            self._update_gnn_online()

    def _update_gnn_online(self):
        """è“„ç©ã•ã‚ŒãŸã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿ã§GNNã‚’ã‚ªãƒ³ãƒ©ã‚¤ãƒ³æ›´æ–°ï¼ˆæ™‚ç³»åˆ—è€ƒæ…®ï¼‰"""
        if (
            not self.online_learning
            or not self.optimizer
            or len(self.interaction_buffer) < 5
        ):
            return

        # æœ€æ–°ã®æ™‚é–“ã‚’å–å¾—
        if not self.interaction_buffer:
            return

        latest_time = max(
            interaction["simulation_time"] for interaction in self.interaction_buffer
        )
        print(f"ğŸ”„ GNNã‚’ã‚ªãƒ³ãƒ©ã‚¤ãƒ³æ›´æ–°ä¸­... (æœ€æ–°æ™‚åˆ»: {latest_time})")

        # æ™‚é–“çª“å†…ã®ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ã‚·ãƒ§ãƒ³ã®ã¿ã‚’ä½¿ç”¨
        from datetime import timedelta

        time_window = timedelta(hours=self.time_window_hours)
        cutoff_time = latest_time - time_window

        recent_interactions = [
            interaction
            for interaction in self.interaction_buffer
            if interaction["simulation_time"] >= cutoff_time
        ]

        print(
            f"  æ™‚é–“çª“å†…ã®ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ã‚·ãƒ§ãƒ³: {len(recent_interactions)}/{len(self.interaction_buffer)}"
        )
        print(f"  æ™‚é–“ç¯„å›²: {cutoff_time} ï½ {latest_time}")

        if len(recent_interactions) < 3:
            print("  æ™‚é–“çª“å†…ã®ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ã‚·ãƒ§ãƒ³æ•°ãŒä¸è¶³ã€ã‚¹ã‚­ãƒƒãƒ—")
            return

        try:
            self.model.train()
            self.optimizer.zero_grad()

            # æ™‚é–“çª“å†…ã®ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ã‚·ãƒ§ãƒ³ã‹ã‚‰æ­£è² ã®ãƒšã‚¢ã‚’ç”Ÿæˆ
            positive_pairs = []
            negative_pairs = []

            # å ±é…¬ã«åŸºã¥ã„ã¦ãƒã‚¸ãƒ†ã‚£ãƒ–/ãƒã‚¬ãƒ†ã‚£ãƒ–ãƒšã‚¢ã‚’ä½œæˆï¼ˆæ™‚é–“çª“å†…ã®ã¿ï¼‰
            for interaction in recent_interactions:
                dev_id = interaction["dev_id"]
                task_id = interaction["task_id"]
                reward = interaction["reward"]

                dev_idx = self.dev_id_to_idx.get(dev_id)
                task_idx = self.task_id_to_idx.get(task_id)

                # ãƒ‡ãƒãƒƒã‚°æƒ…å ±
                print(
                    f"  ãƒã‚§ãƒƒã‚¯: {dev_id} (idx={dev_idx}) + {task_id} (idx={task_idx}) = {reward}"
                )

                if dev_idx is not None and task_idx is not None:
                    if reward > 0:  # è‰¯ã„ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ã‚·ãƒ§ãƒ³
                        positive_pairs.append((dev_idx, task_idx, reward))
                        print(f"    â†’ ãƒã‚¸ãƒ†ã‚£ãƒ–ãƒšã‚¢è¿½åŠ ")
                    elif reward < 0:  # æ‚ªã„ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ã‚·ãƒ§ãƒ³
                        negative_pairs.append((dev_idx, task_idx, abs(reward)))
                        print(f"    â†’ ãƒã‚¬ãƒ†ã‚£ãƒ–ãƒšã‚¢è¿½åŠ ")
                else:
                    print(f"    â†’ ã‚¹ã‚­ãƒƒãƒ—ï¼ˆãƒãƒ¼ãƒ‰ãªã—ï¼‰")

            print(
                f"  æœ‰åŠ¹ãªãƒšã‚¢: ãƒã‚¸ãƒ†ã‚£ãƒ–={len(positive_pairs)}, ãƒã‚¬ãƒ†ã‚£ãƒ–={len(negative_pairs)}"
            )

            if len(positive_pairs) == 0 and len(negative_pairs) == 0:
                print("  æœ‰åŠ¹ãªãƒšã‚¢ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
                return

            # ç¾åœ¨ã®åŸ‹ã‚è¾¼ã¿ã‚’å–å¾—
            current_embeddings = self.model(
                self.graph_data.x_dict, self.graph_data.edge_index_dict
            )

            total_loss = torch.tensor(0.0, device=self.device, requires_grad=True)
            loss_terms = []

            # ãƒã‚¸ãƒ†ã‚£ãƒ–ãƒšã‚¢ã®æå¤±ï¼ˆé¡ä¼¼åº¦ã‚’é«˜ã‚ã‚‹ï¼‰
            if positive_pairs:
                for dev_idx, task_idx, weight in positive_pairs:
                    dev_emb = current_embeddings["dev"][dev_idx]
                    task_emb = current_embeddings["task"][task_idx]

                    similarity = F.cosine_similarity(
                        dev_emb.unsqueeze(0), task_emb.unsqueeze(0)
                    )
                    # é¡ä¼¼åº¦ã‚’é«˜ã‚ãŸã„ã®ã§ã€è² ã®æå¤±ï¼ˆæœ€å¤§åŒ–ï¼‰
                    pos_loss = -weight * torch.log(torch.sigmoid(similarity) + 1e-8)
                    loss_terms.append(pos_loss)
                    print(f"    ãƒã‚¸ãƒ†ã‚£ãƒ–æå¤±: {pos_loss.item():.4f}")

            # ãƒã‚¬ãƒ†ã‚£ãƒ–ãƒšã‚¢ã®æå¤±ï¼ˆé¡ä¼¼åº¦ã‚’ä¸‹ã’ã‚‹ï¼‰
            if negative_pairs:
                for dev_idx, task_idx, weight in negative_pairs:
                    dev_emb = current_embeddings["dev"][dev_idx]
                    task_emb = current_embeddings["task"][task_idx]

                    similarity = F.cosine_similarity(
                        dev_emb.unsqueeze(0), task_emb.unsqueeze(0)
                    )
                    # é¡ä¼¼åº¦ã‚’ä¸‹ã’ãŸã„ã®ã§ã€æ­£ã®æå¤±ï¼ˆæœ€å°åŒ–ï¼‰
                    neg_loss = weight * torch.log(torch.sigmoid(similarity) + 1e-8)
                    loss_terms.append(neg_loss)
                    print(f"    ãƒã‚¬ãƒ†ã‚£ãƒ–æå¤±: {neg_loss.item():.4f}")

            # å…¨ã¦ã®æå¤±ã‚’åˆè¨ˆ
            if loss_terms:
                total_loss = torch.stack(loss_terms).sum()

            # æ­£å‰‡åŒ–é …ï¼ˆåŸ‹ã‚è¾¼ã¿ãŒå¤§ãããªã‚Šã™ããªã„ã‚ˆã†ã«ï¼‰
            reg_loss = 0.001 * (
                torch.norm(current_embeddings["dev"])
                + torch.norm(current_embeddings["task"])
            )
            total_loss = total_loss + reg_loss

            print(f"  ç·æå¤±: {total_loss.item():.4f}")

            # ãƒãƒƒã‚¯ãƒ—ãƒ­ãƒ‘ã‚²ãƒ¼ã‚·ãƒ§ãƒ³
            if total_loss.requires_grad:
                total_loss.backward()
                self.optimizer.step()

                # åŸ‹ã‚è¾¼ã¿ã‚’å†è¨ˆç®—
                self.model.eval()
                with torch.no_grad():
                    self.embeddings = self.model(
                        self.graph_data.x_dict, self.graph_data.edge_index_dict
                    )

                self.stats["updates"] += 1
                print(
                    f"  âœ… GNNæ›´æ–°å®Œäº† (æå¤±: {total_loss.item():.4f}, æ›´æ–°å›æ•°: {self.stats['updates']})"
                )
            else:
                print("  æå¤±ã«å‹¾é…ãŒã‚ã‚Šã¾ã›ã‚“")

        except Exception as e:
            print(f"  âŒ GNNæ›´æ–°ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
            import traceback

            traceback.print_exc()

        finally:
            self.model.eval()

    def add_new_nodes(self, new_developers=None, new_tasks=None):
        """æ–°ã—ã„é–‹ç™ºè€…ã‚„ã‚¿ã‚¹ã‚¯ãƒãƒ¼ãƒ‰ã‚’GNNã«è¿½åŠ """
        if not self.online_learning:
            return

        print("ğŸ†• æ–°ã—ã„ãƒãƒ¼ãƒ‰ã‚’GNNã«è¿½åŠ ä¸­...")

        try:
            # æ–°ã—ã„é–‹ç™ºè€…ãƒãƒ¼ãƒ‰ã‚’è¿½åŠ 
            if new_developers:
                for dev_name, dev_profile in new_developers.items():
                    if dev_name not in self.dev_id_to_idx:
                        # æ–°ã—ã„é–‹ç™ºè€…ã®ç‰¹å¾´é‡ã‚’è¨ˆç®—
                        dev_features = self._compute_dev_features(dev_name, dev_profile)

                        # ã‚°ãƒ©ãƒ•ãƒ‡ãƒ¼ã‚¿ã«è¿½åŠ 
                        new_dev_tensor = torch.tensor(
                            [dev_features], dtype=torch.float, device=self.device
                        )
                        self.graph_data["dev"].x = torch.cat(
                            [self.graph_data["dev"].x, new_dev_tensor], dim=0
                        )

                        # ID ãƒãƒƒãƒ”ãƒ³ã‚°ã‚’æ›´æ–°
                        new_idx = len(self.dev_id_to_idx)
                        self.dev_id_to_idx[dev_name] = new_idx
                        self.graph_data["dev"].node_id.append(dev_name)

                        print(f"  âœ… æ–°ã—ã„é–‹ç™ºè€…è¿½åŠ : {dev_name} (index: {new_idx})")

            # æ–°ã—ã„ã‚¿ã‚¹ã‚¯ãƒãƒ¼ãƒ‰ã‚’è¿½åŠ 
            if new_tasks:
                for task_id, task_info in new_tasks.items():
                    if task_id not in self.task_id_to_idx:
                        # æ–°ã—ã„ã‚¿ã‚¹ã‚¯ã®ç‰¹å¾´é‡ã‚’è¨ˆç®—
                        task_features = self._compute_task_features(task_id, task_info)

                        # ã‚°ãƒ©ãƒ•ãƒ‡ãƒ¼ã‚¿ã«è¿½åŠ 
                        new_task_tensor = torch.tensor(
                            [task_features], dtype=torch.float, device=self.device
                        )
                        self.graph_data["task"].x = torch.cat(
                            [self.graph_data["task"].x, new_task_tensor], dim=0
                        )

                        # ID ãƒãƒƒãƒ”ãƒ³ã‚°ã‚’æ›´æ–°
                        new_idx = len(self.task_id_to_idx)
                        self.task_id_to_idx[task_id] = new_idx
                        self.graph_data["task"].node_id.append(task_id)

                        print(f"  âœ… æ–°ã—ã„ã‚¿ã‚¹ã‚¯è¿½åŠ : {task_id} (index: {new_idx})")

            # åŸ‹ã‚è¾¼ã¿ã‚’å†è¨ˆç®—
            if new_developers or new_tasks:
                with torch.no_grad():
                    self.embeddings = self.model(
                        self.graph_data.x_dict, self.graph_data.edge_index_dict
                    )
                print(f"  âœ… åŸ‹ã‚è¾¼ã¿å†è¨ˆç®—å®Œäº†")

        except Exception as e:
            print(f"  âŒ ãƒãƒ¼ãƒ‰è¿½åŠ ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")

    def _compute_dev_features(self, dev_name, dev_profile):
        """æ–°ã—ã„é–‹ç™ºè€…ã®ç‰¹å¾´é‡ã‚’è¨ˆç®—"""
        return [
            len(dev_profile.get("skills", [])),
            len(dev_profile.get("touched_files", [])),
            0,  # åˆæœŸã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ã‚·ãƒ§ãƒ³æ•°
            0,  # åˆæœŸå‚åŠ ã‚¿ã‚¹ã‚¯æ•°
            0,  # åˆæœŸæ‰±ã£ãŸãƒ©ãƒ™ãƒ«æ•°
            dev_profile.get("label_affinity", {}).get("bug", 0.0),
            dev_profile.get("label_affinity", {}).get("enhancement", 0.0),
            dev_profile.get("label_affinity", {}).get("documentation", 0.0),
        ]

    def _compute_task_features(self, task_id, task_info):
        """æ–°ã—ã„ã‚¿ã‚¹ã‚¯ã®ç‰¹å¾´é‡ã‚’è¨ˆç®—"""
        labels = task_info.get("labels", [])
        return [
            len(task_info.get("title", "")),
            len(task_info.get("body", "")),
            len(labels),
            1 if "bug" in labels else 0,
            1 if "enhancement" in labels else 0,
            1 if "documentation" in labels else 0,
            0,  # åˆæœŸã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ã‚·ãƒ§ãƒ³æ•°
            0,  # åˆæœŸé–¢ä¸é–‹ç™ºè€…æ•°
            task_info.get("body", "").count("```") // 2,
        ]

    def _get_simplified_gnn_features(self, dev_idx, task_idx):
        """ç°¡ç•¥åŒ–ã•ã‚ŒãŸGNNç‰¹å¾´é‡ï¼ˆåŸºæœ¬3æ¬¡å…ƒ + å”åŠ›ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ç‰¹å¾´é‡ï¼‰"""
        # åŸ‹ã‚è¾¼ã¿ã‚’å–å¾—
        dev_emb = self.embeddings["dev"][dev_idx]
        task_emb = self.embeddings["task"][task_idx]

        # ç‰¹å¾´é‡ã‚’è¨ˆç®—
        features = []

        # 1. é¡ä¼¼åº¦ã‚¹ã‚³ã‚¢
        similarity = F.cosine_similarity(
            dev_emb.unsqueeze(0), task_emb.unsqueeze(0)
        ).item()
        features.append(similarity)

        # 2. é–‹ç™ºè€…ã®å°‚é–€æ€§ã‚¹ã‚³ã‚¢
        all_task_sims = F.cosine_similarity(
            dev_emb.unsqueeze(0), self.embeddings["task"]
        )
        dev_expertise = torch.mean(
            torch.topk(all_task_sims, k=min(10, all_task_sims.size(0))).values
        ).item()
        features.append(dev_expertise)

        # 3. ã‚¿ã‚¹ã‚¯ã®äººæ°—åº¦ã‚¹ã‚³ã‚¢
        all_dev_sims = F.cosine_similarity(
            task_emb.unsqueeze(0), self.embeddings["dev"]
        )
        task_popularity = torch.mean(
            torch.topk(all_dev_sims, k=min(10, all_dev_sims.size(0))).values
        ).item()
        features.append(task_popularity)

        # ğŸ†• 4. å”åŠ›ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ç‰¹å¾´é‡ï¼ˆåˆ©ç”¨å¯èƒ½ãªå ´åˆï¼‰
        if self.dev_network is not None:
            # å”åŠ›å¼·åº¦ã‚¹ã‚³ã‚¢ï¼ˆé–‹ç™ºè€…ã®å”åŠ›ã‚¨ãƒƒã‚¸ã®é‡ã¿åˆè¨ˆï¼‰
            collab_strength = self._calculate_collaboration_strength(dev_idx)
            features.append(collab_strength)

            # ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ä¸­å¿ƒæ€§ã‚¹ã‚³ã‚¢ï¼ˆé–‹ç™ºè€…ã®ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯å†…ã§ã®é‡è¦åº¦ï¼‰
            centrality = self._calculate_network_centrality(dev_idx)
            features.append(centrality)

        return features

    def _calculate_collaboration_strength(self, dev_idx):
        """é–‹ç™ºè€…ã®å”åŠ›ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯å†…ã§ã®å¼·åº¦ã‚’è¨ˆç®—"""
        try:
            if self.dev_network is None:
                return 0.0

            edge_index = self.dev_network["dev_collaboration_edge_index"]
            edge_weights = self.dev_network["dev_collaboration_edge_weights"]

            # ã“ã®é–‹ç™ºè€…ãŒé–¢ä¸ã™ã‚‹ã‚¨ãƒƒã‚¸ã‚’è¦‹ã¤ã‘ã‚‹
            dev_edges = (edge_index[0] == dev_idx) | (edge_index[1] == dev_idx)
            if not dev_edges.any():
                return 0.0

            # é–¢é€£ã™ã‚‹ã‚¨ãƒƒã‚¸ã®é‡ã¿ã®åˆè¨ˆã‚’è¨ˆç®—
            strength = edge_weights[dev_edges].sum().item()
            # æ­£è¦åŒ–ï¼ˆæœ€å¤§å¼·åº¦ã§å‰²ã‚‹ï¼‰
            max_strength = (
                edge_weights.max().item() if edge_weights.numel() > 0 else 1.0
            )
            return strength / max_strength if max_strength > 0 else 0.0

        except Exception as e:
            print(f"Error calculating collaboration strength: {e}")
            return 0.0

    def _calculate_network_centrality(self, dev_idx):
        """é–‹ç™ºè€…ã®ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ä¸­å¿ƒæ€§ã‚’è¨ˆç®—ï¼ˆç°¡æ˜“ç‰ˆï¼‰"""
        try:
            if self.dev_network is None:
                return 0.0

            edge_index = self.dev_network["dev_collaboration_edge_index"]

            # ã“ã®é–‹ç™ºè€…ã®æ¬¡æ•°ï¼ˆæ¥ç¶šæ•°ï¼‰ã‚’è¨ˆç®—
            degree = (
                ((edge_index[0] == dev_idx) | (edge_index[1] == dev_idx)).sum().item()
            )

            # æœ€å¤§æ¬¡æ•°ã§æ­£è¦åŒ–
            total_devs = self.dev_network["num_developers"]
            max_possible_degree = total_devs - 1
            return degree / max_possible_degree if max_possible_degree > 0 else 0.0

        except Exception as e:
            print(f"Error calculating network centrality: {e}")
            return 0.0

    def _get_full_gnn_features(self, dev_idx, task_idx):
        """ä¸¡æ–¹ã®ãƒãƒ¼ãƒ‰ãŒå­˜åœ¨ã™ã‚‹å ´åˆã®å®Œå…¨ãªGNNç‰¹å¾´é‡ã‚’è¨ˆç®—"""
        # åŸ‹ã‚è¾¼ã¿ã‚’å–å¾—
        dev_emb = self.embeddings["dev"][dev_idx]
        task_emb = self.embeddings["task"][task_idx]

        # ç‰¹å¾´é‡ã‚’è¨ˆç®—
        features = []

        # 1. é¡ä¼¼åº¦ã‚¹ã‚³ã‚¢
        similarity = F.cosine_similarity(
            dev_emb.unsqueeze(0), task_emb.unsqueeze(0)
        ).item()
        features.append(similarity)

        # 2. é–‹ç™ºè€…ã®å°‚é–€æ€§ã‚¹ã‚³ã‚¢
        all_task_sims = F.cosine_similarity(
            dev_emb.unsqueeze(0), self.embeddings["task"]
        )
        dev_expertise = torch.mean(
            torch.topk(all_task_sims, k=min(10, all_task_sims.size(0))).values
        ).item()
        features.append(dev_expertise)

        # 3. ã‚¿ã‚¹ã‚¯ã®äººæ°—åº¦ã‚¹ã‚³ã‚¢
        all_dev_sims = F.cosine_similarity(
            task_emb.unsqueeze(0), self.embeddings["dev"]
        )
        task_popularity = torch.mean(
            torch.topk(all_dev_sims, k=min(10, all_dev_sims.size(0))).values
        ).item()
        features.append(task_popularity)

        # 4. é–‹ç™ºè€…åŸ‹ã‚è¾¼ã¿ï¼ˆ32æ¬¡å…ƒï¼‰
        features.extend(dev_emb.tolist())

        return features

    def _get_fallback_features_missing_dev(self, task_idx, dev_id):
        """é–‹ç™ºè€…ãƒãƒ¼ãƒ‰ãŒå­˜åœ¨ã—ãªã„å ´åˆã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç‰¹å¾´é‡"""
        # ã‚¿ã‚¹ã‚¯ã¯å­˜åœ¨ã™ã‚‹ã®ã§ã€å¹³å‡çš„ãªé–‹ç™ºè€…ã¨ã®æ¯”è¼ƒã‚’ä½¿ç”¨
        task_emb = self.embeddings["task"][task_idx]
        avg_dev_emb = torch.mean(self.embeddings["dev"], dim=0)

        features = []

        # 1. å¹³å‡é–‹ç™ºè€…ã¨ã®é¡ä¼¼åº¦ï¼ˆä½ã‚ã®å€¤ã«ãªã‚‹ã¨äºˆæƒ³ï¼‰
        similarity = (
            F.cosine_similarity(avg_dev_emb.unsqueeze(0), task_emb.unsqueeze(0)).item()
            * 0.5
        )  # ãƒšãƒŠãƒ«ãƒ†ã‚£ã¨ã—ã¦åŠåˆ†ã«ã™ã‚‹
        features.append(similarity)

        # 2. å¹³å‡çš„ãªå°‚é–€æ€§ã‚¹ã‚³ã‚¢ï¼ˆä¸­ç¨‹åº¦ã®å€¤ï¼‰
        features.append(0.3)

        # 3. ã‚¿ã‚¹ã‚¯ã®äººæ°—åº¦ã¯è¨ˆç®—å¯èƒ½
        all_dev_sims = F.cosine_similarity(
            task_emb.unsqueeze(0), self.embeddings["dev"]
        )
        task_popularity = torch.mean(
            torch.topk(all_dev_sims, k=min(10, all_dev_sims.size(0))).values
        ).item()
        features.append(task_popularity)

        # ğŸ†• 4-5. å”åŠ›ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ç‰¹å¾´é‡ï¼ˆåˆ©ç”¨å¯èƒ½ãªå ´åˆï¼‰
        if self.dev_network is not None:
            # é–‹ç™ºè€…ãŒå­˜åœ¨ã—ãªã„ã®ã§ã€å”åŠ›ç‰¹å¾´é‡ã¯0
            features.append(0.0)  # collaboration_strength
            features.append(0.0)  # network_centrality

        return features

    def _get_fallback_features_missing_task(self, dev_idx, task_id):
        """ã‚¿ã‚¹ã‚¯ãƒãƒ¼ãƒ‰ãŒå­˜åœ¨ã—ãªã„å ´åˆã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç‰¹å¾´é‡"""
        # é–‹ç™ºè€…ã¯å­˜åœ¨ã™ã‚‹ã®ã§ã€å¹³å‡çš„ãªã‚¿ã‚¹ã‚¯ã¨ã®æ¯”è¼ƒã‚’ä½¿ç”¨
        dev_emb = self.embeddings["dev"][dev_idx]
        avg_task_emb = torch.mean(self.embeddings["task"], dim=0)

        features = []

        # 1. å¹³å‡ã‚¿ã‚¹ã‚¯ã¨ã®é¡ä¼¼åº¦ï¼ˆä½ã‚ã®å€¤ã«ãªã‚‹ã¨äºˆæƒ³ï¼‰
        similarity = (
            F.cosine_similarity(dev_emb.unsqueeze(0), avg_task_emb.unsqueeze(0)).item()
            * 0.5
        )  # ãƒšãƒŠãƒ«ãƒ†ã‚£ã¨ã—ã¦åŠåˆ†ã«ã™ã‚‹
        features.append(similarity)

        # 2. é–‹ç™ºè€…ã®å°‚é–€æ€§ã¯è¨ˆç®—å¯èƒ½
        all_task_sims = F.cosine_similarity(
            dev_emb.unsqueeze(0), self.embeddings["task"]
        )
        dev_expertise = torch.mean(
            torch.topk(all_task_sims, k=min(10, all_task_sims.size(0))).values
        ).item()
        features.append(dev_expertise)

        # 3. å¹³å‡çš„ãªäººæ°—åº¦ã‚¹ã‚³ã‚¢ï¼ˆä¸­ç¨‹åº¦ã®å€¤ï¼‰
        features.append(0.3)

        # ğŸ†• 4-5. å”åŠ›ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ç‰¹å¾´é‡ï¼ˆåˆ©ç”¨å¯èƒ½ãªå ´åˆï¼‰
        if self.dev_network is not None:
            # é–‹ç™ºè€…ãŒå­˜åœ¨ã™ã‚‹ã®ã§å”åŠ›ç‰¹å¾´é‡ã‚’è¨ˆç®—
            collab_strength = self._calculate_collaboration_strength(dev_idx)
            features.append(collab_strength)

            centrality = self._calculate_network_centrality(dev_idx)
            features.append(centrality)

        return features

    def get_feature_names(self):
        """GNNç‰¹å¾´é‡ã®åå‰ãƒªã‚¹ãƒˆã‚’è¿”ã™"""
        if not self.model:
            return []

        base_features = ["gnn_similarity", "gnn_dev_expertise", "gnn_task_popularity"]

        # ğŸ†• å”åŠ›ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ãŒåˆ©ç”¨å¯èƒ½ãªå ´åˆã€è¿½åŠ ã®ç‰¹å¾´é‡ã‚’å«ã‚ã‚‹
        if self.dev_network is not None:
            base_features.extend(
                [
                    "gnn_collaboration_strength",  # é–‹ç™ºè€…ã®å”åŠ›ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯å†…ã§ã®é‡è¦åº¦
                    "gnn_network_centrality",  # ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯å†…ã§ã®ä¸­å¿ƒæ€§
                ]
            )

        return base_features

    def print_statistics(self):
        """GNNç‰¹å¾´é‡æŠ½å‡ºã®çµ±è¨ˆã‚’è¡¨ç¤º"""
        total = self.stats["total_requests"]
        if total == 0:
            print("No GNN feature requests yet.")
            return

        print(f"\n=== GNN Feature Extraction Statistics ===")
        print(f"Total requests: {total}")
        print(
            f"Full features (both nodes found): {self.stats['full_features']} ({self.stats['full_features']/total*100:.1f}%)"
        )
        print(
            f"Missing developer: {self.stats['missing_dev']} ({self.stats['missing_dev']/total*100:.1f}%)"
        )
        print(
            f"Missing task: {self.stats['missing_task']} ({self.stats['missing_task']/total*100:.1f}%)"
        )
        print(
            f"Missing both: {self.stats['missing_both']} ({self.stats['missing_both']/total*100:.1f}%)"
        )
        print(f"Errors: {self.stats['errors']} ({self.stats['errors']/total*100:.1f}%)")

        if self.online_learning:
            print(f"Online learning enabled: True")
            print(f"GNN updates performed: {self.stats['updates']}")
            print(f"Interaction buffer size: {len(self.interaction_buffer)}")
            print(f"Update frequency: every {self.update_frequency} requests")
        else:
            print(f"Online learning enabled: False")

        print("=" * 45)

    def save_updated_model(self, save_path=None):
        """æ›´æ–°ã•ã‚ŒãŸGNNãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜"""
        if not self.model:
            print("ä¿å­˜ã™ã‚‹ãƒ¢ãƒ‡ãƒ«ãŒã‚ã‚Šã¾ã›ã‚“")
            return

        if save_path is None:
            save_path = "data/gnn_model_online_updated.pt"

        try:
            torch.save(self.model.state_dict(), save_path)
            print(f"ğŸ’¾ æ›´æ–°ã•ã‚ŒãŸGNNãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜: {save_path}")

            # ã‚°ãƒ©ãƒ•ãƒ‡ãƒ¼ã‚¿ã‚‚ä¿å­˜
            graph_save_path = save_path.replace("model", "graph")
            torch.save(self.graph_data.cpu(), graph_save_path)
            print(f"ğŸ’¾ æ›´æ–°ã•ã‚ŒãŸã‚°ãƒ©ãƒ•ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜: {graph_save_path}")

        except Exception as e:
            print(f"âŒ ãƒ¢ãƒ‡ãƒ«ä¿å­˜ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")

    def reset_interaction_buffer(self):
        """ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ã‚·ãƒ§ãƒ³ãƒãƒƒãƒ•ã‚¡ã‚’ãƒªã‚»ãƒƒãƒˆ"""
        self.interaction_buffer.clear()
        print("ğŸ”„ ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ã‚·ãƒ§ãƒ³ãƒãƒƒãƒ•ã‚¡ã‚’ãƒªã‚»ãƒƒãƒˆã—ã¾ã—ãŸ")
