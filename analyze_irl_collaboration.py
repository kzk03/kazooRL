#!/usr/bin/env python3
"""
é€†å¼·åŒ–å­¦ç¿’ï¼ˆIRLï¼‰ã§å­¦ç¿’ã—ãŸå ±é…¬é‡ã¿ã‚’åˆ†æã—ã€
å”åŠ›é–¢ä¿‚ç‰¹å¾´é‡ã®å½±éŸ¿ã‚’è©³ã—ãèª¿ã¹ã‚‹ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
"""

import json
from collections import defaultdict
from pathlib import Path

import matplotlib.pyplot as plt
import numpy as np
import yaml


def analyze_irl_weights():
    """IRLå­¦ç¿’æ¸ˆã¿é‡ã¿ã®åˆ†æ"""
    
    print("=== IRLå ±é…¬é‡ã¿åˆ†æï¼ˆå”åŠ›é–¢ä¿‚ç‰¹å¾´é‡ä¸­å¿ƒï¼‰ ===\n")
    
    # å­¦ç¿’æ¸ˆã¿é‡ã¿ã‚’èª­ã¿è¾¼ã¿
    weights_files = [
        "data/learned_weights.npy",
        "data/learned_reward_weights.npy"
    ]
    
    latest_weights = None
    for weights_file in weights_files:
        if Path(weights_file).exists():
            weights = np.load(weights_file)
            print(f"âœ… é‡ã¿ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿: {weights_file}")
            print(f"   é‡ã¿ãƒ™ã‚¯ãƒˆãƒ«å½¢çŠ¶: {weights.shape}")
            print(f"   é‡ã¿ã®ç¯„å›²: [{weights.min():.4f}, {weights.max():.4f}]")
            latest_weights = weights
            break
    
    if latest_weights is None:
        print("âŒ å­¦ç¿’æ¸ˆã¿é‡ã¿ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        return
    
    # ç‰¹å¾´é‡åã®å®šç¾©ï¼ˆFeatureExtractorã¨åŒã˜é †åºï¼‰
    all_labels = ["bug", "enhancement", "documentation", "question", "help wanted"]
    
    feature_names = []
    feature_names.extend([
        "task_days_since_last_activity",
        "task_discussion_activity", 
        "task_text_length",
        "task_code_block_count"
    ])
    feature_names.extend([f"task_label_{label}" for label in all_labels])
    feature_names.extend([
        "dev_recent_activity_count",
        "dev_current_workload", 
        "dev_total_lines_changed"
    ])
    # ç¤¾ä¼šçš„ã¤ãªãŒã‚Šç‰¹å¾´é‡
    feature_names.extend([
        "dev_collaboration_network_size",
        "dev_comment_interactions", 
        "dev_cross_issue_activity"
    ])
    # å”åŠ›é–¢ä¿‚ç‰¹å¾´é‡
    feature_names.extend([
        "match_collaborated_with_task_author",
        "match_collaborator_overlap_count",
        "match_has_prior_collaboration"
    ])
    feature_names.extend([
        "match_skill_intersection_count",
        "match_file_experience_count"
    ])
    feature_names.extend([f"match_affinity_for_{label}" for label in all_labels])
    
    # GNNç‰¹å¾´é‡ï¼ˆå”åŠ›ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯å¯¾å¿œï¼‰
    feature_names.extend([
        "gnn_similarity",
        "gnn_dev_expertise", 
        "gnn_task_popularity",
        "gnn_collaboration_strength",
        "gnn_network_centrality"
    ])
    
    print(f"\\nç‰¹å¾´é‡æ•°: {len(feature_names)}, é‡ã¿æ•°: {len(latest_weights)}")
    
    if len(feature_names) != len(latest_weights):
        print(f"âŒ ç‰¹å¾´é‡æ•°ã¨é‡ã¿æ•°ãŒä¸€è‡´ã—ã¾ã›ã‚“")
        # åˆ©ç”¨å¯èƒ½ãªåˆ†ã ã‘åˆ†æ
        min_len = min(len(feature_names), len(latest_weights))
        feature_names = feature_names[:min_len]
        latest_weights = latest_weights[:min_len]
        print(f"   æœ€åˆã®{min_len}å€‹ã®ç‰¹å¾´é‡ã§åˆ†æã‚’ç¶šè¡Œ")
    
    # å”åŠ›é–¢ä¿‚ç‰¹å¾´é‡ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ç‰¹å®š
    collaboration_indices = []
    collaboration_names = []
    for i, name in enumerate(feature_names):
        if any(keyword in name for keyword in [
            "collaboration_network_size", "comment_interactions", "cross_issue_activity",
            "collaborated_with_task_author", "collaborator_overlap_count", "has_prior_collaboration"
        ]):
            collaboration_indices.append(i)
            collaboration_names.append(name)
    
    print(f"\\n1. å”åŠ›é–¢ä¿‚ç‰¹å¾´é‡ã®é‡ã¿åˆ†æ:")
    print(f"   å”åŠ›é–¢ä¿‚ç‰¹å¾´é‡æ•°: {len(collaboration_indices)}")
    
    for idx, name in zip(collaboration_indices, collaboration_names):
        weight = latest_weights[idx]
        print(f"   {name}: {weight:.6f}")
    
    # é‡ã¿ã®çµ¶å¯¾å€¤ã§ãƒ©ãƒ³ã‚­ãƒ³ã‚°
    print(f"\\n2. å…¨ç‰¹å¾´é‡ã®é‡è¦åº¦ãƒ©ãƒ³ã‚­ãƒ³ã‚°ï¼ˆçµ¶å¯¾å€¤ï¼‰:")
    weight_abs = np.abs(latest_weights)
    sorted_indices = np.argsort(weight_abs)[::-1]
    
    print("   TOP 10:")
    for rank, idx in enumerate(sorted_indices[:10], 1):
        weight = latest_weights[idx]
        is_collab = idx in collaboration_indices
        collab_mark = " ğŸ¤" if is_collab else ""
        print(f"   {rank:2d}. {feature_names[idx]:<40} {weight:8.6f}{collab_mark}")
    
    # å”åŠ›é–¢ä¿‚ç‰¹å¾´é‡ã®é †ä½
    print(f"\\n3. å”åŠ›é–¢ä¿‚ç‰¹å¾´é‡ã®é †ä½:")
    for idx, name in zip(collaboration_indices, collaboration_names):
        rank = np.where(sorted_indices == idx)[0][0] + 1
        weight = latest_weights[idx]
        print(f"   {rank:2d}ä½: {name:<40} {weight:8.6f}")
    
    # ã‚«ãƒ†ã‚´ãƒªåˆ¥é‡ã¿åˆ†æ
    analyze_weights_by_category(latest_weights, feature_names)
    
    # å¯è¦–åŒ–
    create_weight_visualization(latest_weights, feature_names, collaboration_indices)

def analyze_weights_by_category(weights, feature_names):
    """ã‚«ãƒ†ã‚´ãƒªåˆ¥ã®é‡ã¿åˆ†æ"""
    
    print(f"\\n4. ã‚«ãƒ†ã‚´ãƒªåˆ¥é‡ã¿åˆ†æ:")
    
    categories = {
        "ã‚¿ã‚¹ã‚¯ç‰¹å¾´": [],
        "é–‹ç™ºè€…ç‰¹å¾´": [],
        "ç¤¾ä¼šçš„ã¤ãªãŒã‚Š": [],
        "å”åŠ›é–¢ä¿‚": [],
        "ãƒãƒƒãƒãƒ³ã‚°": []
    }
    
    for i, name in enumerate(feature_names):
        if name.startswith("task_"):
            categories["ã‚¿ã‚¹ã‚¯ç‰¹å¾´"].append(weights[i])
        elif name.startswith("dev_") and not any(keyword in name for keyword in [
            "collaboration_network_size", "comment_interactions", "cross_issue_activity"
        ]):
            categories["é–‹ç™ºè€…ç‰¹å¾´"].append(weights[i])
        elif any(keyword in name for keyword in [
            "collaboration_network_size", "comment_interactions", "cross_issue_activity"
        ]):
            categories["ç¤¾ä¼šçš„ã¤ãªãŒã‚Š"].append(weights[i])
        elif any(keyword in name for keyword in [
            "collaborated_with_task_author", "collaborator_overlap_count", "has_prior_collaboration"
        ]):
            categories["å”åŠ›é–¢ä¿‚"].append(weights[i])
        elif name.startswith("match_"):
            categories["ãƒãƒƒãƒãƒ³ã‚°"].append(weights[i])
    
    for category, cat_weights in categories.items():
        if cat_weights:
            mean_weight = np.mean(np.abs(cat_weights))
            max_weight = np.max(np.abs(cat_weights))
            print(f"   {category:<12}: å¹³å‡é‡ã¿={mean_weight:.6f}, æœ€å¤§é‡ã¿={max_weight:.6f}, å€‹æ•°={len(cat_weights)}")

def create_weight_visualization(weights, feature_names, collaboration_indices):
    """é‡ã¿ã®å¯è¦–åŒ–"""
    
    print(f"\\n5. é‡ã¿ã®å¯è¦–åŒ–ã‚’ä½œæˆä¸­...")
    
    # å”åŠ›é–¢ä¿‚ç‰¹å¾´é‡ã‚’ãƒã‚¤ãƒ©ã‚¤ãƒˆ
    colors = ['red' if i in collaboration_indices else 'blue' for i in range(len(weights))]
    
    plt.figure(figsize=(15, 8))
    
    # é‡ã¿ã®æ£’ã‚°ãƒ©ãƒ•
    plt.subplot(2, 1, 1)
    bars = plt.bar(range(len(weights)), weights, color=colors, alpha=0.7)
    plt.title('IRLå­¦ç¿’æ¸ˆã¿é‡ã¿ï¼ˆå”åŠ›é–¢ä¿‚ç‰¹å¾´é‡ã‚’èµ¤ã§ãƒã‚¤ãƒ©ã‚¤ãƒˆï¼‰')
    plt.xlabel('ç‰¹å¾´é‡ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹')
    plt.ylabel('é‡ã¿')
    plt.axhline(y=0, color='black', linestyle='-', alpha=0.3)
    
    # å‡¡ä¾‹
    plt.bar([], [], color='red', alpha=0.7, label='å”åŠ›é–¢ä¿‚ç‰¹å¾´é‡')
    plt.bar([], [], color='blue', alpha=0.7, label='ãã®ä»–ã®ç‰¹å¾´é‡')
    plt.legend()
    
    # å”åŠ›é–¢ä¿‚ç‰¹å¾´é‡ã®ã¿ã®æ‹¡å¤§è¡¨ç¤º
    plt.subplot(2, 1, 2)
    collab_weights = [weights[i] for i in collaboration_indices]
    collab_names = [feature_names[i] for i in collaboration_indices]
    
    bars = plt.bar(range(len(collab_weights)), collab_weights, color='red', alpha=0.7)
    plt.title('å”åŠ›é–¢ä¿‚ç‰¹å¾´é‡ã®é‡ã¿è©³ç´°')
    plt.xlabel('å”åŠ›é–¢ä¿‚ç‰¹å¾´é‡')
    plt.ylabel('é‡ã¿')
    plt.xticks(range(len(collab_names)), [name.replace('match_', '').replace('dev_', '') for name in collab_names], 
               rotation=45, ha='right')
    plt.axhline(y=0, color='black', linestyle='-', alpha=0.3)
    
    # é‡ã¿ã®å€¤ã‚’ãƒãƒ¼ã®ä¸Šã«è¡¨ç¤º
    for i, (bar, weight) in enumerate(zip(bars, collab_weights)):
        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.001 * np.sign(weight), 
                f'{weight:.4f}', ha='center', va='bottom' if weight >= 0 else 'top', fontsize=9)
    
    plt.tight_layout()
    plt.savefig('irl_collaboration_weights_analysis.png', dpi=300, bbox_inches='tight')
    print(f"   ğŸ’¾ ã‚°ãƒ©ãƒ•ã‚’ä¿å­˜: irl_collaboration_weights_analysis.png")
    
    # çµ±è¨ˆã‚µãƒãƒªãƒ¼
    print(f"\\n6. å”åŠ›é–¢ä¿‚ç‰¹å¾´é‡ã®çµ±è¨ˆ:")
    collab_weights = [weights[i] for i in collaboration_indices]
    print(f"   å¹³å‡é‡ã¿: {np.mean(collab_weights):.6f}")
    print(f"   é‡ã¿æ¨™æº–åå·®: {np.std(collab_weights):.6f}")
    print(f"   æ­£ã®é‡ã¿ã®æ•°: {sum(1 for w in collab_weights if w > 0)}")
    print(f"   è² ã®é‡ã¿ã®æ•°: {sum(1 for w in collab_weights if w < 0)}")
    print(f"   æœ€å¤§é‡ã¿: {np.max(collab_weights):.6f}")
    print(f"   æœ€å°é‡ã¿: {np.min(collab_weights):.6f}")

def analyze_collaboration_impact():
    """å”åŠ›é–¢ä¿‚ç‰¹å¾´é‡ã®å®Ÿéš›ã®å½±éŸ¿ã‚’åˆ†æ"""
    
    print(f"\\n=== å”åŠ›é–¢ä¿‚ç‰¹å¾´é‡ã®å®Ÿéš›ã®å½±éŸ¿åˆ†æ ===\\n")
    
    # é–‹ç™ºè€…ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿
    with open('configs/dev_profiles.yaml', 'r') as f:
        profiles = yaml.safe_load(f)
    
    # ãƒãƒƒã‚¯ãƒ­ã‚°ã‚’èª­ã¿è¾¼ã¿
    with open('data/backlog.json', 'r') as f:
        backlog = json.load(f)
    
    print(f"1. ãƒ‡ãƒ¼ã‚¿çµ±è¨ˆ:")
    print(f"   é–‹ç™ºè€…æ•°: {len(profiles)}")
    print(f"   ã‚¿ã‚¹ã‚¯æ•°: {len(backlog)}")
    
    # å”åŠ›é–¢ä¿‚ãŒæ¨è–¦ã«ä¸ãˆã‚‹å½±éŸ¿ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
    collaboration_impact_examples = []
    
    # ã„ãã¤ã‹ã®é–‹ç™ºè€…ã¨ã‚¿ã‚¹ã‚¯ã®çµ„ã¿åˆã‚ã›ã§å”åŠ›é–¢ä¿‚ç‰¹å¾´é‡ã‚’è¨ˆç®—
    sample_devs = list(profiles.keys())[:5]  # æœ€åˆã®5äººã®é–‹ç™ºè€…
    sample_tasks = backlog[:3]  # æœ€åˆã®3ã¤ã®ã‚¿ã‚¹ã‚¯
    
    print(f"\\n2. å”åŠ›é–¢ä¿‚ç‰¹å¾´é‡ã®å®Ÿä¾‹è¨ˆç®—:")
    
    for dev_name in sample_devs:
        dev_profile = profiles[dev_name]
        collaborators = set(dev_profile.get('collaborators', []))
        
        print(f"\\n  é–‹ç™ºè€…: {dev_name}")
        print(f"    å”åŠ›è€…æ•°: {len(collaborators)}")
        print(f"    å”åŠ›è€…: {', '.join(list(collaborators)[:3])}{'...' if len(collaborators) > 3 else ''}")
        
        for task_idx, task in enumerate(sample_tasks):
            task_author = task.get('user', {}).get('login') if 'user' in task else None
            task_assignees = task.get('assignees', []) if 'assignees' in task else []
            assignee_logins = {assignee.get('login') for assignee in task_assignees if assignee.get('login')}
            
            # å”åŠ›é–¢ä¿‚ç‰¹å¾´é‡ã‚’è¨ˆç®—
            collaborated_with_author = 1.0 if task_author and task_author in collaborators else 0.0
            collaborator_overlap_count = len(assignee_logins.intersection(collaborators))
            
            task_related_devs = assignee_logins.copy()
            if task_author:
                task_related_devs.add(task_author)
            has_prior_collaboration = 1.0 if len(task_related_devs.intersection(collaborators)) > 0 else 0.0
            
            print(f"    ã‚¿ã‚¹ã‚¯{task_idx+1}:")
            print(f"      ä½œæˆè€…ã¨ã®å”åŠ›å±¥æ­´: {collaborated_with_author}")
            print(f"      æ‹…å½“è€…ã¨ã®é‡è¤‡æ•°: {collaborator_overlap_count}")
            print(f"      é–¢é€£è€…ã¨ã®å”åŠ›æœ‰ç„¡: {has_prior_collaboration}")

if __name__ == "__main__":
    analyze_irl_weights()
    analyze_collaboration_impact()
