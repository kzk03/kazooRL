#!/usr/bin/env python3
"""
å®Ÿéš›ã®é–‹ç™ºè€…è¡Œå‹•ã¨RLã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®äºˆæ¸¬æ¯”è¼ƒè©•ä¾¡

å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ãŒå®Ÿéš›ã®é–‹ç™ºè€…ã®é¸æŠã‚’ã©ã®ç¨‹åº¦äºˆæ¸¬ã§ãã‚‹ã‹ã‚’è©•ä¾¡ã™ã‚‹
"""

import argparse
import json
# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
import sys
from collections import Counter, defaultdict
from datetime import datetime
from pathlib import Path

import numpy as np
import pandas as pd
import torch
import yaml
# Stable-Baselines3
from stable_baselines3 import PPO

sys.path.append("/Users/kazuki-h/rl/kazoo")
sys.path.append("/Users/kazuki-h/rl/kazoo/src")

from scripts.train_simple_unified_rl import SimpleTaskAssignmentEnv
from src.kazoo.envs.task import Task


class BehavioralComparison:
    """å®Ÿéš›ã®é–‹ç™ºè€…è¡Œå‹•ã¨RLã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®äºˆæ¸¬æ¯”è¼ƒ"""

    def __init__(self, config_path, model_path, test_data_path):
        """
        Args:
            config_path: è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
            model_path: å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ãƒ‘ã‚¹
            test_data_path: ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ãƒ‘ã‚¹
        """
        self.config_path = config_path
        self.model_path = model_path
        self.test_data_path = test_data_path

        # è¨­å®šèª­ã¿è¾¼ã¿
        import yaml

        with open(config_path, "r", encoding="utf-8") as f:
            self.config = yaml.safe_load(f)

        # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
        with open(test_data_path, "r", encoding="utf-8") as f:
            all_data = json.load(f)

        # 2023å¹´ã®ãƒ‡ãƒ¼ã‚¿ã®ã¿æŠ½å‡ºï¼ˆãƒ†ã‚¹ãƒˆç”¨ï¼‰
        self.test_data = []
        for task in all_data:
            created_at = task.get("created_at", "")
            if created_at.startswith("2023"):
                self.test_data.append(task)

        print(f"ğŸ“Š å…¨ãƒ‡ãƒ¼ã‚¿: {len(all_data):,} ã‚¿ã‚¹ã‚¯")
        print(f"ğŸ“Š 2023å¹´ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿: {len(self.test_data):,} ã‚¿ã‚¹ã‚¯")

        # ç’°å¢ƒåˆæœŸåŒ–
        self._setup_environment()

        # ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿
        self.model = PPO.load(model_path)
        print(f"âœ… ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿å®Œäº†: {model_path}")

        # çµæœæ ¼ç´
        self.results = {
            "predictions": [],
            "actuals": [],
            "tasks": [],
            "developers": [],
            "accuracies": {},
        }

    def _setup_environment(self):
        """ç’°å¢ƒã®åˆæœŸåŒ–"""
        print("ğŸ® ç’°å¢ƒåˆæœŸåŒ–ä¸­...")

        # é–‹ç™ºè€…ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿
        dev_profiles_path = self.config["env"]["dev_profiles_path"]
        with open(dev_profiles_path, "r", encoding="utf-8") as f:
            self.dev_profiles = yaml.safe_load(f)

        # å­¦ç¿’ã«ä½¿ç”¨ã—ãŸãƒãƒƒã‚¯ãƒ­ã‚°ãƒ‡ãƒ¼ã‚¿ã‚‚èª­ã¿è¾¼ã¿ï¼ˆé–‹ç™ºè€…ãƒ—ãƒ¼ãƒ«ç¢ºèªã®ãŸã‚ï¼‰
        backlog_path = self.config["env"]["backlog_path"]
        with open(backlog_path, "r", encoding="utf-8") as f:
            training_backlog = json.load(f)

        print(f"   å­¦ç¿’ç”¨ãƒãƒƒã‚¯ãƒ­ã‚°: {len(training_backlog):,} ã‚¿ã‚¹ã‚¯")

        # è¨­å®šã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®ä½œæˆï¼ˆdictãƒ™ãƒ¼ã‚¹ã§å±æ€§ã‚¢ã‚¯ã‚»ã‚¹å¯èƒ½ï¼‰
        class DictObj:
            def __init__(self, d):
                for k, v in d.items():
                    if isinstance(v, dict):
                        setattr(self, k, DictObj(v))
                    else:
                        setattr(self, k, v)

            def get(self, key, default=None):
                return getattr(self, key, default)

        cfg = DictObj(self.config)

        # ç’°å¢ƒä½œæˆï¼ˆå­¦ç¿’æ™‚ã®ãƒ‡ãƒ¼ã‚¿æ§‹æˆã‚’ä½¿ç”¨ï¼‰
        self.env = SimpleTaskAssignmentEnv(
            cfg=cfg,
            backlog_data=training_backlog,  # å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ã—ã¦é–‹ç™ºè€…ãƒ—ãƒ¼ãƒ«ã‚’ç¢ºå®š
            dev_profiles_data=self.dev_profiles,
        )

        print(f"   é–‹ç™ºè€…æ•°: {self.env.num_developers}")
        print(f"   ã‚¿ã‚¹ã‚¯æ•°: {len(self.env.tasks)}")
        print(f"   ç‰¹å¾´é‡æ¬¡å…ƒ: {self.env.observation_space.shape[0]}")

    def extract_actual_assignments(self):
        """å®Ÿéš›ã®é–‹ç™ºè€…å‰²ã‚Šå½“ã¦ã‚’æŠ½å‡º"""
        print("ğŸ“‹ å®Ÿéš›ã®å‰²ã‚Šå½“ã¦æŠ½å‡ºä¸­...")

        actual_assignments = {}
        assignment_stats = defaultdict(int)
        developer_stats = Counter()

        for task_data in self.test_data:
            task_id = task_data.get("id") or task_data.get("number")
            if not task_id:
                continue

            # å®Ÿéš›ã®æ‹…å½“è€…ã‚’æŠ½å‡º
            assignee = None

            # assignees ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‹ã‚‰æŠ½å‡º
            if "assignees" in task_data and task_data["assignees"]:
                assignee = task_data["assignees"][0].get("login")

            # events ã‹ã‚‰ ASSIGNED ã‚¤ãƒ™ãƒ³ãƒˆã‚’æ¢ã™
            elif "events" in task_data:
                for event in task_data["events"]:
                    if event.get("event") == "assigned" and event.get("assignee"):
                        assignee = event["assignee"].get("login")
                        break

            # comments ã‹ã‚‰ @ãƒ¡ãƒ³ã‚·ãƒ§ãƒ³ã‚’æ¢ã™ï¼ˆç°¡æ˜“ï¼‰
            elif "comments" in task_data:
                for comment in task_data["comments"]:
                    body = comment.get("body", "")
                    if "@" in body and "assign" in body.lower():
                        # ã‚ˆã‚Šè©³ç´°ãªè§£æãŒå¿…è¦
                        pass

            if assignee:
                actual_assignments[task_id] = assignee
                assignment_stats["assigned"] += 1
                developer_stats[assignee] += 1
            else:
                assignment_stats["unassigned"] += 1

        print(f"   å‰²ã‚Šå½“ã¦æ¸ˆã¿: {assignment_stats['assigned']:,} ã‚¿ã‚¹ã‚¯")
        print(f"   æœªå‰²ã‚Šå½“ã¦: {assignment_stats['unassigned']:,} ã‚¿ã‚¹ã‚¯")
        print(f"   ãƒ¦ãƒ‹ãƒ¼ã‚¯é–‹ç™ºè€…: {len(developer_stats)} äºº")

        # ä¸Šä½é–‹ç™ºè€…è¡¨ç¤º
        top_devs = developer_stats.most_common(10)
        print("   ä¸Šä½é–‹ç™ºè€…:")
        for dev, count in top_devs:
            print(f"     {dev}: {count} ã‚¿ã‚¹ã‚¯")

        return actual_assignments

    def predict_assignments(self, actual_assignments):
        """RLã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã«ã‚ˆã‚‹å‰²ã‚Šå½“ã¦äºˆæ¸¬"""
        print("ğŸ¤– RLã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆäºˆæ¸¬ä¸­...")

        predictions = {}
        prediction_scores = {}

        # å®Ÿéš›ã«å‰²ã‚Šå½“ã¦ã‚‰ã‚ŒãŸ2023å¹´ã®ã‚¿ã‚¹ã‚¯ã‚’å¯¾è±¡ã«äºˆæ¸¬
        test_tasks_with_assignments = []
        for task in self.test_data:
            task_id = task.get("id") or task.get("number")
            if task_id and task_id in actual_assignments:
                test_tasks_with_assignments.append(task)

        print(f"   äºˆæ¸¬å¯¾è±¡: {len(test_tasks_with_assignments)} ã‚¿ã‚¹ã‚¯ï¼ˆ2023å¹´ï¼‰")

        for task_data in test_tasks_with_assignments:
            try:
                # Taskã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã¨ã—ã¦Taskã‚’ä½œæˆ
                from src.kazoo.envs.task import Task

                task_obj = Task(task_data)
                task_id = (
                    task_obj.id
                    if hasattr(task_obj, "id")
                    else task_data.get("id", task_data.get("number"))
                )

                # ç’°å¢ƒã®æœ€åˆã®é–‹ç™ºè€…ã‚’ä½¿ç”¨ã—ã¦ç‰¹å¾´é‡ã‚’è¨ˆç®—
                first_dev_name = self.env.developers[0]
                first_dev_profile = self.dev_profiles.get(first_dev_name, {})
                dev_obj = {"name": first_dev_name, "profile": first_dev_profile}

                # ãƒ€ãƒŸãƒ¼ç’°å¢ƒã§ãƒ™ãƒ¼ã‚¹ç‰¹å¾´é‡ã‚’è¨ˆç®—
                dummy_env = type(
                    "DummyEnv",
                    (),
                    {
                        "backlog": self.env.tasks,
                        "dev_profiles": self.dev_profiles,
                        "assignments": {},
                        "dev_action_history": {},
                    },
                )()

                # å„é–‹ç™ºè€…ã«å¯¾ã™ã‚‹ç‰¹å¾´é‡ã¨äºˆæ¸¬ç¢ºç‡ã‚’è¨ˆç®—
                dev_predictions = []
                all_probabilities = np.zeros(len(self.env.developers))

                # ã‚¿ã‚¹ã‚¯ã«å¯¾ã™ã‚‹åŸºæœ¬ç‰¹å¾´é‡ã‚’å–å¾—ï¼ˆé–‹ç™ºè€…éä¾å­˜éƒ¨åˆ†ï¼‰
                base_features = None

                for dev_idx, dev_name in enumerate(self.env.developers):
                    dev_profile = self.dev_profiles.get(dev_name, {})
                    dev_obj = {"name": dev_name, "profile": dev_profile}

                    # ç‰¹å¾´é‡ã‚’æŠ½å‡º
                    features = self.env.feature_extractor.get_features(
                        task_obj, dev_obj, dummy_env
                    )
                    obs = features.astype(np.float32)

                    # ãƒ¢ãƒ‡ãƒ«ã§äºˆæ¸¬ï¼ˆç¢ºç‡åˆ†å¸ƒï¼‰
                    obs_tensor = torch.tensor(obs).unsqueeze(0).float()
                    with torch.no_grad():
                        logits = self.model.policy.mlp_extractor.policy_net(
                            self.model.policy.features_extractor(obs_tensor)
                        )
                        probs = torch.softmax(logits, dim=-1).numpy()[0]
                        dev_prob = probs[dev_idx] if dev_idx < len(probs) else 0.0
                        dev_predictions.append((dev_idx, dev_name, dev_prob))
                        all_probabilities[dev_idx] = dev_prob

                # æœ€é«˜ç¢ºç‡ã®é–‹ç™ºè€…ã‚’é¸æŠ
                dev_predictions.sort(key=lambda x: x[2], reverse=True)
                top_dev_idx, top_dev_name, top_prob = dev_predictions[0]

                predictions[task_id] = top_dev_name
                prediction_scores[task_id] = {
                    "top_action": top_dev_idx,
                    "top_developer": top_dev_name,
                    "top_probability": top_prob,
                    "probabilities": all_probabilities,  # å…¨é–‹ç™ºè€…ã®ç¢ºç‡
                    "top_5_actions": [p[0] for p in dev_predictions[:5]],
                    "top_5_probs": [p[2] for p in dev_predictions[:5]],
                    "top_5_developers": [p[1] for p in dev_predictions[:5]],
                }

            except Exception as e:
                print(f"âš ï¸ ã‚¿ã‚¹ã‚¯ {task_id} ã®äºˆæ¸¬ã§ã‚¨ãƒ©ãƒ¼: {e}")
                continue

        print(f"   äºˆæ¸¬å®Œäº†: {len(predictions)} ã‚¿ã‚¹ã‚¯")
        return predictions, prediction_scores

    def calculate_metrics(self, actual_assignments, predictions, prediction_scores):
        """è©•ä¾¡æŒ‡æ¨™ã®è¨ˆç®—"""
        print("ğŸ“Š è©•ä¾¡æŒ‡æ¨™è¨ˆç®—ä¸­...")

        # å…±é€šã®ã‚¿ã‚¹ã‚¯ã®ã¿è©•ä¾¡
        common_tasks = set(actual_assignments.keys()) & set(predictions.keys())
        print(f"   è©•ä¾¡å¯¾è±¡ã‚¿ã‚¹ã‚¯: {len(common_tasks)}")

        if not common_tasks:
            print("âš ï¸ è©•ä¾¡å¯èƒ½ãªã‚¿ã‚¹ã‚¯ãŒã‚ã‚Šã¾ã›ã‚“")
            return {}

        # é–‹ç™ºè€…åã®ãƒãƒƒãƒ”ãƒ³ã‚°
        all_actual_devs = set(actual_assignments.values())
        all_predicted_devs = set(predictions.values())
        all_env_devs = set(self.env.developers)

        print(f"   å®Ÿéš›ã®é–‹ç™ºè€…: {len(all_actual_devs)} äºº")
        print(f"   äºˆæ¸¬é–‹ç™ºè€…: {len(all_predicted_devs)} äºº")
        print(f"   ç’°å¢ƒã®é–‹ç™ºè€…: {len(all_env_devs)} äºº")

        # é–‹ç™ºè€…ãƒ—ãƒ¼ãƒ«ã®é‡è¤‡ãƒã‚§ãƒƒã‚¯
        overlap_devs = all_actual_devs & all_env_devs
        print(f"   é‡è¤‡ã™ã‚‹é–‹ç™ºè€…: {len(overlap_devs)} äºº")

        if overlap_devs:
            print("   é‡è¤‡é–‹ç™ºè€…:", sorted(list(overlap_devs)))

        # æ­£ç¢ºã«ä¸€è‡´ã™ã‚‹ã‚±ãƒ¼ã‚¹
        exact_matches = 0
        dev_mapping_matches = 0

        actuals = []
        predicted_actions = []

        # é‡è¤‡é–‹ç™ºè€…ã®ã‚¿ã‚¹ã‚¯ã®ã¿ã§è©•ä¾¡
        overlap_task_count = 0
        overlap_exact_matches = 0

        for task_id in common_tasks:
            actual_dev = actual_assignments[task_id]
            predicted_dev = predictions[task_id]

            # æ­£ç¢ºãªä¸€è‡´
            if actual_dev == predicted_dev:
                exact_matches += 1

            # é‡è¤‡é–‹ç™ºè€…ã®ã‚¿ã‚¹ã‚¯ã§ã®è©•ä¾¡
            if actual_dev in overlap_devs:
                overlap_task_count += 1
                if actual_dev == predicted_dev:
                    overlap_exact_matches += 1

            # é–‹ç™ºè€…ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãƒ™ãƒ¼ã‚¹ã®è©•ä¾¡
            try:
                actual_idx = self.env.developers.index(actual_dev)
                predicted_idx = self.env.developers.index(predicted_dev)

                actuals.append(actual_idx)
                predicted_actions.append(predicted_idx)

                if actual_idx == predicted_idx:
                    dev_mapping_matches += 1

            except ValueError:
                # ç’°å¢ƒã«å­˜åœ¨ã—ãªã„é–‹ç™ºè€…ã®å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
                continue

        metrics = {}

        # åŸºæœ¬çš„ãªç²¾åº¦æŒ‡æ¨™
        metrics["exact_accuracy"] = exact_matches / len(common_tasks)
        metrics["mapping_accuracy"] = (
            dev_mapping_matches / len(actuals) if actuals else 0
        )

        # é‡è¤‡é–‹ç™ºè€…ã§ã®ç²¾åº¦ï¼ˆã‚ˆã‚Šå…¬å¹³ãªè©•ä¾¡ï¼‰
        if overlap_task_count > 0:
            metrics["overlap_accuracy"] = overlap_exact_matches / overlap_task_count
            print(
                f"   é‡è¤‡é–‹ç™ºè€…ç²¾åº¦: {overlap_exact_matches}/{overlap_task_count} = {metrics['overlap_accuracy']:.3f}"
            )
        else:
            metrics["overlap_accuracy"] = 0.0
            print("   é‡è¤‡é–‹ç™ºè€…ç²¾åº¦: N/Aï¼ˆé‡è¤‡é–‹ç™ºè€…ãªã—ï¼‰")

        print(
            f"   å…¨ä½“æ­£ç¢ºä¸€è‡´: {exact_matches}/{len(common_tasks)} = {metrics['exact_accuracy']:.3f}"
        )
        print(
            f"   ãƒãƒƒãƒ”ãƒ³ã‚°ä¸€è‡´: {dev_mapping_matches}/{len(actuals)} = {metrics['mapping_accuracy']:.3f}"
        )

        # æ¨è–¦ã‚·ã‚¹ãƒ†ãƒ ã¨ã—ã¦ã®è©•ä¾¡æŒ‡æ¨™
        recommendation_metrics = self._calculate_recommendation_metrics(
            actual_assignments, predictions, prediction_scores, common_tasks
        )
        metrics.update(recommendation_metrics)

        # Top-Kç²¾åº¦ã®è¨ˆç®—
        if actuals and len(actuals) == len(predicted_actions):
            try:
                # Top-Kç²¾åº¦ã‚’æ‰‹å‹•è¨ˆç®—
                top_k_scores = {}
                for k in [1, 3, 5, 10]:
                    if k <= len(self.env.developers):
                        top_k_correct = 0
                        top_k_overlap_correct = 0
                        top_k_overlap_total = 0

                        for task_id in common_tasks:
                            if task_id in prediction_scores:
                                actual_dev = actual_assignments[task_id]

                                # å…¨ä½“ã§ã®Top-Kç²¾åº¦
                                if actual_dev in self.env.developers:
                                    actual_idx = self.env.developers.index(actual_dev)
                                    top_k_actions = prediction_scores[task_id][
                                        "top_5_actions"
                                    ][:k]
                                    if actual_idx in top_k_actions:
                                        top_k_correct += 1

                                # é‡è¤‡é–‹ç™ºè€…ã§ã®Top-Kç²¾åº¦
                                if actual_dev in overlap_devs:
                                    top_k_overlap_total += 1
                                    actual_idx = self.env.developers.index(actual_dev)
                                    top_k_actions = prediction_scores[task_id][
                                        "top_5_actions"
                                    ][:k]
                                    if actual_idx in top_k_actions:
                                        top_k_overlap_correct += 1

                        top_k_scores[f"top_{k}_accuracy"] = top_k_correct / len(
                            common_tasks
                        )
                        if top_k_overlap_total > 0:
                            top_k_scores[f"top_{k}_overlap_accuracy"] = (
                                top_k_overlap_correct / top_k_overlap_total
                            )
                        else:
                            top_k_scores[f"top_{k}_overlap_accuracy"] = 0.0

                        print(
                            f"   Top-{k}ç²¾åº¦: {top_k_correct}/{len(common_tasks)} = {top_k_scores[f'top_{k}_accuracy']:.3f}"
                        )
                        if top_k_overlap_total > 0:
                            print(
                                f"   Top-{k}é‡è¤‡ç²¾åº¦: {top_k_overlap_correct}/{top_k_overlap_total} = {top_k_scores[f'top_{k}_overlap_accuracy']:.3f}"
                            )

                metrics.update(top_k_scores)

            except Exception as e:
                print(f"âš ï¸ Top-Kç²¾åº¦è¨ˆç®—ã‚¨ãƒ©ãƒ¼: {e}")

        # é–‹ç™ºè€…ãƒ¬ãƒ™ãƒ«ã®çµ±è¨ˆ
        dev_stats = self._calculate_developer_stats(
            actual_assignments, predictions, common_tasks
        )
        metrics["developer_stats"] = dev_stats

        return metrics

    def _calculate_recommendation_metrics(
        self, actual_assignments, predictions, prediction_scores, common_tasks
    ):
        """æ¨è–¦ã‚·ã‚¹ãƒ†ãƒ ã¨ã—ã¦ã®è©•ä¾¡æŒ‡æ¨™"""
        print("ğŸ¯ æ¨è–¦ã‚·ã‚¹ãƒ†ãƒ æŒ‡æ¨™è¨ˆç®—ä¸­...")

        metrics = {}

        # 1. ä¿¡é ¼åº¦åˆ†æï¼ˆäºˆæ¸¬ç¢ºç‡ã®åˆ†å¸ƒï¼‰
        confidence_scores = []
        correct_confidences = []
        incorrect_confidences = []

        for task_id in common_tasks:
            if task_id in prediction_scores:
                actual_dev = actual_assignments[task_id]
                predicted_dev = predictions[task_id]

                # äºˆæ¸¬ã®ä¿¡é ¼åº¦ï¼ˆæœ€é«˜ç¢ºç‡ï¼‰
                max_prob = max(prediction_scores[task_id]["top_5_probs"])
                confidence_scores.append(max_prob)

                if actual_dev == predicted_dev:
                    correct_confidences.append(max_prob)
                else:
                    incorrect_confidences.append(max_prob)

        if confidence_scores:
            metrics["avg_confidence"] = np.mean(confidence_scores)
            metrics["confidence_std"] = np.std(confidence_scores)

            if correct_confidences:
                metrics["correct_avg_confidence"] = np.mean(correct_confidences)
            if incorrect_confidences:
                metrics["incorrect_avg_confidence"] = np.mean(incorrect_confidences)

            print(
                f"   å¹³å‡äºˆæ¸¬ä¿¡é ¼åº¦: {metrics['avg_confidence']:.3f} Â± {metrics['confidence_std']:.3f}"
            )
            if correct_confidences and incorrect_confidences:
                print(f"   æ­£è§£æ™‚ä¿¡é ¼åº¦: {metrics['correct_avg_confidence']:.3f}")
                print(f"   ä¸æ­£è§£æ™‚ä¿¡é ¼åº¦: {metrics['incorrect_avg_confidence']:.3f}")

        # 2. ãƒ©ãƒ³ã‚­ãƒ³ã‚°ç²¾åº¦ï¼ˆå®Ÿéš›ã®é–‹ç™ºè€…ãŒä½•ä½ã«äºˆæ¸¬ã•ã‚Œã‚‹ã‹ï¼‰
        ranking_positions = []
        ranking_similarities = []

        for task_id in common_tasks:
            if task_id in prediction_scores:
                actual_dev = actual_assignments[task_id]
                if actual_dev in self.env.developers:
                    actual_idx = self.env.developers.index(actual_dev)

                    # å…¨é–‹ç™ºè€…ã«å¯¾ã™ã‚‹äºˆæ¸¬ç¢ºç‡ã‚’å–å¾—
                    if "probabilities" in prediction_scores[task_id]:
                        all_probs = prediction_scores[task_id]["probabilities"]
                    else:
                        # top_5_probsã‹ã‚‰å…¨ä½“ã‚’æ¨å®š
                        all_probs = np.zeros(len(self.env.developers))
                        top_actions = prediction_scores[task_id].get(
                            "top_5_actions", []
                        )
                        top_probs = prediction_scores[task_id].get("top_5_probs", [])
                        for act, prob in zip(top_actions, top_probs):
                            if act < len(all_probs):
                                all_probs[act] = prob

                    # ãƒ©ãƒ³ã‚­ãƒ³ã‚°ä½ç½®ã‚’è¨ˆç®—
                    sorted_indices = np.argsort(all_probs)[::-1]  # é™é †
                    try:
                        position = (
                            np.where(sorted_indices == actual_idx)[0][0] + 1
                        )  # 1-based
                        ranking_positions.append(position)

                        # ãƒ©ãƒ³ã‚­ãƒ³ã‚°é¡ä¼¼åº¦è¨ˆç®—ï¼ˆä½ç½®ãƒ™ãƒ¼ã‚¹ï¼‰
                        # 1ä½ãªã‚‰1.0ã€æœ€ä¸‹ä½ãªã‚‰0.0ã«è¿‘ã¥ã
                        similarity = 1.0 - (position - 1) / (
                            len(self.env.developers) - 1
                        )
                        ranking_similarities.append(similarity)

                    except (IndexError, ValueError):
                        # è¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯æœ€ä¸‹ä½æ‰±ã„
                        ranking_positions.append(len(self.env.developers))
                        ranking_similarities.append(0.0)

        if ranking_positions:
            metrics["avg_ranking_position"] = np.mean(ranking_positions)
            metrics["median_ranking_position"] = np.median(ranking_positions)
            metrics["avg_ranking_similarity"] = np.mean(ranking_similarities)
            metrics["ranking_top_1_ratio"] = sum(
                1 for p in ranking_positions if p == 1
            ) / len(ranking_positions)
            metrics["ranking_top_5_ratio"] = sum(
                1 for p in ranking_positions if p <= 5
            ) / len(ranking_positions)
            metrics["ranking_top_10_ratio"] = sum(
                1 for p in ranking_positions if p <= 10
            ) / len(ranking_positions)

            print(f"   å¹³å‡ãƒ©ãƒ³ã‚­ãƒ³ã‚°ä½ç½®: {metrics['avg_ranking_position']:.2f}")
            print(f"   ä¸­å¤®å€¤ãƒ©ãƒ³ã‚­ãƒ³ã‚°ä½ç½®: {metrics['median_ranking_position']:.1f}")
            print(f"   å¹³å‡ãƒ©ãƒ³ã‚­ãƒ³ã‚°é¡ä¼¼åº¦: {metrics['avg_ranking_similarity']:.3f}")
            print(f"   Top-1ç‡: {metrics['ranking_top_1_ratio']:.3f}")
            print(f"   Top-5ç‡: {metrics['ranking_top_5_ratio']:.3f}")
            print(f"   Top-10ç‡: {metrics['ranking_top_10_ratio']:.3f}")

        # 3. ã‚¹ãƒ”ã‚¢ãƒãƒ³é †ä½ç›¸é–¢ï¼ˆé–‹ç™ºè€…ãƒ—ãƒ¼ãƒ«é‡è¤‡éƒ¨åˆ†ã®ã¿ï¼‰
        spearman_correlations = []
        overlap_devs = set(actual_assignments.values()) & set(self.env.developers)

        if len(overlap_devs) >= 2:  # ç›¸é–¢è¨ˆç®—ã«ã¯æœ€ä½2äººå¿…è¦
            for task_id in common_tasks:
                if task_id in prediction_scores:
                    actual_dev = actual_assignments[task_id]
                    if actual_dev in overlap_devs:
                        # é‡è¤‡é–‹ç™ºè€…ã®ã¿ã§ãƒ©ãƒ³ã‚­ãƒ³ã‚°ã‚’ä½œæˆ
                        overlap_indices = [
                            self.env.developers.index(dev) for dev in overlap_devs
                        ]

                        if "probabilities" in prediction_scores[task_id]:
                            all_probs = prediction_scores[task_id]["probabilities"]
                            overlap_probs = [
                                all_probs[idx] if idx < len(all_probs) else 0.0
                                for idx in overlap_indices
                            ]
                        else:
                            # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆç¢ºç‡
                            overlap_probs = [0.1] * len(overlap_indices)

                        # å®Ÿéš›ã®ãƒ©ãƒ³ã‚­ãƒ³ã‚°ï¼ˆå®Ÿéš›ã®é–‹ç™ºè€…ãŒ1ä½ï¼‰
                        actual_ranking = [
                            1 if dev == actual_dev else 2 for dev in overlap_devs
                        ]

                        # äºˆæ¸¬ãƒ©ãƒ³ã‚­ãƒ³ã‚°
                        predicted_ranking = (
                            np.argsort(np.argsort(overlap_probs)[::-1]) + 1
                        )

                        if (
                            len(set(actual_ranking)) > 1
                            and len(set(predicted_ranking)) > 1
                        ):
                            # ã‚·ãƒ³ãƒ—ãƒ«ãªã‚¹ãƒ”ã‚¢ãƒãƒ³é †ä½ç›¸é–¢ã®å®Ÿè£…
                            def simple_spearman(x, y):
                                n = len(x)
                                if n != len(y):
                                    return 0.0

                                # é †ä½ã«å¤‰æ›
                                rank_x = np.argsort(np.argsort(x))
                                rank_y = np.argsort(np.argsort(y))

                                # ã‚¹ãƒ”ã‚¢ãƒãƒ³ç›¸é–¢è¨ˆç®—
                                d_squared = np.sum((rank_x - rank_y) ** 2)
                                correlation = 1 - (6 * d_squared) / (n * (n**2 - 1))
                                return correlation

                            correlation = simple_spearman(
                                actual_ranking, predicted_ranking
                            )
                            if not np.isnan(correlation):
                                spearman_correlations.append(correlation)

            if spearman_correlations:
                metrics["avg_spearman_correlation"] = np.mean(spearman_correlations)
                print(
                    f"   å¹³å‡ã‚¹ãƒ”ã‚¢ãƒãƒ³é †ä½ç›¸é–¢: {metrics['avg_spearman_correlation']:.3f}"
                )

        # 4. å®Ÿéš›ã®é–‹ç™ºè€…ç¾¤ã¨ã®é¡ä¼¼åº¦ï¼ˆã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦ï¼‰
        developer_similarity_scores = []

        for task_id in common_tasks:
            if task_id in prediction_scores:
                actual_dev = actual_assignments[task_id]

                # å®Ÿéš›ã®é–‹ç™ºè€…ãƒ™ã‚¯ãƒˆãƒ«ï¼ˆãƒ¯ãƒ³ãƒ›ãƒƒãƒˆï¼‰
                actual_vector = np.zeros(len(self.env.developers))
                if actual_dev in self.env.developers:
                    actual_idx = self.env.developers.index(actual_dev)
                    actual_vector[actual_idx] = 1.0

                # äºˆæ¸¬ç¢ºç‡ãƒ™ã‚¯ãƒˆãƒ«
                if "probabilities" in prediction_scores[task_id]:
                    predicted_vector = np.array(
                        prediction_scores[task_id]["probabilities"]
                    )
                else:
                    predicted_vector = np.zeros(len(self.env.developers))
                    top_actions = prediction_scores[task_id].get("top_5_actions", [])
                    top_probs = prediction_scores[task_id].get("top_5_probs", [])
                    for act, prob in zip(top_actions, top_probs):
                        if act < len(predicted_vector):
                            predicted_vector[act] = prob

                # ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦è¨ˆç®—
                if (
                    np.linalg.norm(actual_vector) > 0
                    and np.linalg.norm(predicted_vector) > 0
                ):
                    cosine_sim = np.dot(actual_vector, predicted_vector) / (
                        np.linalg.norm(actual_vector) * np.linalg.norm(predicted_vector)
                    )
                    developer_similarity_scores.append(cosine_sim)

        if developer_similarity_scores:
            metrics["avg_cosine_similarity"] = np.mean(developer_similarity_scores)
            metrics["cosine_similarity_std"] = np.std(developer_similarity_scores)
            print(
                f"   å¹³å‡ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦: {metrics['avg_cosine_similarity']:.3f} Â± {metrics['cosine_similarity_std']:.3f}"
            )

        # 5. æ¨è–¦å¤šæ§˜æ€§ã¨ã‚«ãƒãƒ¬ãƒƒã‚¸ï¼ˆå…ƒã®ãƒ­ã‚¸ãƒƒã‚¯ã‚’ç¶­æŒï¼‰
        predicted_dev_counts = Counter(predictions.values())
        total_predictions = len(predictions)

        if total_predictions > 0:
            # ã‚¸ãƒ‹ä¿‚æ•°ï¼ˆä¸å¹³ç­‰åº¦ï¼‰ã®è¨ˆç®—
            counts = list(predicted_dev_counts.values())
            counts.sort()
            n = len(counts)
            gini = (2 * sum((i + 1) * x for i, x in enumerate(counts))) / (
                n * sum(counts)
            ) - (n + 1) / n

            metrics["prediction_diversity"] = 1 - gini  # å¤šæ§˜æ€§ã¯ä¸å¹³ç­‰åº¦ã®é€†
            metrics["unique_predicted_devs"] = len(predicted_dev_counts)
            metrics["prediction_concentration"] = (
                max(predicted_dev_counts.values()) / total_predictions
            )

            print(f"   äºˆæ¸¬å¤šæ§˜æ€§: {metrics['prediction_diversity']:.3f}")
            print(f"   ãƒ¦ãƒ‹ãƒ¼ã‚¯äºˆæ¸¬é–‹ç™ºè€…æ•°: {metrics['unique_predicted_devs']}")
            print(f"   äºˆæ¸¬é›†ä¸­åº¦: {metrics['prediction_concentration']:.3f}")

        return metrics

    def _calculate_developer_stats(self, actual_assignments, predictions, common_tasks):
        """é–‹ç™ºè€…ãƒ¬ãƒ™ãƒ«ã®çµ±è¨ˆ"""
        dev_stats = {
            "actual_distribution": Counter(),
            "predicted_distribution": Counter(),
            "per_developer_accuracy": {},
        }

        for task_id in common_tasks:
            actual_dev = actual_assignments[task_id]
            predicted_dev = predictions[task_id]

            dev_stats["actual_distribution"][actual_dev] += 1
            dev_stats["predicted_distribution"][predicted_dev] += 1

        # é–‹ç™ºè€…åˆ¥ç²¾åº¦
        for dev in dev_stats["actual_distribution"]:
            dev_tasks = [tid for tid in common_tasks if actual_assignments[tid] == dev]
            correct = sum(1 for tid in dev_tasks if predictions.get(tid) == dev)
            dev_stats["per_developer_accuracy"][dev] = (
                correct / len(dev_tasks) if dev_tasks else 0
            )

        return dev_stats

    def generate_report(self, metrics, output_dir):
        """è©•ä¾¡ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
        print("ğŸ“„ ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆä¸­...")

        output_dir = Path(output_dir)
        output_dir.mkdir(exist_ok=True)

        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

        # ãƒ¡ãƒˆãƒªã‚¯ã‚¹ä¿å­˜
        metrics_path = output_dir / f"behavioral_comparison_metrics_{timestamp}.json"
        with open(metrics_path, "w", encoding="utf-8") as f:
            # NumPyé…åˆ—ã‚’å¯¾å¿œ
            def convert_numpy(obj):
                if isinstance(obj, np.ndarray):
                    return obj.tolist()
                elif isinstance(obj, np.integer):
                    return int(obj)
                elif isinstance(obj, np.floating):
                    return float(obj)
                return obj

            json.dump(metrics, f, indent=2, ensure_ascii=False, default=convert_numpy)

        print(f"âœ… ãƒ¡ãƒˆãƒªã‚¯ã‚¹ä¿å­˜: {metrics_path}")

        # å¯è¦–åŒ–
        self._create_visualizations(metrics, output_dir, timestamp)

        # ãƒ†ã‚­ã‚¹ãƒˆãƒ¬ãƒãƒ¼ãƒˆ
        report_path = output_dir / f"behavioral_comparison_report_{timestamp}.txt"
        with open(report_path, "w", encoding="utf-8") as f:
            f.write("å®Ÿéš›ã®é–‹ç™ºè€…è¡Œå‹• vs RLã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆäºˆæ¸¬ æ¯”è¼ƒè©•ä¾¡ãƒ¬ãƒãƒ¼ãƒˆ\n")
            f.write("=" * 60 + "\n\n")

            f.write(f"è©•ä¾¡æ—¥æ™‚: {datetime.now()}\n")
            f.write(f"ãƒ¢ãƒ‡ãƒ«: {self.model_path}\n")
            f.write(f"ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿: {self.test_data_path}\n\n")

            # ãƒ¡ãƒˆãƒªã‚¯ã‚¹
            f.write("ğŸ“Š è©•ä¾¡çµæœ:\n")
            for key, value in metrics.items():
                if key != "developer_stats" and isinstance(value, (int, float)):
                    f.write(f"   {key}: {value:.3f}\n")

            f.write("\nğŸ† ä¸»è¦æŒ‡æ¨™:\n")
            if "exact_accuracy" in metrics:
                f.write(f"   æ­£ç¢ºä¸€è‡´ç‡: {metrics['exact_accuracy']:.3f}\n")
            if "top_1_accuracy" in metrics:
                f.write(f"   Top-1ç²¾åº¦: {metrics['top_1_accuracy']:.3f}\n")
            if "top_5_accuracy" in metrics:
                f.write(f"   Top-5ç²¾åº¦: {metrics['top_5_accuracy']:.3f}\n")

        print(f"âœ… ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜: {report_path}")
        return report_path

    def _create_visualizations(self, metrics, output_dir, timestamp):
        """å¯è¦–åŒ–ã®ä½œæˆ"""
        try:
            import matplotlib.pyplot as plt
            import seaborn as sns

            # ç²¾åº¦æ¯”è¼ƒ
            if any(key.startswith("top_") for key in metrics.keys()):
                fig, ax = plt.subplots(figsize=(10, 6))

                accuracy_metrics = {
                    k: v
                    for k, v in metrics.items()
                    if k.endswith("_accuracy") and isinstance(v, (int, float))
                }

                names = list(accuracy_metrics.keys())
                values = list(accuracy_metrics.values())

                bars = ax.bar(range(len(names)), values, color="steelblue", alpha=0.7)
                ax.set_xlabel("è©•ä¾¡æŒ‡æ¨™")
                ax.set_ylabel("ç²¾åº¦")
                ax.set_title("RLã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ vs å®Ÿéš›ã®é–‹ç™ºè€…è¡Œå‹• - ç²¾åº¦æ¯”è¼ƒ")
                ax.set_xticks(range(len(names)))
                ax.set_xticklabels(names, rotation=45, ha="right")
                ax.set_ylim(0, 1)

                # å€¤ã‚’ãƒãƒ¼ã®ä¸Šã«è¡¨ç¤º
                for bar, value in zip(bars, values):
                    ax.text(
                        bar.get_x() + bar.get_width() / 2,
                        bar.get_height() + 0.01,
                        f"{value:.3f}",
                        ha="center",
                        va="bottom",
                    )

                plt.tight_layout()
                accuracy_path = output_dir / f"accuracy_comparison_{timestamp}.png"
                plt.savefig(accuracy_path, dpi=300, bbox_inches="tight")
                plt.close()

                print(f"âœ… ç²¾åº¦æ¯”è¼ƒã‚°ãƒ©ãƒ•: {accuracy_path}")

        except ImportError:
            print(
                "âš ï¸ matplotlib/seabornãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚å¯è¦–åŒ–ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚"
            )
        except Exception as e:
            print(f"âš ï¸ å¯è¦–åŒ–ã‚¨ãƒ©ãƒ¼: {e}")

    def run_evaluation(self, output_dir="outputs"):
        """è©•ä¾¡å®Ÿè¡Œ"""
        print("ğŸš€ å®Ÿéš›ã®é–‹ç™ºè€…è¡Œå‹•ã¨ã®æ¯”è¼ƒè©•ä¾¡é–‹å§‹")
        print("=" * 60)

        # å®Ÿéš›ã®å‰²ã‚Šå½“ã¦æŠ½å‡º
        actual_assignments = self.extract_actual_assignments()

        if not actual_assignments:
            print("âŒ å®Ÿéš›ã®å‰²ã‚Šå½“ã¦ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            return None

        # RLã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆäºˆæ¸¬
        predictions, prediction_scores = self.predict_assignments(actual_assignments)

        # è©•ä¾¡æŒ‡æ¨™è¨ˆç®—
        metrics = self.calculate_metrics(
            actual_assignments, predictions, prediction_scores
        )

        # ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
        report_path = self.generate_report(metrics, output_dir)

        print("\nâœ… è©•ä¾¡å®Œäº†!")
        print(f"ğŸ“„ è©³ç´°ãƒ¬ãƒãƒ¼ãƒˆ: {report_path}")

        return metrics


def main():
    parser = argparse.ArgumentParser(description="å®Ÿéš›ã®é–‹ç™ºè€…è¡Œå‹•ã¨ã®æ¯”è¼ƒè©•ä¾¡")
    parser.add_argument(
        "--config", default="configs/unified_rl.yaml", help="è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹"
    )
    parser.add_argument(
        "--model",
        default="models/simple_unified_rl_agent.zip",
        help="å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ãƒ‘ã‚¹",
    )
    parser.add_argument(
        "--test-data", default="data/backlog.json", help="ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ãƒ‘ã‚¹ï¼ˆçµ±åˆæ¸ˆã¿ï¼‰"
    )
    parser.add_argument("--output", default="outputs", help="å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª")

    args = parser.parse_args()

    # ãƒ‘ã‚¹ã®å­˜åœ¨ç¢ºèª
    for path_name, path_value in [
        ("config", args.config),
        ("model", args.model),
        ("test-data", args.test_data),
    ]:
        if not Path(path_value).exists():
            print(f"âŒ {path_name}ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {path_value}")
            return 1

    # è©•ä¾¡å®Ÿè¡Œ
    evaluator = BehavioralComparison(args.config, args.model, args.test_data)
    metrics = evaluator.run_evaluation(args.output)

    if metrics:
        print("\nğŸ¯ ä¸»è¦çµæœ:")
        for key, value in metrics.items():
            if isinstance(value, (int, float)) and key.endswith("_accuracy"):
                print(f"   {key}: {value:.3f}")

    return 0


if __name__ == "__main__":
    exit(main())
