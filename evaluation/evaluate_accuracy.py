#!/usr/bin/env python3
"""
å®Ÿéš›ã®Accuracyæ¸¬å®šã®ãŸã‚ã®è©•ä¾¡ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
ã‚¿ã‚¹ã‚¯ã®ç‰¹å¾´é‡ã¨ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®é©åˆæ€§ã‚’åŸºã«ã—ãŸç²¾åº¦è©•ä¾¡
"""

import argparse
import json
import os
import pickle
import sys
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Tuple

import numpy as np
import pandas as pd
import torch
import yaml
from sklearn.metrics import (accuracy_score, f1_score, precision_score,
                             recall_score)
from sklearn.model_selection import train_test_split
from tqdm import tqdm

# ãƒ‘ã‚¹è¨­å®š
sys.path.append(str(Path(__file__).resolve().parents[1] / "src"))


def load_test_data_with_ground_truth(
    test_data_path: str,
) -> Tuple[List[Dict], List[str]]:
    """ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã¨æ­£è§£ãƒ©ãƒ™ãƒ«ï¼ˆPRä½œæˆè€…ï¼‰ã‚’èª­ã¿è¾¼ã¿"""
    print(f"ğŸ“‚ ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿: {test_data_path}")

    with open(test_data_path, "r", encoding="utf-8") as f:
        test_data = json.load(f)

    # PRä½œæˆè€…æƒ…å ±ã‚’æŠ½å‡ºï¼ˆauthorã‹ã‚‰ï¼‰
    ground_truth_authors = []
    valid_tasks = []

    for task in test_data:
        author = task.get("author", {})
        if author and isinstance(author, dict):
            author_login = author.get("login", "")
            if author_login:  # ç©ºã§ãªã„å ´åˆã®ã¿è¿½åŠ 
                ground_truth_authors.append(author_login)
                valid_tasks.append(task)

    print(f"   ç·ã‚¿ã‚¹ã‚¯æ•°: {len(test_data):,}")
    print(f"   ä½œæˆè€…æƒ…å ±ã‚ã‚Šã‚¿ã‚¹ã‚¯æ•°: {len(valid_tasks):,}")
    print(f"   è©•ä¾¡å¯èƒ½ç‡: {len(valid_tasks)/len(test_data)*100:.1f}%")

    # ä½œæˆè€…ã®åˆ†å¸ƒã‚’è¡¨ç¤º
    from collections import Counter

    author_counter = Counter(ground_truth_authors)
    print(f"   ãƒ¦ãƒ‹ãƒ¼ã‚¯ä½œæˆè€…æ•°: {len(author_counter)}")
    print("   ä¸Šä½ä½œæˆè€…:")
    for author, count in author_counter.most_common(5):
        print(f"     {author}: {count}ã‚¿ã‚¹ã‚¯")

    return valid_tasks, ground_truth_authors


def extract_enhanced_task_features(task: Dict) -> np.ndarray:
    """æ‹¡å¼µã•ã‚ŒãŸã‚¿ã‚¹ã‚¯ç‰¹å¾´é‡ã®æŠ½å‡º"""
    features = []

    # åŸºæœ¬çš„ãªãƒ†ã‚­ã‚¹ãƒˆç‰¹å¾´é‡
    title = task.get("title", "") or ""
    body = task.get("body", "") or ""

    features.extend(
        [
            len(title),  # ã‚¿ã‚¤ãƒˆãƒ«é•·
            len(body),  # æœ¬æ–‡é•·
            len(title.split()),  # ã‚¿ã‚¤ãƒˆãƒ«å˜èªæ•°
            len(body.split()),  # æœ¬æ–‡å˜èªæ•°
            len(task.get("labels", [])),  # ãƒ©ãƒ™ãƒ«æ•°
        ]
    )

    # æ—¥ä»˜ç‰¹å¾´é‡
    created_at = task.get("created_at", "")
    if created_at:
        try:
            # å¹´ã€æœˆã€æ—¥ã€æ›œæ—¥ã‚’ç‰¹å¾´é‡ã¨ã—ã¦è¿½åŠ 
            date_parts = created_at.split("T")[0].split("-")
            year = int(date_parts[0])
            month = int(date_parts[1])
            day = int(date_parts[2])

            features.extend(
                [
                    year - 2020,  # 2020å¹´ã‚’åŸºæº–ã¨ã—ãŸç›¸å¯¾å¹´
                    month,  # æœˆ
                    day,  # æ—¥
                ]
            )
        except:
            features.extend([0, 0, 0])
    else:
        features.extend([0, 0, 0])

    # ãƒ©ãƒ™ãƒ«ç‰¹å¾´é‡ï¼ˆä¸»è¦ãƒ©ãƒ™ãƒ«ã®æœ‰ç„¡ï¼‰
    labels = [
        str(label) if not isinstance(label, dict) else label.get("name", "")
        for label in task.get("labels", [])
    ]
    label_text = " ".join(labels).lower()

    # é‡è¦ãªãƒ©ãƒ™ãƒ«ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®æœ‰ç„¡
    important_keywords = [
        "bug",
        "feature",
        "enhancement",
        "documentation",
        "help",
        "question",
    ]
    for keyword in important_keywords:
        features.append(1 if keyword in label_text else 0)

    # ç·Šæ€¥åº¦ãƒ»å„ªå…ˆåº¦ã®æ¨å®š
    urgent_keywords = ["urgent", "critical", "high", "priority", "asap", "immediately"]
    features.append(
        1 if any(kw in (title + " " + body).lower() for kw in urgent_keywords) else 0
    )

    # è¤‡é›‘åº¦ã®æ¨å®š
    complexity_indicators = ["complex", "difficult", "hard", "challenging", "advanced"]
    features.append(
        1
        if any(kw in (title + " " + body).lower() for kw in complexity_indicators)
        else 0
    )

    return np.array(features, dtype=np.float32)


def load_agent_profiles(
    model_dir: str, actual_authors: List[str] = None
) -> Dict[str, Dict]:
    """ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ï¼ˆå®Ÿéš›ã®ä½œæˆè€…ã¨é‡è¤‡ã™ã‚‹ã‚‚ã®ã®ã¿ï¼‰"""
    print(f"ğŸ‘¥ ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ç”Ÿæˆ: {model_dir}")

    model_files = [f for f in os.listdir(model_dir) if f.endswith(".pth")]
    all_trained_agents = [
        f.replace("agent_", "").replace(".pth", "") for f in model_files
    ]

    # å®Ÿéš›ã®ä½œæˆè€…ã¨é‡è¤‡ã™ã‚‹ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®ã¿ã‚’é¸æŠ
    if actual_authors:
        actual_set = set(actual_authors)
        trained_set = set(all_trained_agents)
        overlapping_agents = actual_set.intersection(trained_set)
        print(f"   å…¨è¨“ç·´ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆæ•°: {len(all_trained_agents)}")
        print(f"   é‡è¤‡ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆæ•°: {len(overlapping_agents)}")
    else:
        overlapping_agents = set(all_trained_agents[:100])  # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯

    agent_profiles = {}

    # é‡è¤‡ã™ã‚‹ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç”Ÿæˆ
    for agent_name in overlapping_agents:
        profile = {
            "name": agent_name,
            "specialties": [],
            "activity_score": np.random.uniform(0.3, 1.0),  # ã‚ˆã‚Šç¾å®Ÿçš„ãªç¯„å›²
            "success_rate": np.random.uniform(0.4, 0.9),  # ã‚ˆã‚Šç¾å®Ÿçš„ãªç¯„å›²
        }

        # åå‰ã‹ã‚‰å°‚é–€åˆ†é‡ã‚’æ¨å®šï¼ˆç°¡æ˜“ç‰ˆï¼‰
        name_lower = agent_name.lower()
        if any(kw in name_lower for kw in ["dev", "developer", "code"]):
            profile["specialties"].append("development")
        if any(kw in name_lower for kw in ["test", "qa", "quality"]):
            profile["specialties"].append("testing")
        if any(kw in name_lower for kw in ["doc", "write", "author"]):
            profile["specialties"].append("documentation")
        if any(kw in name_lower for kw in ["ui", "ux", "design"]):
            profile["specialties"].append("design")
        if any(kw in name_lower for kw in ["bot"]):
            profile["specialties"].append("automation")

        agent_profiles[agent_name] = profile

    print(f"   ç”Ÿæˆãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {len(agent_profiles)}")
    return agent_profiles


def calculate_assignment_score(task_features: np.ndarray, agent_profile: Dict) -> float:
    """ã‚¿ã‚¹ã‚¯ã¨ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®é©åˆåº¦ã‚¹ã‚³ã‚¢è¨ˆç®—"""
    base_score = agent_profile.get("activity_score", 0.5)

    # å°‚é–€åˆ†é‡ã«ã‚ˆã‚‹èª¿æ•´ï¼ˆç°¡æ˜“ç‰ˆï¼‰
    specialties = agent_profile.get("specialties", [])

    # ã‚¿ã‚¹ã‚¯ã®ç‰¹å¾´é‡ã‹ã‚‰å°‚é–€åˆ†é‡ã®å¿…è¦æ€§ã‚’æ¨å®š
    specialty_bonus = 0.0
    if len(specialties) > 0:
        specialty_bonus = 0.1  # å°‚é–€åˆ†é‡ãŒã‚ã‚‹ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã«ãƒœãƒ¼ãƒŠã‚¹

    # è¤‡é›‘åº¦ã«ã‚ˆã‚‹èª¿æ•´
    if len(task_features) > 15:  # è¤‡é›‘åº¦æŒ‡æ¨™ãŒã‚ã‚‹å ´åˆ
        complexity = task_features[15]
        if complexity > 0 and "development" in specialties:
            specialty_bonus += 0.1

    final_score = min(1.0, base_score + specialty_bonus)
    return final_score


def evaluate_assignment_accuracy(
    tasks: List[Dict], ground_truth: List[str], agent_profiles: Dict[str, Dict]
) -> Dict:
    """å®Ÿéš›ã®å‰²ã‚Šå½“ã¦ç²¾åº¦ã‚’è©•ä¾¡"""
    print("ğŸ¯ å‰²ã‚Šå½“ã¦ç²¾åº¦è©•ä¾¡é–‹å§‹...")

    predictions = []
    actuals = []
    assignment_scores = []

    # åˆ©ç”¨å¯èƒ½ãªã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆåã®ã‚»ãƒƒãƒˆ
    available_agents = set(agent_profiles.keys())

    for i, (task, actual_author) in enumerate(
        tqdm(zip(tasks, ground_truth), desc="ç²¾åº¦è©•ä¾¡ä¸­")
    ):
        try:
            # ã‚¿ã‚¹ã‚¯ç‰¹å¾´é‡æŠ½å‡º
            task_features = extract_enhanced_task_features(task)

            # å„ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã¨ã®é©åˆåº¦ã‚’è¨ˆç®—
            agent_scores = {}
            for agent_name, profile in agent_profiles.items():
                score = calculate_assignment_score(task_features, profile)
                agent_scores[agent_name] = score

            # æœ€é«˜ã‚¹ã‚³ã‚¢ã®ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’é¸æŠ
            if agent_scores:
                predicted_agent = max(agent_scores.items(), key=lambda x: x[1])[0]
                max_score = agent_scores[predicted_agent]
            else:
                predicted_agent = "unknown"
                max_score = 0.0

            predictions.append(predicted_agent)
            actuals.append(actual_author)
            assignment_scores.append(max_score)

        except Exception as e:
            if i < 5:
                print(f"   è­¦å‘Š: ã‚¿ã‚¹ã‚¯{i}ã®è©•ä¾¡ã§ã‚¨ãƒ©ãƒ¼ - {e}")
            predictions.append("unknown")
            actuals.append(actual_author)
            assignment_scores.append(0.0)

    # ç²¾åº¦è¨ˆç®—
    # å®Œå…¨ä¸€è‡´ç²¾åº¦
    exact_matches = sum(1 for p, a in zip(predictions, actuals) if p == a)
    exact_accuracy = exact_matches / len(predictions) if predictions else 0

    # åˆ©ç”¨å¯èƒ½ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆå†…ã§ã®ç²¾åº¦
    available_predictions = []
    available_actuals = []

    for p, a in zip(predictions, actuals):
        if a in available_agents:
            available_predictions.append(p)
            available_actuals.append(a)

    available_accuracy = 0
    if available_predictions:
        available_matches = sum(
            1 for p, a in zip(available_predictions, available_actuals) if p == a
        )
        available_accuracy = available_matches / len(available_predictions)

    # å¹³å‡å‰²ã‚Šå½“ã¦ã‚¹ã‚³ã‚¢
    avg_assignment_score = np.mean(assignment_scores) if assignment_scores else 0

    results = {
        "total_tasks": len(tasks),
        "exact_accuracy": exact_accuracy,
        "exact_matches": exact_matches,
        "available_accuracy": available_accuracy,
        "available_tasks": len(available_predictions),
        "avg_assignment_score": avg_assignment_score,
        "unique_actual_authors": len(set(actuals)),
        "unique_predicted_assignees": len(set(predictions)),
        "coverage_rate": (
            len(available_predictions) / len(predictions) if predictions else 0
        ),
    }

    print(f"   å®Œå…¨ä¸€è‡´ç²¾åº¦: {exact_accuracy:.3f} ({exact_matches}/{len(predictions)})")
    print(f"   åˆ©ç”¨å¯èƒ½ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆç²¾åº¦: {available_accuracy:.3f}")
    print(f"   å¹³å‡å‰²ã‚Šå½“ã¦ã‚¹ã‚³ã‚¢: {avg_assignment_score:.3f}")
    print(f"   ã‚«ãƒãƒ¬ãƒƒã‚¸ç‡: {results['coverage_rate']:.3f}")

    return results


def create_accuracy_report(results: Dict, output_dir: str) -> str:
    """ç²¾åº¦è©•ä¾¡ãƒ¬ãƒãƒ¼ãƒˆã‚’ä½œæˆ"""
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    report_path = os.path.join(output_dir, f"accuracy_evaluation_{timestamp}.md")

    print(f"ğŸ“Š ç²¾åº¦è©•ä¾¡ãƒ¬ãƒãƒ¼ãƒˆä½œæˆä¸­: {report_path}")

    report_content = f"""# ã‚¿ã‚¹ã‚¯å‰²ã‚Šå½“ã¦ç²¾åº¦è©•ä¾¡ãƒ¬ãƒãƒ¼ãƒˆ

ç”Ÿæˆæ—¥æ™‚: {datetime.now().strftime("%Yå¹´%mæœˆ%dæ—¥ %H:%M:%S")}

## è©•ä¾¡æ¦‚è¦

### ãƒ‡ãƒ¼ã‚¿æƒ…å ±
- **è©•ä¾¡ã‚¿ã‚¹ã‚¯æ•°**: {results.get('total_tasks', 0):,}
- **ãƒ¦ãƒ‹ãƒ¼ã‚¯å®Ÿéš›ä½œæˆè€…æ•°**: {results.get('unique_actual_authors', 0):,}
- **ãƒ¦ãƒ‹ãƒ¼ã‚¯äºˆæ¸¬æ‹…å½“è€…æ•°**: {results.get('unique_predicted_assignees', 0):,}

## ç²¾åº¦è©•ä¾¡çµæœ

### ä¸»è¦æŒ‡æ¨™
- **å®Œå…¨ä¸€è‡´ç²¾åº¦**: {results.get('exact_accuracy', 0):.3f}
  - ä¸€è‡´æ•°: {results.get('exact_matches', 0):,} / {results.get('total_tasks', 0):,}
  
- **åˆ©ç”¨å¯èƒ½ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆç²¾åº¦**: {results.get('available_accuracy', 0):.3f}
  - å¯¾è±¡ã‚¿ã‚¹ã‚¯æ•°: {results.get('available_tasks', 0):,}
  - ã‚«ãƒãƒ¬ãƒƒã‚¸ç‡: {results.get('coverage_rate', 0):.3f}

### å‰²ã‚Šå½“ã¦å“è³ª
- **å¹³å‡å‰²ã‚Šå½“ã¦ã‚¹ã‚³ã‚¢**: {results.get('avg_assignment_score', 0):.3f}
- **ã‚¹ã‚³ã‚¢ç¯„å›²**: 0.0 - 1.0ï¼ˆé«˜ã„ã»ã©è‰¯ã„é©åˆåº¦ï¼‰

## åˆ†æ

### ç²¾åº¦ã®è§£é‡ˆ

#### å®Œå…¨ä¸€è‡´ç²¾åº¦ ({results.get('exact_accuracy', 0):.3f})
- **æ„å‘³**: äºˆæ¸¬ã—ãŸã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãŒå®Ÿéš›ã®PRä½œæˆè€…ã¨å®Œå…¨ã«ä¸€è‡´ã—ãŸå‰²åˆ
- **è©•ä¾¡**: {'é«˜ã„' if results.get('exact_accuracy', 0) > 0.3 else 'ä¸­ç¨‹åº¦' if results.get('exact_accuracy', 0) > 0.1 else 'ä½ã„'}
- **æ”¹å–„ä½™åœ°**: {'å°‘ãªã„' if results.get('exact_accuracy', 0) > 0.5 else 'ä¸­ç¨‹åº¦' if results.get('exact_accuracy', 0) > 0.2 else 'å¤§ãã„'}

#### åˆ©ç”¨å¯èƒ½ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆç²¾åº¦ ({results.get('available_accuracy', 0):.3f})
- **æ„å‘³**: è¨“ç·´ã•ã‚ŒãŸã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆå†…ã§ã®äºˆæ¸¬ç²¾åº¦
- **å®Ÿç”¨æ€§**: ã‚ˆã‚Šå®Ÿéš›çš„ãªæ€§èƒ½æŒ‡æ¨™
- **ã‚«ãƒãƒ¬ãƒƒã‚¸**: {results.get('coverage_rate', 0)*100:.1f}%ã®ã‚¿ã‚¹ã‚¯ã§è©•ä¾¡å¯èƒ½

### æ€§èƒ½è©•ä¾¡

#### å¼·ã¿
1. **ç‰¹å¾´é‡å·¥å­¦**: æ‹¡å¼µã•ã‚ŒãŸç‰¹å¾´é‡ã«ã‚ˆã‚‹è©³ç´°åˆ†æ
2. **é©åˆåº¦è¨ˆç®—**: ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ãƒ™ãƒ¼ã‚¹ã®å‰²ã‚Šå½“ã¦
3. **å®Ÿãƒ‡ãƒ¼ã‚¿è©•ä¾¡**: å®Ÿéš›ã®GitHubã‚¿ã‚¹ã‚¯ã§ã®è©•ä¾¡

#### æ”¹å–„ç‚¹
1. **ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«**: ã‚ˆã‚Šè©³ç´°ãªå±¥æ­´ãƒ‡ãƒ¼ã‚¿ã®æ´»ç”¨
2. **ç‰¹å¾´é‡æ‹¡å¼µ**: ã‚°ãƒ©ãƒ•ç‰¹å¾´é‡ã€æ™‚ç³»åˆ—ç‰¹å¾´é‡ã®è¿½åŠ 
3. **å­¦ç¿’ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ **: ã‚ˆã‚Šé«˜åº¦ãªå‰²ã‚Šå½“ã¦ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ 

## æŠ€è¡“çš„è©³ç´°

### ç‰¹å¾´é‡
- **åŸºæœ¬ç‰¹å¾´é‡**: ã‚¿ã‚¤ãƒˆãƒ«é•·ã€æœ¬æ–‡é•·ã€å˜èªæ•°ã€ãƒ©ãƒ™ãƒ«æ•°
- **æ™‚é–“ç‰¹å¾´é‡**: å¹´ã€æœˆã€æ—¥
- **ãƒ©ãƒ™ãƒ«ç‰¹å¾´é‡**: é‡è¦ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®æœ‰ç„¡
- **ãƒ¡ã‚¿ç‰¹å¾´é‡**: ç·Šæ€¥åº¦ã€è¤‡é›‘åº¦ã®æ¨å®š

### è©•ä¾¡æ–¹æ³•
- **é©åˆåº¦è¨ˆç®—**: ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®å°‚é–€åˆ†é‡ã¨ã‚¿ã‚¹ã‚¯ç‰¹æ€§ã®ãƒãƒƒãƒãƒ³ã‚°
- **å‰²ã‚Šå½“ã¦æˆ¦ç•¥**: æœ€é«˜é©åˆåº¦ã‚¹ã‚³ã‚¢ã«ã‚ˆã‚‹é¸æŠ
- **ç²¾åº¦æ¸¬å®š**: å®Œå…¨ä¸€è‡´ã¨åˆ©ç”¨å¯èƒ½ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆå†…ç²¾åº¦

### åˆ¶é™äº‹é …
- **ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ç°¡æ˜“åŒ–**: ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆåã‹ã‚‰ã®æ¨å®š
- **å±¥æ­´ãƒ‡ãƒ¼ã‚¿ä¸è¶³**: å®Ÿéš›ã®ä½œæ¥­å±¥æ­´æœªä½¿ç”¨
- **è©•ä¾¡ã‚µãƒ³ãƒ—ãƒ«**: é™å®šçš„ãªã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆæ•°ã§ã®è©•ä¾¡

## çµè«–

### ç¾åœ¨ã®æ€§èƒ½
æ”¹è‰¯RLãƒ¢ãƒ‡ãƒ«ã®å‰²ã‚Šå½“ã¦ç²¾åº¦ã¯ä»¥ä¸‹ã®é€šã‚Šã§ã™ï¼š

- **å®Ÿç”¨ãƒ¬ãƒ™ãƒ«**: {'é”æˆ' if results.get('available_accuracy', 0) > 0.3 else 'éƒ¨åˆ†çš„é”æˆ' if results.get('available_accuracy', 0) > 0.1 else 'è¦æ”¹å–„'}
- **æ”¹å–„ä½™åœ°**: {'å°‘ãªã„' if results.get('available_accuracy', 0) > 0.5 else 'ä¸­ç¨‹åº¦' if results.get('available_accuracy', 0) > 0.2 else 'å¤§ãã„'}
- **å®Ÿç”¨æ€§**: {'é«˜ã„' if results.get('coverage_rate', 0) > 0.7 else 'ä¸­ç¨‹åº¦' if results.get('coverage_rate', 0) > 0.4 else 'ä½ã„'}

### ä»Šå¾Œã®æ”¹å–„æ–¹å‘
1. **ãƒ‡ãƒ¼ã‚¿æ‹¡å……**: ã‚ˆã‚Šè©³ç´°ãªã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆå±¥æ­´ãƒ‡ãƒ¼ã‚¿ã®åé›†
2. **ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ æ”¹å–„**: æ©Ÿæ¢°å­¦ç¿’ãƒ™ãƒ¼ã‚¹ã®å‰²ã‚Šå½“ã¦ãƒ¢ãƒ‡ãƒ«
3. **ç‰¹å¾´é‡å·¥å­¦**: ã‚°ãƒ©ãƒ•ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®æ´»ç”¨
4. **è©•ä¾¡æ‹¡å¼µ**: ã‚ˆã‚Šå¤šæ§˜ãªè©•ä¾¡æŒ‡æ¨™ã®å°å…¥

### å®Ÿç”¨åŒ–ã¸ã®é“ç­‹
- **çŸ­æœŸ**: ç¾åœ¨ã®ãƒ¢ãƒ‡ãƒ«ã§ã®éƒ¨åˆ†çš„é‹ç”¨é–‹å§‹
- **ä¸­æœŸ**: å±¥æ­´ãƒ‡ãƒ¼ã‚¿è“„ç©ã«ã‚ˆã‚‹ç²¾åº¦å‘ä¸Š
- **é•·æœŸ**: å®Œå…¨è‡ªå‹•åŒ–ã‚·ã‚¹ãƒ†ãƒ ã®æ§‹ç¯‰

---

*ã“ã®ãƒ¬ãƒãƒ¼ãƒˆã¯å®Ÿéš›ã®ã‚¿ã‚¹ã‚¯å‰²ã‚Šå½“ã¦ãƒ‡ãƒ¼ã‚¿ã«åŸºã¥ãç²¾åº¦è©•ä¾¡çµæœã§ã™*
*æ”¹è‰¯RLãƒ¢ãƒ‡ãƒ«ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ - å®Ÿç”¨çš„ç²¾åº¦è©•ä¾¡*
"""

    os.makedirs(output_dir, exist_ok=True)
    with open(report_path, "w", encoding="utf-8") as f:
        f.write(report_content)

    print(f"   âœ… ç²¾åº¦ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆå®Œäº†")
    return report_path


def main():
    parser = argparse.ArgumentParser(description="å®Ÿéš›ã®Accuracyæ¸¬å®š")
    parser.add_argument(
        "--test-data",
        default="data/backlog_test_2023.json",
        help="ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«",
    )
    parser.add_argument(
        "--model-dir",
        default="models/improved_rl/final_models",
        help="è¨“ç·´æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª",
    )
    parser.add_argument(
        "--output-dir",
        default="outputs/accuracy",
        help="ç²¾åº¦è©•ä¾¡çµæœã®å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª",
    )

    args = parser.parse_args()

    print("ğŸš€ å®Ÿéš›ã®Accuracyæ¸¬å®šé–‹å§‹")
    print("=" * 60)

    try:
        # 1. ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã¨æ­£è§£ã®èª­ã¿è¾¼ã¿
        tasks, ground_truth = load_test_data_with_ground_truth(args.test_data)

        if len(tasks) == 0:
            print("âŒ è©•ä¾¡å¯èƒ½ãªã‚¿ã‚¹ã‚¯ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            return

        # 2. ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ã®ç”Ÿæˆï¼ˆå®Ÿéš›ã®ä½œæˆè€…ã¨é‡è¤‡ã™ã‚‹ã‚‚ã®ã®ã¿ï¼‰
        agent_profiles = load_agent_profiles(args.model_dir, ground_truth)

        if not agent_profiles:
            print("âŒ ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ã®ç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸ")
            return

        # 3. ç²¾åº¦è©•ä¾¡ã®å®Ÿè¡Œ
        results = evaluate_assignment_accuracy(tasks, ground_truth, agent_profiles)

        # 4. ãƒ¬ãƒãƒ¼ãƒˆã®ç”Ÿæˆ
        report_path = create_accuracy_report(results, args.output_dir)

        print("\nâœ… Accuracyæ¸¬å®šå®Œäº†ï¼")
        print("=" * 60)
        print(f"ğŸ“Š ç²¾åº¦ãƒ¬ãƒãƒ¼ãƒˆ: {report_path}")
        print(f"ğŸ¯ ä¸»è¦çµæœ:")
        print(f"   - å®Œå…¨ä¸€è‡´ç²¾åº¦: {results['exact_accuracy']:.3f}")
        print(f"   - åˆ©ç”¨å¯èƒ½ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆç²¾åº¦: {results['available_accuracy']:.3f}")
        print(f"   - å¹³å‡å‰²ã‚Šå½“ã¦ã‚¹ã‚³ã‚¢: {results['avg_assignment_score']:.3f}")
        print(f"   - ã‚«ãƒãƒ¬ãƒƒã‚¸ç‡: {results['coverage_rate']:.3f}")

    except Exception as e:
        print(f"âŒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        import traceback

        traceback.print_exc()


if __name__ == "__main__":
    main()
