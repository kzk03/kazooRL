#!/usr/bin/env python3
"""
é«˜åº¦ãªã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«æ¨è–¦ã‚·ã‚¹ãƒ†ãƒ  (ä¿®æ­£ç‰ˆ)
Top-1ç²¾åº¦ã®åŠ‡çš„æ”¹å–„ã‚’ç›®æŒ‡ã™æœ€å…ˆç«¯æ‰‹æ³•
"""

import json
import os
from collections import Counter, defaultdict
from datetime import datetime
from typing import Dict, List, Optional, Tuple

import numpy as np
import torch
import torch.nn as nn
from tqdm import tqdm


class PPOPolicyNetwork(nn.Module):
    """PPOãƒãƒªã‚·ãƒ¼ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®å†æ§‹ç¯‰"""

    def __init__(self, input_dim=64, hidden_dim=128):
        super(PPOPolicyNetwork, self).__init__()

        self.feature_extractor = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.LayerNorm(hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(hidden_dim, hidden_dim),
            nn.LayerNorm(hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.1),
        )

        self.actor = nn.Sequential(
            nn.Linear(hidden_dim, 64),
            nn.LayerNorm(64),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(64, 16),
            nn.Softmax(dim=-1),
        )

        self.critic = nn.Sequential(
            nn.Linear(hidden_dim, 64),
            nn.LayerNorm(64),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(64, 1),
        )

    def forward(self, x):
        features = self.feature_extractor(x)
        action_probs = self.actor(features)
        value = self.critic(features)
        return action_probs, value

    def get_action_score(self, x):
        with torch.no_grad():
            action_probs, value = self.forward(x)
            score = torch.max(action_probs).item()
            return score


def is_bot(username: str) -> bool:
    """ãƒ¦ãƒ¼ã‚¶ãƒ¼åãŒBotã‹ã©ã†ã‹åˆ¤å®š"""
    bot_indicators = [
        "[bot]",
        "bot",
        "dependabot",
        "renovate",
        "greenkeeper",
        "codecov",
        "travis",
        "circleci",
        "github-actions",
        "automated",
    ]
    username_lower = username.lower()
    return any(indicator in username_lower for indicator in bot_indicators)


class AdvancedEnsembleSystem:
    """é«˜åº¦ãªã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«æ¨è–¦ã‚·ã‚¹ãƒ†ãƒ  - Top-1ç²¾åº¦ç‰¹åŒ–"""

    def __init__(self, model_dir: str, test_data_path: str):
        print("ğŸ”§ ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–é–‹å§‹...")
        self.model_dir = model_dir
        self.test_data_path = test_data_path
        self.models = {}
        self.author_contributions = {}
        self.author_task_history = defaultdict(list)
        self.temporal_patterns = {}
        self.author_features = {}

        # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ãƒ»åˆ†æ
        self._load_test_data()
        self._analyze_contributions()
        self._load_all_models()
        self._build_task_history()
        self._analyze_temporal_patterns()
        self._compute_similarity_matrix()

    def _load_test_data(self):
        """ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿"""
        print("ğŸ“‚ ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ä¸­...")

        with open(self.test_data_path, "r", encoding="utf-8") as f:
            test_data = json.load(f)

        self.tasks = []
        self.ground_truth = []

        for task in test_data:
            author = task.get("author", {})
            if author and isinstance(author, dict):
                author_login = author.get("login", "")
                if author_login and not is_bot(author_login):
                    self.tasks.append(task)
                    self.ground_truth.append(author_login)

        print(f"   èª­ã¿è¾¼ã¿å®Œäº†: {len(self.tasks):,}ã‚¿ã‚¹ã‚¯")

    def _analyze_contributions(self):
        """è²¢çŒ®é‡åˆ†æ"""
        print("ğŸ“Š è²¢çŒ®é‡åˆ†æä¸­...")
        self.author_contributions = Counter(self.ground_truth)
        print(f"   ãƒ¦ãƒ‹ãƒ¼ã‚¯é–‹ç™ºè€…æ•°: {len(self.author_contributions)}")

    def _load_all_models(self):
        """å…¨ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿"""
        print("ğŸ¤– å…¨ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ä¸­...")

        model_files = [f for f in os.listdir(self.model_dir) if f.endswith(".pth")]
        all_trained_agents = [
            f.replace("agent_", "").replace(".pth", "") for f in model_files
        ]

        human_agents = [agent for agent in all_trained_agents if not is_bot(agent)]
        actual_set = set(self.ground_truth)
        overlapping_agents = actual_set.intersection(set(human_agents))

        loaded_count = 0
        for agent_name in overlapping_agents:
            model_path = os.path.join(self.model_dir, f"agent_{agent_name}.pth")

            try:
                model_data = torch.load(
                    model_path, map_location="cpu", weights_only=False
                )
                policy_network = PPOPolicyNetwork()
                policy_network.load_state_dict(model_data["policy_state_dict"])
                policy_network.eval()

                self.models[agent_name] = policy_network
                loaded_count += 1

            except Exception:
                continue

        print(f"   èª­ã¿è¾¼ã¿çµæœ: {loaded_count}æˆåŠŸ")

    def _build_task_history(self):
        """å„é–‹ç™ºè€…ã®ã‚¿ã‚¹ã‚¯å±¥æ­´ã‚’æ§‹ç¯‰"""
        print("ğŸ“‹ ã‚¿ã‚¹ã‚¯å±¥æ­´æ§‹ç¯‰ä¸­...")

        for task, author in zip(self.tasks, self.ground_truth):
            if author in self.models:
                self.author_task_history[author].append(task)

        print(f"   å±¥æ­´æ§‹ç¯‰å®Œäº†: {len(self.author_task_history)}é–‹ç™ºè€…")

    def _analyze_temporal_patterns(self):
        """æ™‚é–“çš„ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ"""
        print("â° æ™‚é–“çš„ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æä¸­...")

        for author, tasks in self.author_task_history.items():
            monthly_activity = defaultdict(int)
            weekday_activity = defaultdict(int)

            for task in tasks:
                created_at = task.get("created_at", "")
                if created_at:
                    try:
                        month = int(created_at.split("-")[1])
                        monthly_activity[month] += 1

                        day = int(created_at.split("-")[2].split("T")[0])
                        weekday = day % 7
                        weekday_activity[weekday] += 1
                    except:
                        continue

            self.temporal_patterns[author] = {
                "monthly": dict(monthly_activity),
                "weekday": dict(weekday_activity),
            }

        print(f"   æ™‚é–“çš„ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æå®Œäº†: {len(self.temporal_patterns)}é–‹ç™ºè€…")

    def _compute_similarity_matrix(self):
        """ã‚¿ã‚¹ã‚¯-é–‹ç™ºè€…é¡ä¼¼åº¦ãƒãƒˆãƒªãƒƒã‚¯ã‚¹è¨ˆç®—"""
        print("ğŸ” é¡ä¼¼åº¦ãƒãƒˆãƒªãƒƒã‚¯ã‚¹è¨ˆç®—ä¸­...")

        author_features = {}

        for author, tasks in self.author_task_history.items():
            if len(tasks) == 0:
                continue

            keyword_counts = defaultdict(int)
            total_tasks = len(tasks)

            for task in tasks:
                title = (task.get("title", "") or "").lower()
                body = (task.get("body", "") or "").lower()
                labels = task.get("labels", [])

                label_text = " ".join(
                    [
                        (
                            str(label)
                            if not isinstance(label, dict)
                            else label.get("name", "")
                        )
                        for label in labels
                    ]
                ).lower()

                full_text = f"{title} {body} {label_text}"

                important_keywords = [
                    "bug",
                    "fix",
                    "error",
                    "feature",
                    "enhancement",
                    "new",
                    "doc",
                    "readme",
                    "guide",
                    "ui",
                    "ux",
                    "design",
                    "performance",
                    "optimize",
                    "security",
                    "auth",
                    "api",
                    "endpoint",
                    "test",
                    "spec",
                    "docker",
                    "compose",
                    "build",
                    "deploy",
                    "config",
                ]

                for keyword in important_keywords:
                    if keyword in full_text:
                        keyword_counts[keyword] += 1

            feature_vector = []
            for keyword in important_keywords:
                feature_vector.append(keyword_counts[keyword] / total_tasks)

            author_features[author] = np.array(feature_vector)

        self.author_features = author_features
        print(f"   é¡ä¼¼åº¦ãƒãƒˆãƒªãƒƒã‚¯ã‚¹è¨ˆç®—å®Œäº†: {len(author_features)}é–‹ç™ºè€…")

    def _extract_task_features(self, task: Dict) -> torch.Tensor:
        """é«˜åº¦ãªã‚¿ã‚¹ã‚¯ç‰¹å¾´é‡æŠ½å‡º"""
        features = []

        title = task.get("title", "") or ""
        body = task.get("body", "") or ""
        labels = task.get("labels", [])

        # åŸºæœ¬ç‰¹å¾´é‡
        basic_features = [
            len(title),
            len(body),
            len(title.split()),
            len(body.split()),
            len(labels),
            title.count("?"),
            title.count("!"),
            body.count("\n"),
            len(set(title.lower().split())),
            1 if any(kw in title.lower() for kw in ["bug", "fix", "error"]) else 0,
        ]
        features.extend(basic_features)

        # æ—¥ä»˜ç‰¹å¾´é‡
        created_at = task.get("created_at", "")
        if created_at:
            try:
                date_parts = created_at.split("T")[0].split("-")
                year, month, day = (
                    int(date_parts[0]),
                    int(date_parts[1]),
                    int(date_parts[2]),
                )
                features.extend([year - 2020, month, day, day % 7])
            except:
                features.extend([0, 0, 0, 0])
        else:
            features.extend([0, 0, 0, 0])

        # ãƒ©ãƒ™ãƒ«ç‰¹å¾´é‡
        label_text = " ".join(
            [
                str(label) if not isinstance(label, dict) else label.get("name", "")
                for label in labels
            ]
        ).lower()

        extended_keywords = [
            "bug",
            "feature",
            "enhancement",
            "documentation",
            "help",
            "question",
            "performance",
            "security",
            "ui",
            "api",
            "docker",
            "compose",
            "build",
            "deploy",
            "config",
            "test",
            "spec",
            "coverage",
            "ci",
            "cd",
        ]

        for keyword in extended_keywords:
            features.append(
                1 if keyword in f"{title} {body} {label_text}".lower() else 0
            )

        # ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°
        while len(features) < 64:
            features.append(0.0)
        features = features[:64]

        # æ­£è¦åŒ–
        features = np.array(features, dtype=np.float32)
        if np.std(features) != 0:
            features = (features - np.mean(features)) / (np.std(features) + 1e-8)

        return torch.tensor(features, dtype=torch.float32)

    def _calculate_task_similarity(self, task: Dict, author: str) -> float:
        """ã‚¿ã‚¹ã‚¯ã¨é–‹ç™ºè€…ã®é«˜åº¦ãªé¡ä¼¼åº¦è¨ˆç®—"""
        if author not in self.author_features:
            return 0.0

        title = (task.get("title", "") or "").lower()
        body = (task.get("body", "") or "").lower()
        labels = task.get("labels", [])

        label_text = " ".join(
            [
                str(label) if not isinstance(label, dict) else label.get("name", "")
                for label in labels
            ]
        ).lower()

        full_text = f"{title} {body} {label_text}"

        important_keywords = [
            "bug",
            "fix",
            "error",
            "feature",
            "enhancement",
            "new",
            "doc",
            "readme",
            "guide",
            "ui",
            "ux",
            "design",
            "performance",
            "optimize",
            "security",
            "auth",
            "api",
            "endpoint",
            "test",
            "spec",
            "docker",
            "compose",
            "build",
            "deploy",
            "config",
        ]

        task_vector = []
        for keyword in important_keywords:
            task_vector.append(1.0 if keyword in full_text else 0.0)

        task_vector = np.array(task_vector)
        author_vector = self.author_features[author]

        # ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦
        dot_product = np.dot(task_vector, author_vector)
        task_norm = np.linalg.norm(task_vector)
        author_norm = np.linalg.norm(author_vector)

        if task_norm == 0 or author_norm == 0:
            return 0.0

        similarity = dot_product / (task_norm * author_norm)
        return max(0.0, similarity)

    def _calculate_temporal_match(self, task: Dict, author: str) -> float:
        """æ™‚é–“çš„ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒãƒƒãƒãƒ³ã‚°"""
        if author not in self.temporal_patterns:
            return 0.5

        created_at = task.get("created_at", "")
        if not created_at:
            return 0.5

        try:
            month = int(created_at.split("-")[1])
            day = int(created_at.split("-")[2].split("T")[0])
            weekday = day % 7

            patterns = self.temporal_patterns[author]

            monthly_activity = patterns.get("monthly", {})
            total_monthly = sum(monthly_activity.values()) if monthly_activity else 1
            month_score = monthly_activity.get(month, 0) / total_monthly

            weekday_activity = patterns.get("weekday", {})
            total_weekday = sum(weekday_activity.values()) if weekday_activity else 1
            weekday_score = weekday_activity.get(weekday, 0) / total_weekday

            temporal_score = 0.6 * month_score + 0.4 * weekday_score
            return min(1.0, temporal_score * 2)

        except:
            return 0.5

    def ultra_advanced_ensemble_recommendation(
        self, task_features: torch.Tensor, task: Dict, k: int = 5
    ) -> List[Tuple[str, float]]:
        """ğŸš€ è¶…é«˜åº¦ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«æ¨è–¦ - Top-1ç²¾åº¦ç‰¹åŒ–"""

        agent_scores = {}

        for agent_name, model in self.models.items():
            try:
                # 1. PPOãƒ¢ãƒ‡ãƒ«ã‚¹ã‚³ã‚¢
                ppo_score = model.get_action_score(task_features)

                # 2. è²¢çŒ®é‡ã‚¹ã‚³ã‚¢ï¼ˆéç·šå½¢å¤‰æ›ï¼‰
                contribution = self.author_contributions.get(agent_name, 0)
                if contribution >= 200:
                    contribution_score = 1.0
                elif contribution >= 100:
                    contribution_score = 0.9
                elif contribution >= 50:
                    contribution_score = 0.75
                elif contribution >= 20:
                    contribution_score = 0.6
                elif contribution >= 10:
                    contribution_score = 0.45
                else:
                    contribution_score = 0.3

                # 3. é«˜åº¦ãªé¡ä¼¼åº¦ã‚¹ã‚³ã‚¢
                similarity_score = self._calculate_task_similarity(task, agent_name)

                # 4. æ™‚é–“çš„ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒãƒƒãƒãƒ³ã‚°
                temporal_score = self._calculate_temporal_match(task, agent_name)

                # 5. å°‚é–€æ€§é›†ä¸­åº¦ã‚¹ã‚³ã‚¢
                author_tasks = self.author_task_history.get(agent_name, [])
                if len(author_tasks) > 0:
                    task_types = set()
                    for t in author_tasks:
                        title_lower = (t.get("title", "") or "").lower()
                        if any(kw in title_lower for kw in ["bug", "fix", "error"]):
                            task_types.add("bug")
                        elif any(
                            kw in title_lower for kw in ["feature", "enhancement"]
                        ):
                            task_types.add("feature")
                        elif any(kw in title_lower for kw in ["doc", "readme"]):
                            task_types.add("doc")
                        else:
                            task_types.add("other")

                    specialization_score = max(0.3, 1.0 - (len(task_types) - 1) * 0.2)
                else:
                    specialization_score = 0.5

                # 6. ç›¸å¯¾çš„ãƒ©ãƒ³ã‚­ãƒ³ã‚°ã‚¹ã‚³ã‚¢
                relative_strength = contribution / max(
                    self.author_contributions.values()
                )

                # ğŸ¯ é‡ã¿ä»˜ã‘
                weights = {
                    "ppo": 0.30,
                    "contribution": 0.25,
                    "similarity": 0.20,
                    "temporal": 0.10,
                    "specialization": 0.10,
                    "relative_strength": 0.05,
                }

                # æœ€çµ‚ã‚¹ã‚³ã‚¢è¨ˆç®—
                final_score = (
                    weights["ppo"] * ppo_score
                    + weights["contribution"] * contribution_score
                    + weights["similarity"] * similarity_score
                    + weights["temporal"] * temporal_score
                    + weights["specialization"] * specialization_score
                    + weights["relative_strength"] * relative_strength
                )

                # ğŸš€ Top-1ç‰¹åŒ–ãƒ–ãƒ¼ã‚¹ãƒˆ
                if contribution >= 200:
                    final_score *= 1.15
                elif contribution >= 100:
                    final_score *= 1.10
                elif contribution >= 50:
                    final_score *= 1.05

                if similarity_score > 0.8:
                    final_score *= 1.1
                elif similarity_score > 0.6:
                    final_score *= 1.05

                if temporal_score > 0.8:
                    final_score *= 1.05

                final_score = min(final_score, 1.0)
                agent_scores[agent_name] = final_score

            except Exception:
                agent_scores[agent_name] = 0.0

        sorted_agents = sorted(agent_scores.items(), key=lambda x: x[1], reverse=True)
        return sorted_agents[:k]

    def meta_ensemble_recommendation(
        self, task_features: torch.Tensor, task: Dict, k: int = 5
    ) -> List[Tuple[str, float]]:
        """ğŸ¯ ãƒ¡ã‚¿ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«æ¨è–¦ - è¤‡æ•°æ‰‹æ³•ã®å‹•çš„çµ±åˆ"""

        methods_results = {}

        # 1. åŸºæœ¬ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«
        basic_scores = {}
        for agent_name, model in self.models.items():
            try:
                ppo_score = model.get_action_score(task_features)
                contribution = self.author_contributions.get(agent_name, 0)
                contribution_score = min(contribution / 100.0, 1.0)
                similarity_score = self._calculate_task_similarity(task, agent_name)

                basic_score = (
                    0.4 * ppo_score + 0.4 * contribution_score + 0.2 * similarity_score
                )
                basic_scores[agent_name] = basic_score
            except:
                basic_scores[agent_name] = 0.0

        methods_results["basic"] = basic_scores

        # 2. è²¢çŒ®é‡é‡è¦–
        contribution_scores = {}
        for agent_name in self.models.keys():
            contribution = self.author_contributions.get(agent_name, 0)
            contribution_scores[agent_name] = min(contribution / 200.0, 1.0)

        methods_results["contribution"] = contribution_scores

        # 3. é¡ä¼¼åº¦é‡è¦–
        similarity_scores = {}
        for agent_name in self.models.keys():
            similarity_score = self._calculate_task_similarity(task, agent_name)
            similarity_scores[agent_name] = similarity_score

        methods_results["similarity"] = similarity_scores

        # 4. æ™‚é–“çš„ãƒ‘ã‚¿ãƒ¼ãƒ³é‡è¦–
        temporal_scores = {}
        for agent_name in self.models.keys():
            temporal_score = self._calculate_temporal_match(task, agent_name)
            temporal_scores[agent_name] = temporal_score

        methods_results["temporal"] = temporal_scores

        # ãƒ¡ã‚¿çµ±åˆï¼ˆå‹•çš„é‡ã¿èª¿æ•´ï¼‰
        final_scores = {}

        for agent_name in self.models.keys():
            basic_score = methods_results["basic"].get(agent_name, 0.0)
            contrib_score = methods_results["contribution"].get(agent_name, 0.0)
            sim_score = methods_results["similarity"].get(agent_name, 0.0)
            temp_score = methods_results["temporal"].get(agent_name, 0.0)

            # ã‚¿ã‚¹ã‚¯ã‚¿ã‚¤ãƒ—ã«å¿œã˜ãŸå‹•çš„é‡ã¿
            title_lower = (task.get("title", "") or "").lower()
            body_lower = (task.get("body", "") or "").lower()
            full_text = f"{title_lower} {body_lower}"

            if any(kw in full_text for kw in ["bug", "fix", "error", "issue"]):
                weights = [0.2, 0.4, 0.2, 0.2]  # ãƒã‚°ä¿®æ­£: çµŒé¨“é‡è¦–
            elif any(kw in full_text for kw in ["feature", "enhancement", "new"]):
                weights = [0.3, 0.2, 0.4, 0.1]  # æ–°æ©Ÿèƒ½: é¡ä¼¼åº¦é‡è¦–
            elif any(kw in full_text for kw in ["doc", "readme", "guide"]):
                weights = [0.2, 0.2, 0.5, 0.1]  # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ: é¡ä¼¼åº¦æœ€é‡è¦–
            else:
                weights = [0.35, 0.25, 0.25, 0.15]  # ä¸€èˆ¬: ãƒãƒ©ãƒ³ã‚¹

            # ãƒ¡ã‚¿ã‚¹ã‚³ã‚¢è¨ˆç®—
            meta_score = (
                weights[0] * basic_score
                + weights[1] * contrib_score
                + weights[2] * sim_score
                + weights[3] * temp_score
            )

            # æœ€é«˜è²¢çŒ®è€…ã¸ã®ç‰¹åˆ¥ãƒ–ãƒ¼ã‚¹ãƒˆ
            contribution = self.author_contributions.get(agent_name, 0)
            if contribution >= 200:
                meta_score *= 1.2
            elif contribution >= 100:
                meta_score *= 1.15
            elif contribution >= 50:
                meta_score *= 1.1

            final_scores[agent_name] = min(meta_score, 1.0)

        sorted_agents = sorted(final_scores.items(), key=lambda x: x[1], reverse=True)
        return sorted_agents[:k]

    def evaluate_system(self, method: str = "ultra_advanced", sample_size: int = 200):
        """ã‚·ã‚¹ãƒ†ãƒ è©•ä¾¡ï¼ˆè»½é‡ç‰ˆï¼‰"""
        print(f"ğŸ¯ {method}æ¨è–¦ã‚·ã‚¹ãƒ†ãƒ ã®è©•ä¾¡é–‹å§‹")
        print("-" * 50)

        available_agents = set(self.models.keys())

        # è©•ä¾¡å¯¾è±¡ã‚¿ã‚¹ã‚¯ã‚’é¸æŠ
        eval_tasks = []
        eval_ground_truth = []

        for task, author in zip(
            self.tasks[: sample_size * 3], self.ground_truth[: sample_size * 3]
        ):
            if author in available_agents and len(eval_tasks) < sample_size:
                eval_tasks.append(task)
                eval_ground_truth.append(author)

        print(f"   è©•ä¾¡ã‚¿ã‚¹ã‚¯æ•°: {len(eval_tasks)}")

        results = {}

        for k in [1, 3, 5]:
            correct_predictions = 0
            all_recommendations = []

            for task, actual_author in tqdm(
                zip(eval_tasks, eval_ground_truth),
                desc=f"Top-{k}è©•ä¾¡ä¸­",
                total=len(eval_tasks),
            ):
                try:
                    task_features = self._extract_task_features(task)

                    if method == "ultra_advanced":
                        recommendations = self.ultra_advanced_ensemble_recommendation(
                            task_features, task, k
                        )
                    elif method == "meta_ensemble":
                        recommendations = self.meta_ensemble_recommendation(
                            task_features, task, k
                        )
                    else:
                        raise ValueError(f"Unknown method: {method}")

                    recommended_agents = [agent for agent, _ in recommendations]
                    all_recommendations.extend(recommended_agents)

                    if actual_author in recommended_agents:
                        correct_predictions += 1

                except Exception:
                    continue

            accuracy = correct_predictions / len(eval_tasks) if eval_tasks else 0
            diversity_score = (
                len(set(all_recommendations)) / len(all_recommendations)
                if all_recommendations
                else 0
            )

            results[f"top_{k}"] = {
                "accuracy": accuracy,
                "diversity_score": diversity_score,
            }

            print(f"   Top-{k}ç²¾åº¦: {accuracy:.3f} ({accuracy*100:.1f}%)")
            print(f"   å¤šæ§˜æ€§ã‚¹ã‚³ã‚¢: {diversity_score:.3f}")

        return results


def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    print("ğŸš€ é«˜åº¦ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«æ¨è–¦ã‚·ã‚¹ãƒ†ãƒ  (ä¿®æ­£ç‰ˆ)")
    print("=" * 60)

    try:
        # ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–
        system = AdvancedEnsembleSystem(
            model_dir="models/improved_rl/final_models",
            test_data_path="data/backlog_test_2023.json",
        )

        print(f"\n## ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å®Œäº†")
        print(f"   èª­ã¿è¾¼ã¿ãƒ¢ãƒ‡ãƒ«æ•°: {len(system.models)}")
        print(f"   ã‚¿ã‚¹ã‚¯å±¥æ­´æ§‹ç¯‰: {len(system.author_task_history)}é–‹ç™ºè€…")
        print(f"   æ™‚é–“çš„ãƒ‘ã‚¿ãƒ¼ãƒ³: {len(system.temporal_patterns)}é–‹ç™ºè€…")
        print(f"   é¡ä¼¼åº¦ãƒãƒˆãƒªãƒƒã‚¯ã‚¹: {len(system.author_features)}é–‹ç™ºè€…")

        # æ‰‹æ³•ã®è©•ä¾¡
        methods = [
            ("ultra_advanced", "è¶…é«˜åº¦ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«æ¨è–¦"),
            ("meta_ensemble", "ãƒ¡ã‚¿ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«æ¨è–¦"),
        ]

        all_results = {}

        for method_key, method_name in methods:
            print(f"\n## {method_name}ã®è©•ä¾¡")
            results = system.evaluate_system(method_key, sample_size=150)
            all_results[method_key] = results

        # æœ€è‰¯ã®çµæœã‚’ç‰¹å®š
        best_method = max(
            all_results.keys(), key=lambda x: all_results[x]["top_1"]["accuracy"]
        )

        print(f"\nğŸ‰ è©•ä¾¡å®Œäº†ï¼")
        print("=" * 60)
        print(f"ğŸ† æœ€å„ªç§€æ‰‹æ³•: {best_method}")

        # ä¸»è¦çµæœã®è¡¨ç¤º
        for method_key, method_name in methods:
            results = all_results[method_key]
            top1_accuracy = results["top_1"]["accuracy"]
            top3_accuracy = results["top_3"]["accuracy"]
            top5_accuracy = results["top_5"]["accuracy"]
            print(f"   {method_name}:")
            print(f"     Top-1ç²¾åº¦: {top1_accuracy*100:.1f}%")
            print(f"     Top-3ç²¾åº¦: {top3_accuracy*100:.1f}%")
            print(f"     Top-5ç²¾åº¦: {top5_accuracy*100:.1f}%")

        # æ”¹å–„åº¦ã®è¨ˆç®—
        best_top1 = max(all_results[m]["top_1"]["accuracy"] for m in all_results.keys())
        print(f"\nğŸ¯ Top-1ç²¾åº¦ã®æœ€é«˜å€¤: {best_top1*100:.1f}%")

        if best_top1 > 0.037:
            improvement = (best_top1 - 0.037) / 0.037 * 100
            print(f"ğŸš€ åŸºæœ¬æ‰‹æ³•ã‹ã‚‰ã®æ”¹å–„: +{improvement:.1f}%")
            print(f"   âœ… æ”¹å–„æˆåŠŸï¼")
        else:
            print(f"   ğŸ“Š åŸºæœ¬æ‰‹æ³•ã¨åŒç­‰ã®æ€§èƒ½")

    except Exception as e:
        print(f"âŒ ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿ: {e}")
        import traceback

        traceback.print_exc()


if __name__ == "__main__":
    main()
