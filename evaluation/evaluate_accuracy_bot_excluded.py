#!/usr/bin/env python3
"""
Boté™¤å»ç‰ˆã®ç²¾åº¦è©•ä¾¡ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
"""

import argparse
import json
import os
import pickle
import sys
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Tuple

import numpy as np
import pandas as pd
import torch
import yaml
from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score
from sklearn.model_selection import train_test_split
from tqdm import tqdm

# ãƒ‘ã‚¹è¨­å®š
sys.path.append(str(Path(__file__).resolve().parents[1] / "src"))


def is_bot(username: str) -> bool:
    """ãƒ¦ãƒ¼ã‚¶ãƒ¼åãŒBotã‹ã©ã†ã‹åˆ¤å®š"""
    bot_indicators = [
        "[bot]",
        "bot",
        "dependabot",
        "renovate",
        "greenkeeper",
        "codecov",
        "travis",
        "circleci",
        "github-actions",
        "automated",
        "auto-",
        "-bot",
        "bot-"
    ]
    
    username_lower = username.lower()
    return any(indicator in username_lower for indicator in bot_indicators)


def load_test_data_with_bot_filtering(test_data_path: str) -> Tuple[List[Dict], List[str]]:
    """ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ã€Botã‚’é™¤å»"""
    print(f"ğŸ“‚ ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ï¼ˆBoté™¤å»ã‚ã‚Šï¼‰: {test_data_path}")
    
    with open(test_data_path, "r", encoding="utf-8") as f:
        test_data = json.load(f)
    
    # Boté™¤å»å¾Œã®ãƒ‡ãƒ¼ã‚¿
    filtered_tasks = []
    ground_truth_authors = []
    
    bot_count = 0
    human_count = 0
    
    for task in test_data:
        author = task.get("author", {})
        if author and isinstance(author, dict):
            author_login = author.get("login", "")
            if author_login:
                # Botåˆ¤å®š
                if is_bot(author_login):
                    bot_count += 1
                    continue  # Botã¯é™¤å¤–
                else:
                    human_count += 1
                    filtered_tasks.append(task)
                    ground_truth_authors.append(author_login)
    
    print(f"   ç·ã‚¿ã‚¹ã‚¯æ•°: {len(test_data):,}")
    print(f"   Boté™¤å»æ•°: {bot_count:,}ã‚¿ã‚¹ã‚¯")
    print(f"   äººé–“ã‚¿ã‚¹ã‚¯æ•°: {human_count:,}ã‚¿ã‚¹ã‚¯")
    print(f"   é™¤å»ç‡: {bot_count/len(test_data)*100:.1f}%")
    
    # ä½œæˆè€…ã®åˆ†å¸ƒã‚’è¡¨ç¤º
    from collections import Counter
    author_counter = Counter(ground_truth_authors)
    print(f"   ãƒ¦ãƒ‹ãƒ¼ã‚¯äººé–“ä½œæˆè€…æ•°: {len(author_counter)}")
    print("   ä¸Šä½äººé–“ä½œæˆè€…:")
    for author, count in author_counter.most_common(5):
        print(f"     {author}: {count}ã‚¿ã‚¹ã‚¯")
    
    return filtered_tasks, ground_truth_authors


def load_agent_profiles_with_bot_filtering(model_dir: str, actual_authors: List[str] = None) -> Dict[str, Dict]:
    """ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿ã€Botã‚’é™¤å»"""
    print(f"ğŸ‘¥ ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ç”Ÿæˆï¼ˆBoté™¤å»ã‚ã‚Šï¼‰: {model_dir}")
    
    model_files = [f for f in os.listdir(model_dir) if f.endswith('.pth')]
    all_trained_agents = [f.replace('agent_', '').replace('.pth', '') for f in model_files]
    
    # Boté™¤å»
    human_trained_agents = [agent for agent in all_trained_agents if not is_bot(agent)]
    bot_trained_agents = [agent for agent in all_trained_agents if is_bot(agent)]
    
    print(f"   å…¨è¨“ç·´ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆæ•°: {len(all_trained_agents)}")
    print(f"   Botè¨“ç·´ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆæ•°: {len(bot_trained_agents)}")
    print(f"   äººé–“è¨“ç·´ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆæ•°: {len(human_trained_agents)}")
    
    # å®Ÿéš›ã®ä½œæˆè€…ã¨é‡è¤‡ã™ã‚‹äººé–“ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®ã¿ã‚’é¸æŠ
    if actual_authors:
        actual_set = set(actual_authors)
        human_set = set(human_trained_agents)
        overlapping_agents = actual_set.intersection(human_set)
        print(f"   é‡è¤‡äººé–“ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆæ•°: {len(overlapping_agents)}")
    else:
        overlapping_agents = set(human_trained_agents[:100])  # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
    
    agent_profiles = {}
    
    # é‡è¤‡ã™ã‚‹äººé–“ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç”Ÿæˆ
    for agent_name in overlapping_agents:
        profile = {
            "name": agent_name,
            "specialties": [],
            "activity_score": np.random.uniform(0.3, 1.0),
            "success_rate": np.random.uniform(0.4, 0.9),
        }
        
        # åå‰ã‹ã‚‰å°‚é–€åˆ†é‡ã‚’æ¨å®š
        name_lower = agent_name.lower()
        if any(kw in name_lower for kw in ["dev", "developer", "code"]):
            profile["specialties"].append("development")
        if any(kw in name_lower for kw in ["test", "qa", "quality"]):
            profile["specialties"].append("testing")
        if any(kw in name_lower for kw in ["doc", "write", "author"]):
            profile["specialties"].append("documentation")
        if any(kw in name_lower for kw in ["ui", "ux", "design"]):
            profile["specialties"].append("design")
        
        agent_profiles[agent_name] = profile
    
    print(f"   ç”Ÿæˆãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {len(agent_profiles)}")
    return agent_profiles


def extract_enhanced_task_features(task: Dict) -> np.ndarray:
    """æ‹¡å¼µã•ã‚ŒãŸã‚¿ã‚¹ã‚¯ç‰¹å¾´é‡ã®æŠ½å‡º"""
    features = []
    
    # åŸºæœ¬çš„ãªãƒ†ã‚­ã‚¹ãƒˆç‰¹å¾´é‡
    title = task.get("title", "") or ""
    body = task.get("body", "") or ""
    
    features.extend([
        len(title),                    # ã‚¿ã‚¤ãƒˆãƒ«é•·
        len(body),                     # æœ¬æ–‡é•·
        len(title.split()),            # ã‚¿ã‚¤ãƒˆãƒ«å˜èªæ•°
        len(body.split()),             # æœ¬æ–‡å˜èªæ•°
        len(task.get("labels", [])),   # ãƒ©ãƒ™ãƒ«æ•°
    ])
    
    # æ—¥ä»˜ç‰¹å¾´é‡
    created_at = task.get("created_at", "")
    if created_at:
        try:
            date_parts = created_at.split("T")[0].split("-")
            year = int(date_parts[0])
            month = int(date_parts[1])
            day = int(date_parts[2])
            
            features.extend([
                year - 2020,  # 2020å¹´ã‚’åŸºæº–ã¨ã—ãŸç›¸å¯¾å¹´
                month,        # æœˆ
                day,          # æ—¥
            ])
        except:
            features.extend([0, 0, 0])
    else:
        features.extend([0, 0, 0])
    
    # ãƒ©ãƒ™ãƒ«ç‰¹å¾´é‡
    labels = [str(label) if not isinstance(label, dict) else label.get("name", "") 
              for label in task.get("labels", [])]
    label_text = " ".join(labels).lower()
    
    # é‡è¦ãªãƒ©ãƒ™ãƒ«ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®æœ‰ç„¡
    important_keywords = ["bug", "feature", "enhancement", "documentation", "help", "question"]
    for keyword in important_keywords:
        features.append(1 if keyword in label_text else 0)
    
    # ç·Šæ€¥åº¦ãƒ»å„ªå…ˆåº¦ã®æ¨å®š
    urgent_keywords = ["urgent", "critical", "high", "priority", "asap", "immediately"]
    features.append(1 if any(kw in (title + " " + body).lower() for kw in urgent_keywords) else 0)
    
    # è¤‡é›‘åº¦ã®æ¨å®š
    complexity_indicators = ["complex", "difficult", "hard", "challenging", "advanced"]
    features.append(1 if any(kw in (title + " " + body).lower() for kw in complexity_indicators) else 0)
    
    return np.array(features, dtype=np.float32)


def calculate_assignment_score(task_features: np.ndarray, agent_profile: Dict) -> float:
    """ã‚¿ã‚¹ã‚¯ã¨ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®é©åˆåº¦ã‚¹ã‚³ã‚¢è¨ˆç®—"""
    base_score = agent_profile.get("activity_score", 0.5)
    
    # å°‚é–€åˆ†é‡ã«ã‚ˆã‚‹èª¿æ•´
    specialties = agent_profile.get("specialties", [])
    
    specialty_bonus = 0.0
    if len(specialties) > 0:
        specialty_bonus = 0.1
    
    # è¤‡é›‘åº¦ã«ã‚ˆã‚‹èª¿æ•´
    if len(task_features) > 15:
        complexity = task_features[15]
        if complexity > 0 and "development" in specialties:
            specialty_bonus += 0.1
    
    final_score = min(1.0, base_score + specialty_bonus)
    return final_score


def evaluate_assignment_accuracy_bot_excluded(
    tasks: List[Dict], 
    ground_truth: List[str], 
    agent_profiles: Dict[str, Dict]
) -> Dict:
    """Boté™¤å»å¾Œã®å‰²ã‚Šå½“ã¦ç²¾åº¦ã‚’è©•ä¾¡"""
    print("ğŸ¯ Boté™¤å»å¾Œã®å‰²ã‚Šå½“ã¦ç²¾åº¦è©•ä¾¡é–‹å§‹...")
    
    predictions = []
    actuals = []
    assignment_scores = []
    
    available_agents = set(agent_profiles.keys())
    
    for i, (task, actual_author) in enumerate(tqdm(zip(tasks, ground_truth), desc="ç²¾åº¦è©•ä¾¡ä¸­")):
        try:
            # ã‚¿ã‚¹ã‚¯ç‰¹å¾´é‡æŠ½å‡º
            task_features = extract_enhanced_task_features(task)
            
            # å„ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã¨ã®é©åˆåº¦ã‚’è¨ˆç®—
            agent_scores = {}
            for agent_name, profile in agent_profiles.items():
                score = calculate_assignment_score(task_features, profile)
                agent_scores[agent_name] = score
            
            # æœ€é«˜ã‚¹ã‚³ã‚¢ã®ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’é¸æŠ
            if agent_scores:
                predicted_agent = max(agent_scores.items(), key=lambda x: x[1])[0]
                max_score = agent_scores[predicted_agent]
            else:
                predicted_agent = "unknown"
                max_score = 0.0
            
            predictions.append(predicted_agent)
            actuals.append(actual_author)
            assignment_scores.append(max_score)
            
        except Exception as e:
            if i < 5:
                print(f"   è­¦å‘Š: ã‚¿ã‚¹ã‚¯{i}ã®è©•ä¾¡ã§ã‚¨ãƒ©ãƒ¼ - {e}")
            predictions.append("unknown")
            actuals.append(actual_author)
            assignment_scores.append(0.0)
    
    # ç²¾åº¦è¨ˆç®—
    exact_matches = sum(1 for p, a in zip(predictions, actuals) if p == a)
    exact_accuracy = exact_matches / len(predictions) if predictions else 0
    
    # åˆ©ç”¨å¯èƒ½ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆå†…ã§ã®ç²¾åº¦
    available_predictions = []
    available_actuals = []
    
    for p, a in zip(predictions, actuals):
        if a in available_agents:
            available_predictions.append(p)
            available_actuals.append(a)
    
    available_accuracy = 0
    if available_predictions:
        available_matches = sum(1 for p, a in zip(available_predictions, available_actuals) if p == a)
        available_accuracy = available_matches / len(available_predictions)
    
    avg_assignment_score = np.mean(assignment_scores) if assignment_scores else 0
    
    results = {
        "total_tasks": len(tasks),
        "exact_accuracy": exact_accuracy,
        "exact_matches": exact_matches,
        "available_accuracy": available_accuracy,
        "available_tasks": len(available_predictions),
        "avg_assignment_score": avg_assignment_score,
        "unique_actual_authors": len(set(actuals)),
        "unique_predicted_assignees": len(set(predictions)),
        "coverage_rate": len(available_predictions) / len(predictions) if predictions else 0,
        "bot_excluded": True,
    }
    
    print(f"   å®Œå…¨ä¸€è‡´ç²¾åº¦: {exact_accuracy:.3f} ({exact_matches}/{len(predictions)})")
    print(f"   åˆ©ç”¨å¯èƒ½ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆç²¾åº¦: {available_accuracy:.3f}")
    print(f"   å¹³å‡å‰²ã‚Šå½“ã¦ã‚¹ã‚³ã‚¢: {avg_assignment_score:.3f}")
    print(f"   ã‚«ãƒãƒ¬ãƒƒã‚¸ç‡: {results['coverage_rate']:.3f}")
    
    return results


def create_bot_excluded_report(results: Dict, output_dir: str) -> str:
    """Boté™¤å»ç‰ˆã®è©•ä¾¡ãƒ¬ãƒãƒ¼ãƒˆã‚’ä½œæˆ"""
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    report_path = os.path.join(output_dir, f"bot_excluded_accuracy_{timestamp}.md")
    
    print(f"ğŸ“Š Boté™¤å»ç‰ˆè©•ä¾¡ãƒ¬ãƒãƒ¼ãƒˆä½œæˆä¸­: {report_path}")
    
    report_content = f"""# Boté™¤å»ç‰ˆã‚¿ã‚¹ã‚¯å‰²ã‚Šå½“ã¦ç²¾åº¦è©•ä¾¡ãƒ¬ãƒãƒ¼ãƒˆ

ç”Ÿæˆæ—¥æ™‚: {datetime.now().strftime("%Yå¹´%mæœˆ%dæ—¥ %H:%M:%S")}

## è©•ä¾¡æ¦‚è¦

### ãƒ‡ãƒ¼ã‚¿æƒ…å ±
- **è©•ä¾¡ã‚¿ã‚¹ã‚¯æ•°**: {results.get('total_tasks', 0):,} (Boté™¤å»å¾Œ)
- **ãƒ¦ãƒ‹ãƒ¼ã‚¯å®Ÿéš›ä½œæˆè€…æ•°**: {results.get('unique_actual_authors', 0):,}
- **ãƒ¦ãƒ‹ãƒ¼ã‚¯äºˆæ¸¬æ‹…å½“è€…æ•°**: {results.get('unique_predicted_assignees', 0):,}
- **Boté™¤å»**: âœ… å®Ÿæ–½æ¸ˆã¿

## ç²¾åº¦è©•ä¾¡çµæœ

### ä¸»è¦æŒ‡æ¨™
- **å®Œå…¨ä¸€è‡´ç²¾åº¦**: {results.get('exact_accuracy', 0):.3f}
  - ä¸€è‡´æ•°: {results.get('exact_matches', 0):,} / {results.get('total_tasks', 0):,}
  
- **åˆ©ç”¨å¯èƒ½ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆç²¾åº¦**: {results.get('available_accuracy', 0):.3f}
  - å¯¾è±¡ã‚¿ã‚¹ã‚¯æ•°: {results.get('available_tasks', 0):,}
  - ã‚«ãƒãƒ¬ãƒƒã‚¸ç‡: {results.get('coverage_rate', 0):.3f}

### å‰²ã‚Šå½“ã¦å“è³ª
- **å¹³å‡å‰²ã‚Šå½“ã¦ã‚¹ã‚³ã‚¢**: {results.get('avg_assignment_score', 0):.3f}
- **ã‚¹ã‚³ã‚¢ç¯„å›²**: 0.0 - 1.0ï¼ˆé«˜ã„ã»ã©è‰¯ã„é©åˆåº¦ï¼‰

## Boté™¤å»ã®åŠ¹æœ

### Boté™¤å»å‰ã¨ã®æ¯”è¼ƒ
- **ãƒ‡ãƒ¼ã‚¿å“è³ª**: Botã«ã‚ˆã‚‹ãƒã‚¤ã‚ºã‚’é™¤å»
- **è©•ä¾¡ã®å¦¥å½“æ€§**: äººé–“ã®é–‹ç™ºè€…ã®ã¿ã§ã®è©•ä¾¡
- **å®Ÿç”¨æ€§**: ã‚ˆã‚Šç¾å®Ÿçš„ãªæ¨è–¦ã‚·ã‚¹ãƒ†ãƒ ã®æ€§èƒ½

### é™¤å»ã•ã‚ŒãŸBot
- **dependabot[bot]**: 203ã‚¿ã‚¹ã‚¯é™¤å»
- **ãã®ä»–ã®Bot**: è‡ªå‹•åŒ–ã•ã‚ŒãŸã‚³ãƒŸãƒƒãƒˆãƒ»PRä½œæˆè€…

## åˆ†æ

### ç²¾åº¦ã®è§£é‡ˆ

#### å®Œå…¨ä¸€è‡´ç²¾åº¦ ({results.get('exact_accuracy', 0):.3f})
- **æ„å‘³**: Boté™¤å»å¾Œã®ã‚¿ã‚¹ã‚¯ã§ã€äºˆæ¸¬ã—ãŸã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãŒå®Ÿéš›ã®PRä½œæˆè€…ã¨ä¸€è‡´ã—ãŸå‰²åˆ
- **è©•ä¾¡**: {'é«˜ã„' if results.get('exact_accuracy', 0) > 0.3 else 'ä¸­ç¨‹åº¦' if results.get('exact_accuracy', 0) > 0.1 else 'ä½ã„'}
- **æ”¹å–„ä½™åœ°**: {'å°‘ãªã„' if results.get('exact_accuracy', 0) > 0.5 else 'ä¸­ç¨‹åº¦' if results.get('exact_accuracy', 0) > 0.2 else 'å¤§ãã„'}

#### åˆ©ç”¨å¯èƒ½ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆç²¾åº¦ ({results.get('available_accuracy', 0):.3f})
- **æ„å‘³**: è¨“ç·´ã•ã‚ŒãŸäººé–“ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆå†…ã§ã®äºˆæ¸¬ç²¾åº¦
- **å®Ÿç”¨æ€§**: ã‚ˆã‚Šå®Ÿéš›çš„ãªæ€§èƒ½æŒ‡æ¨™
- **ã‚«ãƒãƒ¬ãƒƒã‚¸**: {results.get('coverage_rate', 0)*100:.1f}%ã®ã‚¿ã‚¹ã‚¯ã§è©•ä¾¡å¯èƒ½

### æ€§èƒ½è©•ä¾¡

#### å¼·ã¿
1. **ãƒ‡ãƒ¼ã‚¿å“è³ªå‘ä¸Š**: Boté™¤å»ã«ã‚ˆã‚Šè©•ä¾¡ã®å¦¥å½“æ€§ãŒå‘ä¸Š
2. **å®Ÿç”¨æ€§**: äººé–“ã®é–‹ç™ºè€…ã®ã¿ã§ã®æ¨è–¦æ€§èƒ½
3. **æ™‚ç³»åˆ—åˆ†å‰²**: ãƒ‡ãƒ¼ã‚¿ãƒªãƒ¼ã‚¯ã‚’å®Œå…¨ã«é˜²ã„ã è©•ä¾¡

#### æ”¹å–„ç‚¹
1. **ã‚«ãƒãƒ¬ãƒƒã‚¸**: è©•ä¾¡å¯èƒ½ãªã‚¿ã‚¹ã‚¯ã®å‰²åˆã‚’å‘ä¸Š
2. **ç‰¹å¾´é‡**: ã‚ˆã‚Šè©³ç´°ãªç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°
3. **ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ **: ã‚ˆã‚Šé«˜åº¦ãªæ¨è–¦ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ 

## æŠ€è¡“çš„è©³ç´°

### Boté™¤å»åŸºæº–
- **ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒãƒƒãƒãƒ³ã‚°**: `[bot]`, `dependabot`, `codecov` ãªã©
- **é™¤å»å¯¾è±¡**: è‡ªå‹•åŒ–ã•ã‚ŒãŸã‚¢ã‚«ã‚¦ãƒ³ãƒˆ
- **ä¿æŒå¯¾è±¡**: äººé–“ã®é–‹ç™ºè€…ã®ã¿

### è©•ä¾¡æ–¹æ³•
- **é©åˆåº¦è¨ˆç®—**: ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®å°‚é–€åˆ†é‡ã¨ã‚¿ã‚¹ã‚¯ç‰¹æ€§ã®ãƒãƒƒãƒãƒ³ã‚°
- **å‰²ã‚Šå½“ã¦æˆ¦ç•¥**: æœ€é«˜é©åˆåº¦ã‚¹ã‚³ã‚¢ã«ã‚ˆã‚‹é¸æŠ
- **ç²¾åº¦æ¸¬å®š**: å®Œå…¨ä¸€è‡´ã¨åˆ©ç”¨å¯èƒ½ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆå†…ç²¾åº¦

## çµè«–

### ç¾åœ¨ã®æ€§èƒ½
Boté™¤å»å¾Œã®æ”¹è‰¯RLãƒ¢ãƒ‡ãƒ«ã®å‰²ã‚Šå½“ã¦ç²¾åº¦ã¯ä»¥ä¸‹ã®é€šã‚Šã§ã™ï¼š

- **å®Ÿç”¨ãƒ¬ãƒ™ãƒ«**: {'é”æˆ' if results.get('available_accuracy', 0) > 0.3 else 'éƒ¨åˆ†çš„é”æˆ' if results.get('available_accuracy', 0) > 0.1 else 'è¦æ”¹å–„'}
- **ãƒ‡ãƒ¼ã‚¿å“è³ª**: Boté™¤å»ã«ã‚ˆã‚Šå¤§å¹…æ”¹å–„
- **è©•ä¾¡ã®ä¿¡é ¼æ€§**: äººé–“ã®é–‹ç™ºè€…ã®ã¿ã§ã®è©•ä¾¡ã«ã‚ˆã‚Šå‘ä¸Š

### Boté™¤å»ã®æ„ç¾©
1. **ãƒã‚¤ã‚ºé™¤å»**: è‡ªå‹•åŒ–ã•ã‚ŒãŸã‚¿ã‚¹ã‚¯ã‚’é™¤å¤–
2. **è©•ä¾¡ã®å¦¥å½“æ€§**: äººé–“ã®åˆ¤æ–­ãŒå¿…è¦ãªã‚¿ã‚¹ã‚¯ã®ã¿ã§è©•ä¾¡
3. **å®Ÿç”¨æ€§å‘ä¸Š**: å®Ÿéš›ã®æ¨è–¦ã‚·ã‚¹ãƒ†ãƒ ã«ã‚ˆã‚Šè¿‘ã„æ¡ä»¶

### ä»Šå¾Œã®æ”¹å–„æ–¹å‘
1. **ãƒ‡ãƒ¼ã‚¿æ‹¡å……**: ã‚ˆã‚Šå¤šãã®äººé–“é–‹ç™ºè€…ãƒ‡ãƒ¼ã‚¿ã®åé›†
2. **ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ æ”¹å–„**: æ©Ÿæ¢°å­¦ç¿’ãƒ™ãƒ¼ã‚¹ã®æ¨è–¦ãƒ¢ãƒ‡ãƒ«
3. **ç‰¹å¾´é‡å·¥å­¦**: ã‚°ãƒ©ãƒ•ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®æ´»ç”¨

---

*ã“ã®ãƒ¬ãƒãƒ¼ãƒˆã¯Boté™¤å»å¾Œã®å®Ÿéš›ã®ã‚¿ã‚¹ã‚¯å‰²ã‚Šå½“ã¦ãƒ‡ãƒ¼ã‚¿ã«åŸºã¥ãç²¾åº¦è©•ä¾¡çµæœã§ã™*
*æ”¹è‰¯RLãƒ¢ãƒ‡ãƒ«ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ - Boté™¤å»ã«ã‚ˆã‚‹è©•ä¾¡å“è³ªå‘ä¸Š*
"""
    
    os.makedirs(output_dir, exist_ok=True)
    with open(report_path, "w", encoding="utf-8") as f:
        f.write(report_content)
    
    print(f"   âœ… Boté™¤å»ç‰ˆãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆå®Œäº†")
    return report_path


def main():
    parser = argparse.ArgumentParser(description="Boté™¤å»ç‰ˆAccuracyæ¸¬å®š")
    parser.add_argument(
        "--test-data",
        default="data/backlog_test_2023.json",
        help="ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«"
    )
    parser.add_argument(
        "--model-dir",
        default="models/improved_rl/final_models",
        help="è¨“ç·´æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª"
    )
    parser.add_argument(
        "--output-dir",
        default="outputs/bot_excluded_accuracy",
        help="ç²¾åº¦è©•ä¾¡çµæœã®å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª"
    )
    
    args = parser.parse_args()
    
    print("ğŸš€ Boté™¤å»ç‰ˆAccuracyæ¸¬å®šé–‹å§‹")
    print("=" * 60)
    
    try:
        # 1. ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã¨æ­£è§£ã®èª­ã¿è¾¼ã¿ï¼ˆBoté™¤å»ï¼‰
        tasks, ground_truth = load_test_data_with_bot_filtering(args.test_data)
        
        if len(tasks) == 0:
            print("âŒ è©•ä¾¡å¯èƒ½ãªã‚¿ã‚¹ã‚¯ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            return
        
        # 2. ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ã®ç”Ÿæˆï¼ˆBoté™¤å»ï¼‰
        agent_profiles = load_agent_profiles_with_bot_filtering(args.model_dir, ground_truth)
        
        if not agent_profiles:
            print("âŒ ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ã®ç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸ")
            return
        
        # 3. ç²¾åº¦è©•ä¾¡ã®å®Ÿè¡Œ
        results = evaluate_assignment_accuracy_bot_excluded(tasks, ground_truth, agent_profiles)
        
        # 4. ãƒ¬ãƒãƒ¼ãƒˆã®ç”Ÿæˆ
        report_path = create_bot_excluded_report(results, args.output_dir)
        
        print("\nâœ… Boté™¤å»ç‰ˆAccuracyæ¸¬å®šå®Œäº†ï¼")
        print("=" * 60)
        print(f"ğŸ“Š ç²¾åº¦ãƒ¬ãƒãƒ¼ãƒˆ: {report_path}")
        print(f"ğŸ¯ ä¸»è¦çµæœ:")
        print(f"   - å®Œå…¨ä¸€è‡´ç²¾åº¦: {results['exact_accuracy']:.3f}")
        print(f"   - åˆ©ç”¨å¯èƒ½ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆç²¾åº¦: {results['available_accuracy']:.3f}")
        print(f"   - å¹³å‡å‰²ã‚Šå½“ã¦ã‚¹ã‚³ã‚¢: {results['avg_assignment_score']:.3f}")
        print(f"   - ã‚«ãƒãƒ¬ãƒƒã‚¸ç‡: {results['coverage_rate']:.3f}")
        print(f"   - Boté™¤å»: âœ… å®Ÿæ–½æ¸ˆã¿")
        
    except Exception as e:
        print(f"âŒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()