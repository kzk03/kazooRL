#!/usr/bin/env python3
"""
æ”¹å–„ç‰ˆGATç‰¹å¾´é‡çµ±åˆå¼·åŒ–å­¦ç¿’æ¨è–¦ã‚·ã‚¹ãƒ†ãƒ 

ä¿®æ­£ç‚¹:
1. è¡Œå‹•ç©ºé–“ã‚’å®Ÿéš›ã®è¨“ç·´é–‹ç™ºè€…ã«åˆ¶é™
2. ã‚ˆã‚Šå¯†ãªå ±é…¬è¨­è¨ˆ
3. GATç‰¹å¾´é‡ã®åŠ¹æœçš„æ´»ç”¨
4. ã‚·ãƒ³ãƒ—ãƒ«é¡ä¼¼åº¦ãƒ™ãƒ¼ã‚¹ã¨ã®æ­£ç¢ºãªæ¯”è¼ƒ
"""

import argparse
import json
import pickle
import sys
from collections import Counter, defaultdict
from datetime import datetime
from pathlib import Path

import gymnasium as gym
import numpy as np
import pandas as pd
import torch
import yaml
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import StandardScaler
from stable_baselines3 import DQN, PPO
from stable_baselines3.common.callbacks import EvalCallback
from stable_baselines3.common.vec_env import DummyVecEnv

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
sys.path.append('/Users/kazuki-h/rl/kazoo')
sys.path.append('/Users/kazuki-h/rl/kazoo/src')

from src.kazoo.envs.task import Task
from src.kazoo.features.feature_extractor import FeatureExtractor


class ImprovedGATRLEnvironment(gym.Env):
    """
    æ”¹å–„ç‰ˆGATç‰¹å¾´é‡çµ±åˆå¼·åŒ–å­¦ç¿’ç’°å¢ƒ
    - å®Ÿéš›ã®è¨“ç·´é–‹ç™ºè€…ã®ã¿ã‚’è¡Œå‹•ç©ºé–“ã«å«ã‚ã‚‹
    - GATç‰¹å¾´é‡ã‚’æœ€å¤§é™æ´»ç”¨
    - å¯†ãªå ±é…¬è¨­è¨ˆ
    """
    
    def __init__(self, config, training_data, dev_profiles):
        super().__init__()
        
        self.config = config
        self.training_data = training_data
        self.dev_profiles = dev_profiles
        
        # ç‰¹å¾´é‡æŠ½å‡ºå™¨ã®åˆæœŸåŒ–
        self.setup_feature_extractor()
        
        # å®Ÿéš›ã®è¨“ç·´é–‹ç™ºè€…ã®ã¿æŠ½å‡º
        self.active_developers = self._extract_active_developers()
        self.num_developers = len(self.active_developers)
        
        print(f"ğŸ¯ è¡Œå‹•ç©ºé–“ã‚’å®Ÿè¨“ç·´é–‹ç™ºè€…ã«åˆ¶é™: {self.num_developers}äºº")
        
        # è¡Œå‹•ç©ºé–“: å®Ÿéš›ã®é–‹ç™ºè€…ã®ã¿
        self.action_space = gym.spaces.Discrete(self.num_developers)
        
        # è¦³æ¸¬ç©ºé–“: GATç‰¹å¾´é‡ã®ã¿ï¼ˆé–‹ç™ºè€…é¸æŠè‚¢ã¯é™¤å¤–ï¼‰
        feature_dim = len(self.feature_extractor.feature_names)
        self.observation_space = gym.spaces.Box(
            low=-np.inf, high=np.inf,
            shape=(feature_dim,),  # GATç‰¹å¾´é‡ã®ã¿
            dtype=np.float32
        )
        
        # å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ãƒšã‚¢ã‚’æŠ½å‡º
        self.training_pairs = self._extract_training_pairs()
        
        # ç¾åœ¨ã®ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰çŠ¶æ…‹
        self.current_pair = None
        self.episode_count = 0
        
        print(f"ğŸ¤– æ”¹å–„ç‰ˆGAT-RLç’°å¢ƒåˆæœŸåŒ–å®Œäº†")
        print(f"   ã‚¢ã‚¯ãƒ†ã‚£ãƒ–é–‹ç™ºè€…: {self.num_developers}")
        print(f"   è¦³æ¸¬æ¬¡å…ƒ: {self.observation_space.shape[0]} (GATç‰¹å¾´é‡ã®ã¿)")
        print(f"   å­¦ç¿’ãƒšã‚¢æ•°: {len(self.training_pairs)}")
    
    def setup_feature_extractor(self):
        """GATç‰¹å¾´é‡æŠ½å‡ºå™¨ã®åˆæœŸåŒ–"""
        print("ğŸ”§ GATç‰¹å¾´é‡æŠ½å‡ºå™¨åˆæœŸåŒ–ä¸­...")
        
        # è¨­å®šã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®ä½œæˆ
        class DictObj:
            def __init__(self, d):
                for k, v in d.items():
                    if isinstance(v, dict):
                        setattr(self, k, DictObj(v))
                    else:
                        setattr(self, k, v)
            
            def get(self, key, default=None):
                return getattr(self, key, default)
        
        cfg = DictObj(self.config)
        self.feature_extractor = FeatureExtractor(cfg)
        
        # IRLé‡ã¿ã‚’èª­ã¿è¾¼ã¿
        self.irl_weights = self._load_irl_weights()
        
        print(f"   ç‰¹å¾´é‡æ¬¡å…ƒ: {len(self.feature_extractor.feature_names)}")
        print(f"   GATç‰¹å¾´é‡: {sum(1 for name in self.feature_extractor.feature_names if 'gat_' in name)}")
    
    def _load_irl_weights(self):
        """IRLå­¦ç¿’æ¸ˆã¿é‡ã¿ã‚’èª­ã¿è¾¼ã¿"""
        weights_path = self.config.get('irl', {}).get('output_weights_path')
        
        if weights_path and Path(weights_path).exists():
            try:
                weights = np.load(weights_path)
                print(f"âœ… IRLé‡ã¿èª­ã¿è¾¼ã¿: {weights_path} ({weights.shape})")
                return torch.tensor(weights, dtype=torch.float32)
            except Exception as e:
                print(f"âš ï¸ IRLé‡ã¿èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
        
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ãƒ©ãƒ³ãƒ€ãƒ é‡ã¿
        feature_dim = len(self.feature_extractor.feature_names)
        weights = torch.randn(feature_dim, dtype=torch.float32)
        print(f"âš ï¸ ãƒ©ãƒ³ãƒ€ãƒ é‡ã¿ã‚’ä½¿ç”¨: {weights.shape}")
        return weights
    
    def _extract_active_developers(self):
        """å®Ÿéš›ã«è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã«ç™»å ´ã™ã‚‹é–‹ç™ºè€…ã‚’æŠ½å‡º"""
        active_devs = set()
        
        for task_data in self.training_data:
            # å®Ÿéš›ã®æ‹…å½“è€…ã‚’æŠ½å‡º
            assignee = None
            if 'assignees' in task_data and task_data['assignees']:
                assignee = task_data['assignees'][0].get('login')
            elif 'events' in task_data:
                for event in task_data['events']:
                    if event.get('event') == 'assigned' and event.get('assignee'):
                        assignee = event['assignee'].get('login')
                        break
            
            if assignee and assignee in self.dev_profiles:
                active_devs.add(assignee)
        
        active_list = sorted(list(active_devs))
        print(f"ğŸ“‹ ã‚¢ã‚¯ãƒ†ã‚£ãƒ–é–‹ç™ºè€…æŠ½å‡º: {len(active_list)}äºº")
        
        # é–‹ç™ºè€…çµ±è¨ˆ
        dev_stats = Counter()
        for task_data in self.training_data:
            assignee = None
            if 'assignees' in task_data and task_data['assignees']:
                assignee = task_data['assignees'][0].get('login')
            elif 'events' in task_data:
                for event in task_data['events']:
                    if event.get('event') == 'assigned' and event.get('assignee'):
                        assignee = event['assignee'].get('login')
                        break
            if assignee in active_list:
                dev_stats[assignee] += 1
        
        print("   ä¸Šä½ã‚¢ã‚¯ãƒ†ã‚£ãƒ–é–‹ç™ºè€…:")
        for dev, count in dev_stats.most_common(10):
            print(f"     {dev}: {count} ã‚¿ã‚¹ã‚¯")
        
        return active_list
    
    def _extract_training_pairs(self):
        """å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ã‚¿ã‚¹ã‚¯-é–‹ç™ºè€…ãƒšã‚¢ã‚’æŠ½å‡º"""
        pairs = []
        
        for task_data in self.training_data:
            task_id = task_data.get('id') or task_data.get('number')
            if not task_id:
                continue
            
            # å®Ÿéš›ã®æ‹…å½“è€…ã‚’æŠ½å‡º
            assignee = None
            if 'assignees' in task_data and task_data['assignees']:
                assignee = task_data['assignees'][0].get('login')
            elif 'events' in task_data:
                for event in task_data['events']:
                    if event.get('event') == 'assigned' and event.get('assignee'):
                        assignee = event['assignee'].get('login')
                        break
            
            if assignee and assignee in self.active_developers:
                pairs.append({
                    'task_data': task_data,
                    'developer': assignee,
                    'task_id': task_id
                })
        
        return pairs
    
    def reset(self, seed=None, options=None):
        """ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã‚’ãƒªã‚»ãƒƒãƒˆ"""
        super().reset(seed=seed)
        
        # ãƒ©ãƒ³ãƒ€ãƒ ã«ãƒšã‚¢ã‚’é¸æŠ
        if self.training_pairs:
            self.current_pair = np.random.choice(self.training_pairs)
        else:
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            self.current_pair = {
                'task_data': self.training_data[0] if self.training_data else {},
                'developer': self.active_developers[0] if self.active_developers else 'unknown',
                'task_id': 'dummy'
            }
        
        # è¦³æ¸¬ã‚’ç”Ÿæˆ
        obs = self._get_observation()
        
        self.episode_count += 1
        return obs, {}
    
    def step(self, action):
        """ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚’å®Ÿè¡Œ"""
        if action >= self.num_developers:
            # ç„¡åŠ¹ãªã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã«å¯¾ã™ã‚‹é‡ã„ãƒšãƒŠãƒ«ãƒ†ã‚£
            reward = -50.0  # ã•ã‚‰ã«é‡ã„ãƒšãƒŠãƒ«ãƒ†ã‚£
            terminated = True
            obs = np.zeros(self.observation_space.shape[0], dtype=np.float32)
            return obs, reward, terminated, False, {}
        
        selected_dev = self.active_developers[action]
        actual_dev = self.current_pair['developer']
        
        # å ±é…¬è¨ˆç®—ï¼ˆæ”¹å–„ç‰ˆï¼‰
        reward = self._calculate_improved_reward(selected_dev, actual_dev)
        
        # ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰çµ‚äº†
        terminated = True
        obs = np.zeros(self.observation_space.shape[0], dtype=np.float32)
        
        info = {
            'selected_dev': selected_dev,
            'actual_dev': actual_dev,
            'task_id': self.current_pair['task_id'],
            'correct': selected_dev == actual_dev,
            'reward_breakdown': getattr(self, 'last_reward_breakdown', {})
        }
        
        return obs, reward, terminated, False, info
    
    def _get_observation(self):
        """æ”¹å–„ç‰ˆè¦³æ¸¬ç”Ÿæˆï¼ˆGATç‰¹å¾´é‡ã®ã¿ï¼‰"""
        try:
            # ã‚¿ã‚¹ã‚¯ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆä½œæˆ
            task_obj = Task(self.current_pair['task_data'])
            
            # å®Ÿéš›ã®é–‹ç™ºè€…ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ
            actual_dev_name = self.current_pair['developer']
            dev_profile = self.dev_profiles.get(actual_dev_name, {})
            developer_obj = {"name": actual_dev_name, "profile": dev_profile}
            
            # ãƒ€ãƒŸãƒ¼ç’°å¢ƒä½œæˆ
            dummy_env = type('DummyEnv', (), {
                'backlog': [task_obj],
                'dev_profiles': self.dev_profiles,
                'assignments': {},
                'dev_action_history': {}
            })()
            
            # GATç‰¹å¾´é‡æŠ½å‡ºï¼ˆ62æ¬¡å…ƒå›ºå®šï¼‰
            gat_features = self.feature_extractor.get_features(
                task_obj, developer_obj, dummy_env
            )
            
            return gat_features.astype(np.float32)
            
        except Exception as e:
            print(f"âš ï¸ è¦³æ¸¬å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ã‚¼ãƒ­ãƒ™ã‚¯ãƒˆãƒ«
            return np.zeros(self.observation_space.shape[0], dtype=np.float32)
    
    def _calculate_improved_reward(self, selected_dev, actual_dev):
        """æ”¹å–„ç‰ˆå ±é…¬è¨ˆç®— - æ­£è§£å ±é…¬ã‚’åœ§å€’çš„ã«é‡è¦ã«"""
        total_reward = 0.0
        reward_breakdown = {}
        
        # 1. æ­£è§£å ±é…¬ (åœ§å€’çš„ã«é‡è¦)
        if selected_dev == actual_dev:
            correct_reward = 100.0  # å¤§å¹…å¢—åŠ : æ­£è§£ãŒæœ€å„ªå…ˆ
            total_reward += correct_reward
            reward_breakdown['correct'] = correct_reward
        else:
            # é–“é•ã„ã«å¯¾ã™ã‚‹æ˜ç¢ºãªãƒšãƒŠãƒ«ãƒ†ã‚£
            wrong_penalty = -20.0  # ãƒšãƒŠãƒ«ãƒ†ã‚£å¼·åŒ–
            total_reward += wrong_penalty
            reward_breakdown['correct'] = wrong_penalty
        
        # 2. GATç‰¹å¾´é‡å ±é…¬ (è£œåŠ©çš„å½¹å‰²ã®ã¿)
        try:
            # é¸æŠã•ã‚ŒãŸé–‹ç™ºè€…ã§ã®GATç‰¹å¾´é‡æŠ½å‡º
            task_obj = Task(self.current_pair['task_data'])
            selected_dev_profile = self.dev_profiles.get(selected_dev, {})
            selected_dev_obj = {"name": selected_dev, "profile": selected_dev_profile}
            
            dummy_env = type('DummyEnv', (), {
                'backlog': [task_obj],
                'dev_profiles': self.dev_profiles,
                'assignments': {},
                'dev_action_history': {}
            })()
            
            # GATç‰¹å¾´é‡æŠ½å‡º
            features = self.feature_extractor.get_features(
                task_obj, selected_dev_obj, dummy_env
            )
            
            # GATé¡ä¼¼åº¦ç‰¹å¾´é‡ (è£œåŠ©çš„)
            gat_similarity_idx = [i for i, name in enumerate(self.feature_extractor.feature_names) 
                                if name == 'gat_similarity']
            if gat_similarity_idx:
                similarity_score = features[gat_similarity_idx[0]]
                similarity_reward = similarity_score * 2.0  # å¤§å¹…å‰Šæ¸›: è£œåŠ©çš„å½¹å‰²
                total_reward += similarity_reward
                reward_breakdown['gat_similarity'] = similarity_reward
            
            # GATå°‚é–€æ€§å ±é…¬ (è£œåŠ©çš„)
            expertise_idx = [i for i, name in enumerate(self.feature_extractor.feature_names) 
                           if name == 'gat_dev_expertise']
            if expertise_idx:
                expertise_score = features[expertise_idx[0]]
                expertise_reward = expertise_score * 1.5  # å¤§å¹…å‰Šæ¸›
                total_reward += expertise_reward
                reward_breakdown['gat_expertise'] = expertise_reward
            
            # GATå”åŠ›å¼·åº¦å ±é…¬ (è£œåŠ©çš„)
            collaboration_idx = [i for i, name in enumerate(self.feature_extractor.feature_names) 
                               if name == 'gat_collaboration_strength']
            if collaboration_idx:
                collab_score = features[collaboration_idx[0]]
                collab_reward = collab_score * 1.0  # å¤§å¹…å‰Šæ¸›
                total_reward += collab_reward
                reward_breakdown['gat_collaboration'] = collab_reward
            
            # IRLé‡ã¿ã¨ã®çµ±åˆå ±é…¬ (è£œåŠ©çš„)
            features_tensor = torch.tensor(features, dtype=torch.float32)
            irl_score = torch.dot(self.irl_weights, features_tensor).item()
            irl_reward = np.tanh(irl_score) * 1.0  # å¤§å¹…å‰Šæ¸›
            total_reward += irl_reward
            reward_breakdown['irl_compatibility'] = irl_reward
            
        except Exception as e:
            print(f"âš ï¸ GATå ±é…¬è¨ˆç®—ã‚¨ãƒ©ãƒ¼: {e}")
            # ã‚¨ãƒ©ãƒ¼æ™‚ã¯GATå ±é…¬ã‚’0ã«
            reward_breakdown['gat_similarity'] = 0.0
            reward_breakdown['gat_expertise'] = 0.0
            reward_breakdown['gat_collaboration'] = 0.0
            reward_breakdown['irl_compatibility'] = 0.0
        
        # å ±é…¬è©³ç´°ã‚’ä¿å­˜
        self.last_reward_breakdown = reward_breakdown
        
        return total_reward


class ImprovedGATRLRecommender:
    """æ”¹å–„ç‰ˆGATç‰¹å¾´é‡çµ±åˆå¼·åŒ–å­¦ç¿’æ¨è–¦ã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self, config_path):
        self.config_path = config_path
        
        with open(config_path, 'r', encoding='utf-8') as f:
            self.config = yaml.safe_load(f)
        
        print("ğŸš€ æ”¹å–„ç‰ˆGATçµ±åˆå¼·åŒ–å­¦ç¿’æ¨è–¦ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å®Œäº†")
    
    def load_data(self, data_path):
        """ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã‚“ã§æ™‚ç³»åˆ—åˆ†å‰²"""
        print("ğŸ“Š ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ä¸­...")
        
        with open(data_path, 'r', encoding='utf-8') as f:
            all_data = json.load(f)
        
        # æ™‚ç³»åˆ—åˆ†å‰²
        training_data = []  # 2022å¹´ä»¥å‰
        test_data = []      # 2023å¹´
        
        for task in all_data:
            created_at = task.get('created_at', '')
            if created_at.startswith('2022'):
                training_data.append(task)
            elif created_at.startswith('2023'):
                test_data.append(task)
        
        print(f"   å­¦ç¿’ãƒ‡ãƒ¼ã‚¿: {len(training_data):,} ã‚¿ã‚¹ã‚¯ (2022å¹´)")
        print(f"   ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿: {len(test_data):,} ã‚¿ã‚¹ã‚¯ (2023å¹´)")
        
        return training_data, test_data
    
    def load_dev_profiles(self):
        """é–‹ç™ºè€…ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿"""
        dev_profiles_path = self.config['env']['dev_profiles_path']
        with open(dev_profiles_path, 'r', encoding='utf-8') as f:
            dev_profiles = yaml.safe_load(f)
        
        print(f"ğŸ“‹ é–‹ç™ºè€…ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿: {len(dev_profiles)} äºº")
        return dev_profiles
    
    def train_improved_rl_agent(self, training_data, dev_profiles, total_timesteps=50000):
        """æ”¹å–„ç‰ˆGATå¼·åŒ–å­¦ç¿’ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’è¨“ç·´"""
        print("ğŸ“ æ”¹å–„ç‰ˆGATçµ±åˆå¼·åŒ–å­¦ç¿’ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆè¨“ç·´é–‹å§‹...")
        
        # ç’°å¢ƒä½œæˆ
        def make_env():
            return ImprovedGATRLEnvironment(self.config, training_data, dev_profiles)
        
        env = DummyVecEnv([make_env])
        
        # PPOã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆï¼ˆæ”¹å–„ç‰ˆï¼‰
        self.rl_agent = PPO(
            "MlpPolicy",
            env,
            verbose=1,
            learning_rate=1e-3,  # å­¦ç¿’ç‡ä¸Šæ˜‡
            n_steps=1024,        # ã‚¹ãƒ†ãƒƒãƒ—æ•°å‰Šæ¸›
            batch_size=128,      # ãƒãƒƒãƒã‚µã‚¤ã‚ºå¢—åŠ 
            n_epochs=20,         # ã‚¨ãƒãƒƒã‚¯æ•°å¢—åŠ 
            gamma=0.95,          # å‰²å¼•ç‡èª¿æ•´
            gae_lambda=0.9,      # GAEèª¿æ•´
            clip_range=0.3,      # ã‚¯ãƒªãƒƒãƒ—ç¯„å›²æ‹¡å¤§
            ent_coef=0.05,       # ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ä¿‚æ•°å¢—åŠ 
            vf_coef=1.0,         # ä¾¡å€¤é–¢æ•°ä¿‚æ•°å¢—åŠ 
            max_grad_norm=1.0,   # å‹¾é…ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°å¢—åŠ 
            device="auto",
            policy_kwargs=dict(
                net_arch=[128, 128, 64],  # ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯èª¿æ•´
                activation_fn=torch.nn.Tanh  # æ´»æ€§åŒ–é–¢æ•°å¤‰æ›´
            )
        )
        
        # è©•ä¾¡ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯
        eval_env = DummyVecEnv([make_env])
        eval_callback = EvalCallback(
            eval_env,
            best_model_save_path="./models/improved_gat_rl_best/",
            log_path="./logs/improved_gat_rl_eval/",
            eval_freq=2000,
            deterministic=True,
            render=False,
            verbose=1
        )
        
        # è¨“ç·´å®Ÿè¡Œ
        print(f"   è¨“ç·´ã‚¹ãƒ†ãƒƒãƒ—: {total_timesteps:,}")
        print(f"   æ¨å®šæ™‚é–“: {total_timesteps / 1000:.1f}åˆ†")
        
        self.rl_agent.learn(
            total_timesteps=total_timesteps,
            callback=eval_callback,
            progress_bar=False
        )
        
        print("âœ… æ”¹å–„ç‰ˆGATçµ±åˆå¼·åŒ–å­¦ç¿’ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆè¨“ç·´å®Œäº†")
    
    def predict_with_improved_gat_rl(self, test_data, dev_profiles):
        """æ”¹å–„ç‰ˆGATå¼·åŒ–å­¦ç¿’ã§äºˆæ¸¬"""
        print("ğŸ¤– æ”¹å–„ç‰ˆGATå¼·åŒ–å­¦ç¿’äºˆæ¸¬å®Ÿè¡Œä¸­...")
        
        # ãƒ†ã‚¹ãƒˆç’°å¢ƒä½œæˆ
        test_env = ImprovedGATRLEnvironment(self.config, test_data, dev_profiles)
        
        predictions = {}
        prediction_scores = {}
        
        # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®å®Ÿéš›ã®å‰²ã‚Šå½“ã¦ã‚’æŠ½å‡ºï¼ˆåŒã˜é–‹ç™ºè€…åˆ¶é™ï¼‰
        test_assignments = {}
        for task_data in test_data:
            task_id = task_data.get('id') or task_data.get('number')
            if not task_id:
                continue
                
            assignee = None
            if 'assignees' in task_data and task_data['assignees']:
                assignee = task_data['assignees'][0].get('login')
            elif 'events' in task_data:
                for event in task_data['events']:
                    if event.get('event') == 'assigned' and event.get('assignee'):
                        assignee = event['assignee'].get('login')
                        break
            
            # ã‚¢ã‚¯ãƒ†ã‚£ãƒ–é–‹ç™ºè€…ã®ã¿å¯¾è±¡
            if assignee and assignee in test_env.active_developers:
                test_assignments[task_id] = assignee
        
        print(f"   äºˆæ¸¬å¯¾è±¡: {len(test_assignments)} ã‚¿ã‚¹ã‚¯ï¼ˆã‚¢ã‚¯ãƒ†ã‚£ãƒ–é–‹ç™ºè€…ã®ã¿ï¼‰")
        
        prediction_count = 0
        
        for task_data in test_data:
            task_id = task_data.get('id') or task_data.get('number')
            if task_id not in test_assignments:
                continue
            
            try:
                # ãƒ†ã‚¹ãƒˆç’°å¢ƒã§ã‚¿ã‚¹ã‚¯ã‚’è¨­å®š
                test_env.current_pair = {
                    'task_data': task_data,
                    'developer': test_assignments[task_id],
                    'task_id': task_id
                }
                
                # è¦³æ¸¬å–å¾—
                obs = test_env._get_observation()
                
                # å¼·åŒ–å­¦ç¿’ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã§äºˆæ¸¬
                action, _ = self.rl_agent.predict(obs, deterministic=True)
                
                if action < len(test_env.active_developers):
                    predicted_dev = test_env.active_developers[action]
                    
                    predictions[task_id] = predicted_dev
                    prediction_scores[task_id] = {
                        'predicted_dev': predicted_dev,
                        'action': int(action),
                        'method': 'improved_gat_reinforcement_learning'
                    }
                    
                    prediction_count += 1
                    
                    if prediction_count % 20 == 0:
                        print(f"   é€²æ—: {prediction_count}/{len(test_assignments)}")
            
            except Exception as e:
                print(f"âš ï¸ ã‚¿ã‚¹ã‚¯ {task_id} ã®æ”¹å–„GAT-RLäºˆæ¸¬ã‚¨ãƒ©ãƒ¼: {e}")
                continue
        
        print(f"   æ”¹å–„GAT-RLäºˆæ¸¬å®Œäº†: {len(predictions)} ã‚¿ã‚¹ã‚¯")
        return predictions, prediction_scores, test_assignments
    
    def compare_with_baseline(self, gat_predictions, test_assignments, baseline_path=None):
        """ã‚·ãƒ³ãƒ—ãƒ«é¡ä¼¼åº¦ãƒ™ãƒ¼ã‚¹ã¨ã®è©³ç´°æ¯”è¼ƒ"""
        print("ğŸ“Š è©³ç´°ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³æ¯”è¼ƒå®Ÿè¡Œä¸­...")
        
        # GAT-RLè©•ä¾¡
        gat_common_tasks = set(gat_predictions.keys()) & set(test_assignments.keys())
        gat_correct = sum(1 for task_id in gat_common_tasks 
                         if gat_predictions[task_id] == test_assignments[task_id])
        gat_accuracy = gat_correct / len(gat_common_tasks) if gat_common_tasks else 0.0
        
        print(f"ğŸ¤– æ”¹å–„ç‰ˆGATå¼·åŒ–å­¦ç¿’ã‚·ã‚¹ãƒ†ãƒ :")
        print(f"   ç²¾åº¦: {gat_accuracy:.3f} ({gat_correct}/{len(gat_common_tasks)})")
        
        # ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³èª­ã¿è¾¼ã¿
        baseline_accuracy = 0.0
        baseline_correct = 0
        baseline_total = 0
        
        if baseline_path and Path(baseline_path).exists():
            try:
                baseline_df = pd.read_csv(baseline_path)
                baseline_accuracy = baseline_df['correct'].mean()
                baseline_correct = baseline_df['correct'].sum()
                baseline_total = len(baseline_df)
                
                print(f"ğŸ“ ã‚·ãƒ³ãƒ—ãƒ«é¡ä¼¼åº¦ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³:")
                print(f"   ç²¾åº¦: {baseline_accuracy:.3f} ({baseline_correct}/{baseline_total})")
                
                improvement = gat_accuracy - baseline_accuracy
                print(f"ğŸ“ˆ æ”¹å–„åº¦: {improvement:+.3f} ({improvement/baseline_accuracy*100:+.1f}%)")
                
            except Exception as e:
                print(f"âš ï¸ ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
        
        return {
            'improved_gat_rl_accuracy': gat_accuracy,
            'improved_gat_rl_correct': gat_correct,
            'improved_gat_rl_total': len(gat_common_tasks),
            'baseline_accuracy': baseline_accuracy,
            'baseline_correct': baseline_correct,
            'baseline_total': baseline_total,
            'improvement': gat_accuracy - baseline_accuracy
        }
    
    def save_improved_gat_rl_model(self, output_dir="models"):
        """æ”¹å–„ç‰ˆGATçµ±åˆå¼·åŒ–å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜"""
        output_dir = Path(output_dir)
        output_dir.mkdir(exist_ok=True)
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # RLã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆä¿å­˜
        rl_model_path = output_dir / f"improved_gat_rl_recommender_{timestamp}.zip"
        self.rl_agent.save(rl_model_path)
        
        print(f"âœ… æ”¹å–„ç‰ˆGATçµ±åˆRLãƒ¢ãƒ‡ãƒ«ä¿å­˜: {rl_model_path}")
        return rl_model_path
    
    def run_full_pipeline(self, data_path, baseline_path=None, total_timesteps=50000):
        """å®Œå…¨ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè¡Œ"""
        print("ğŸš€ æ”¹å–„ç‰ˆGATçµ±åˆå¼·åŒ–å­¦ç¿’æ¨è–¦ã‚·ã‚¹ãƒ†ãƒ å®Ÿè¡Œé–‹å§‹")
        print("=" * 70)
        
        # 1. ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
        training_data, test_data = self.load_data(data_path)
        
        # 2. é–‹ç™ºè€…ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿
        dev_profiles = self.load_dev_profiles()
        
        # 3. æ”¹å–„ç‰ˆå¼·åŒ–å­¦ç¿’ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆè¨“ç·´
        self.train_improved_rl_agent(training_data, dev_profiles, total_timesteps)
        
        # 4. äºˆæ¸¬å®Ÿè¡Œ
        predictions, scores, test_assignments = self.predict_with_improved_gat_rl(test_data, dev_profiles)
        
        # 5. ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã¨ã®æ¯”è¼ƒ
        metrics = self.compare_with_baseline(predictions, test_assignments, baseline_path)
        
        # 6. ãƒ¢ãƒ‡ãƒ«ä¿å­˜
        self.save_improved_gat_rl_model()
        
        print("âœ… æ”¹å–„ç‰ˆGATçµ±åˆå¼·åŒ–å­¦ç¿’æ¨è–¦ã‚·ã‚¹ãƒ†ãƒ å®Œäº†")
        return metrics


def main():
    parser = argparse.ArgumentParser(description='æ”¹å–„ç‰ˆGATçµ±åˆå¼·åŒ–å­¦ç¿’æ¨è–¦ã‚·ã‚¹ãƒ†ãƒ ')
    parser.add_argument('--config', default='configs/unified_rl.yaml',
                       help='è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹')
    parser.add_argument('--data', default='data/backlog.json',
                       help='çµ±åˆãƒ‡ãƒ¼ã‚¿ãƒ‘ã‚¹')
    parser.add_argument('--baseline', 
                       help='ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³çµæœCSVãƒ‘ã‚¹')
    parser.add_argument('--timesteps', type=int, default=50000,
                       help='è¨“ç·´ã‚¹ãƒ†ãƒƒãƒ—æ•°')
    parser.add_argument('--output', default='outputs',
                       help='å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª')
    
    args = parser.parse_args()
    
    # æ”¹å–„ç‰ˆGATçµ±åˆå¼·åŒ–å­¦ç¿’ã‚·ã‚¹ãƒ†ãƒ å®Ÿè¡Œ
    recommender = ImprovedGATRLRecommender(args.config)
    metrics = recommender.run_full_pipeline(
        args.data, args.baseline, args.timesteps
    )
    
    print("\nğŸ¯ æœ€çµ‚çµæœ:")
    for key, value in metrics.items():
        if isinstance(value, (int, float)):
            print(f"   {key}: {value:.3f}")
    
    return 0


if __name__ == "__main__":
    exit(main())
