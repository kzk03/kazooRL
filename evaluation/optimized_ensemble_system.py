#!/usr/bin/env python3
"""
æœ€é©åŒ–ã•ã‚ŒãŸã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«æ¨è–¦ã‚·ã‚¹ãƒ†ãƒ 
feature_optimizedãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’åŸºã«45%â†’50%ã‚’ç›®æŒ‡ã™
"""

import json
import os
from collections import Counter, defaultdict
from typing import Dict, List, Tuple

import numpy as np
import torch
import torch.nn as nn
from advanced_ensemble_system import AdvancedEnsembleSystem, PPOPolicyNetwork, is_bot
from tqdm import tqdm


class OptimizedEnsembleSystem(AdvancedEnsembleSystem):
    """æœ€é©åŒ–ã•ã‚ŒãŸã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«æ¨è–¦ã‚·ã‚¹ãƒ†ãƒ """

    def __init__(self, model_dir: str, test_data_path: str):
        super().__init__(model_dir, test_data_path)
        print("ğŸ”§ æœ€é©åŒ–ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å®Œäº†")

    def ultra_optimized_recommendation(
        self, task_features: torch.Tensor, task: Dict, k: int = 5
    ) -> List[Tuple[str, float]]:
        """ğŸš€ è¶…æœ€é©åŒ–æ¨è–¦ - 45%â†’50%ã‚’ç›®æŒ‡ã™"""

        methods_results = {}

        # 1. åŸºæœ¬ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ï¼ˆæ”¹è‰¯ç‰ˆï¼‰
        basic_scores = {}
        for agent_name, model in self.models.items():
            try:
                ppo_score = model.get_action_score(task_features)
                contribution = self.author_contributions.get(agent_name, 0)
                
                # æ”¹è‰¯ã•ã‚ŒãŸè²¢çŒ®é‡ã‚¹ã‚³ã‚¢ï¼ˆã‚ˆã‚Šç´°ã‹ã„æ®µéšï¼‰
                if contribution >= 500:
                    contribution_score = 1.0
                elif contribution >= 300:
                    contribution_score = 0.95
                elif contribution >= 200:
                    contribution_score = 0.9
                elif contribution >= 100:
                    contribution_score = 0.8
                elif contribution >= 50:
                    contribution_score = 0.65
                elif contribution >= 20:
                    contribution_score = 0.5
                elif contribution >= 10:
                    contribution_score = 0.35
                else:
                    contribution_score = 0.2
                
                similarity_score = self._calculate_task_similarity(task, agent_name)
                
                # æ”¹è‰¯ã•ã‚ŒãŸåŸºæœ¬ã‚¹ã‚³ã‚¢é‡ã¿
                basic_score = (
                    0.35 * ppo_score + 
                    0.3 * contribution_score + 
                    0.35 * similarity_score
                )
                basic_scores[agent_name] = basic_score
            except:
                basic_scores[agent_name] = 0.0

        methods_results["basic"] = basic_scores

        # 2. é«˜åº¦ãªé¡ä¼¼åº¦ã‚¹ã‚³ã‚¢
        enhanced_similarity_scores = {}
        for agent_name in self.models.keys():
            base_similarity = self._calculate_task_similarity(task, agent_name)
            
            # ã‚¿ã‚¹ã‚¯ç‰¹å¾´ã¨ã®è©³ç´°ãƒãƒƒãƒãƒ³ã‚°
            title_lower = (task.get("title", "") or "").lower()
            body_lower = (task.get("body", "") or "").lower()
            
            # è¿½åŠ ã®é¡ä¼¼åº¦ãƒ–ãƒ¼ã‚¹ãƒˆ
            author_tasks = self.author_task_history.get(agent_name, [])
            if len(author_tasks) > 0:
                # æœ€è¿‘ã®ã‚¿ã‚¹ã‚¯ã¨ã®é¡ä¼¼åº¦ï¼ˆæ™‚é–“é‡ã¿ä»˜ãï¼‰
                recent_similarity = 0.0
                for i, past_task in enumerate(author_tasks[-10:]):  # æœ€æ–°10ä»¶
                    past_title = (past_task.get("title", "") or "").lower()
                    past_body = (past_task.get("body", "") or "").lower()
                    
                    # ç°¡æ˜“æ–‡å­—åˆ—é¡ä¼¼åº¦
                    title_overlap = len(set(title_lower.split()) & set(past_title.split()))
                    body_overlap = len(set(body_lower.split()) & set(past_body.split()))
                    
                    task_similarity = (title_overlap + body_overlap) / (len(title_lower.split()) + len(body_lower.split()) + 1)
                    
                    # æ™‚é–“é‡ã¿ï¼ˆæ–°ã—ã„ã»ã©é‡è¦ï¼‰
                    time_weight = (i + 1) / 10
                    recent_similarity += task_similarity * time_weight
                
                recent_similarity /= min(len(author_tasks), 10)
                
                # åŸºæœ¬é¡ä¼¼åº¦ã¨æœ€è¿‘ã‚¿ã‚¹ã‚¯é¡ä¼¼åº¦ã‚’çµ±åˆ
                enhanced_similarity = 0.7 * base_similarity + 0.3 * recent_similarity
            else:
                enhanced_similarity = base_similarity
            
            enhanced_similarity_scores[agent_name] = enhanced_similarity

        methods_results["enhanced_similarity"] = enhanced_similarity_scores

        # 3. è²¢çŒ®é‡ã‚¹ã‚³ã‚¢ï¼ˆæ¨™æº–ï¼‰
        contribution_scores = {}
        for agent_name in self.models.keys():
            contribution = self.author_contributions.get(agent_name, 0)
            contribution_scores[agent_name] = min(contribution / 200.0, 1.0)

        methods_results["contribution"] = contribution_scores

        # 4. æ™‚é–“çš„ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚¹ã‚³ã‚¢
        temporal_scores = {}
        for agent_name in self.models.keys():
            temporal_score = self._calculate_temporal_match(task, agent_name)
            temporal_scores[agent_name] = temporal_score

        methods_results["temporal"] = temporal_scores

        # 5. å°‚é–€æ€§ã‚¹ã‚³ã‚¢ï¼ˆæ”¹è‰¯ç‰ˆï¼‰
        specialization_scores = {}
        for agent_name in self.models.keys():
            author_tasks = self.author_task_history.get(agent_name, [])
            if len(author_tasks) > 0:
                # ã‚ˆã‚Šè©³ç´°ãªã‚¿ã‚¹ã‚¯ã‚¿ã‚¤ãƒ—åˆ†é¡
                task_type_counts = defaultdict(int)
                
                for t in author_tasks:
                    title_lower = (t.get("title", "") or "").lower()
                    body_lower = (t.get("body", "") or "").lower()
                    full_text = f"{title_lower} {body_lower}"
                    
                    if any(kw in full_text for kw in ["bug", "fix", "error", "issue"]):
                        task_type_counts["bug"] += 1
                    elif any(kw in full_text for kw in ["feature", "enhancement", "new"]):
                        task_type_counts["feature"] += 1
                    elif any(kw in full_text for kw in ["doc", "readme", "guide", "documentation"]):
                        task_type_counts["doc"] += 1
                    elif any(kw in full_text for kw in ["test", "spec", "coverage"]):
                        task_type_counts["test"] += 1
                    elif any(kw in full_text for kw in ["ui", "ux", "design", "frontend"]):
                        task_type_counts["ui"] += 1
                    elif any(kw in full_text for kw in ["api", "backend", "server"]):
                        task_type_counts["api"] += 1
                    else:
                        task_type_counts["other"] += 1
                
                # ç¾åœ¨ã®ã‚¿ã‚¹ã‚¯ã®ã‚¿ã‚¤ãƒ—ã‚’åˆ¤å®š
                current_title = (task.get("title", "") or "").lower()
                current_body = (task.get("body", "") or "").lower()
                current_full = f"{current_title} {current_body}"
                
                current_type = "other"
                if any(kw in current_full for kw in ["bug", "fix", "error", "issue"]):
                    current_type = "bug"
                elif any(kw in current_full for kw in ["feature", "enhancement", "new"]):
                    current_type = "feature"
                elif any(kw in current_full for kw in ["doc", "readme", "guide", "documentation"]):
                    current_type = "doc"
                elif any(kw in current_full for kw in ["test", "spec", "coverage"]):
                    current_type = "test"
                elif any(kw in current_full for kw in ["ui", "ux", "design", "frontend"]):
                    current_type = "ui"
                elif any(kw in current_full for kw in ["api", "backend", "server"]):
                    current_type = "api"
                
                # è©²å½“ã‚¿ã‚¤ãƒ—ã§ã®çµŒé¨“å€¤
                type_experience = task_type_counts[current_type] / len(author_tasks)
                specialization_scores[agent_name] = min(type_experience * 2, 1.0)  # 2å€ã«ã—ã¦ä¸Šé™1.0
            else:
                specialization_scores[agent_name] = 0.0

        methods_results["specialization"] = specialization_scores

        # è¶…æœ€é©åŒ–çµ±åˆ
        final_scores = {}

        for agent_name in self.models.keys():
            basic_score = methods_results["basic"].get(agent_name, 0.0)
            enhanced_sim_score = methods_results["enhanced_similarity"].get(agent_name, 0.0)
            contrib_score = methods_results["contribution"].get(agent_name, 0.0)
            temp_score = methods_results["temporal"].get(agent_name, 0.0)
            spec_score = methods_results["specialization"].get(agent_name, 0.0)

            # ã‚¿ã‚¹ã‚¯ã‚¿ã‚¤ãƒ—åˆ¥ã®æœ€é©é‡ã¿ï¼ˆå®Ÿé¨“çµæœã«åŸºã¥ãï¼‰
            title_lower = (task.get("title", "") or "").lower()
            body_lower = (task.get("body", "") or "").lower()
            full_text = f"{title_lower} {body_lower}"

            if any(kw in full_text for kw in ["bug", "fix", "error", "issue"]):
                # ãƒã‚°ä¿®æ­£ï¼šçµŒé¨“ã¨å°‚é–€æ€§é‡è¦–
                weights = [0.2, 0.3, 0.25, 0.1, 0.15]
            elif any(kw in full_text for kw in ["feature", "enhancement", "new"]):
                # æ–°æ©Ÿèƒ½ï¼šé¡ä¼¼åº¦æœ€é‡è¦–ï¼ˆå®Ÿé¨“çµæœã‚ˆã‚Šï¼‰
                weights = [0.2, 0.5, 0.1, 0.05, 0.15]
            elif any(kw in full_text for kw in ["doc", "readme", "guide"]):
                # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆï¼šé¡ä¼¼åº¦è¶…é‡è¦–
                weights = [0.15, 0.55, 0.1, 0.05, 0.15]
            elif any(kw in full_text for kw in ["test", "spec", "coverage"]):
                # ãƒ†ã‚¹ãƒˆï¼šå°‚é–€æ€§é‡è¦–
                weights = [0.25, 0.35, 0.15, 0.05, 0.2]
            else:
                # ä¸€èˆ¬ï¼šãƒãƒ©ãƒ³ã‚¹å‹ï¼ˆfeature_optimizedãƒ™ãƒ¼ã‚¹ï¼‰
                weights = [0.25, 0.45, 0.15, 0.05, 0.1]

            # è¶…æœ€é©åŒ–ã‚¹ã‚³ã‚¢è¨ˆç®—
            ultra_score = (
                weights[0] * basic_score +
                weights[1] * enhanced_sim_score +
                weights[2] * contrib_score +
                weights[3] * temp_score +
                weights[4] * spec_score
            )

            # å¼·åŒ–ãƒ–ãƒ¼ã‚¹ãƒˆï¼ˆæ®µéšçš„ï¼‰
            contribution = self.author_contributions.get(agent_name, 0)
            if contribution >= 500:
                ultra_score *= 1.25
            elif contribution >= 300:
                ultra_score *= 1.2
            elif contribution >= 200:
                ultra_score *= 1.15
            elif contribution >= 100:
                ultra_score *= 1.1
            elif contribution >= 50:
                ultra_score *= 1.05

            # é¡ä¼¼åº¦ãƒ–ãƒ¼ã‚¹ãƒˆï¼ˆæ®µéšçš„ï¼‰
            if enhanced_sim_score > 0.9:
                ultra_score *= 1.15
            elif enhanced_sim_score > 0.8:
                ultra_score *= 1.1
            elif enhanced_sim_score > 0.6:
                ultra_score *= 1.05

            # å°‚é–€æ€§ãƒ–ãƒ¼ã‚¹ãƒˆ
            if spec_score > 0.8:
                ultra_score *= 1.1
            elif spec_score > 0.6:
                ultra_score *= 1.05

            final_scores[agent_name] = min(ultra_score, 1.0)

        # ã‚¹ã‚³ã‚¢é †ã«ã‚½ãƒ¼ãƒˆ
        sorted_agents = sorted(final_scores.items(), key=lambda x: x[1], reverse=True)
        return sorted_agents[:k]

    def evaluate_ultra_optimized(self, sample_size: int = 200):
        """è¶…æœ€é©åŒ–ã‚·ã‚¹ãƒ†ãƒ ã®è©•ä¾¡"""
        print("ğŸš€ è¶…æœ€é©åŒ–æ¨è–¦ã‚·ã‚¹ãƒ†ãƒ ã®è©•ä¾¡")
        print("=" * 50)

        available_agents = set(self.models.keys())
        eval_tasks = []
        eval_ground_truth = []

        for task, author in zip(
            self.tasks[:sample_size * 3], self.ground_truth[:sample_size * 3]
        ):
            if author in available_agents and len(eval_tasks) < sample_size:
                eval_tasks.append(task)
                eval_ground_truth.append(author)

        print(f"   è©•ä¾¡ã‚¿ã‚¹ã‚¯æ•°: {len(eval_tasks)}")

        results = {}

        for k in [1, 3, 5]:
            correct_predictions = 0
            all_recommendations = []

            for task, actual_author in tqdm(
                zip(eval_tasks, eval_ground_truth),
                desc=f"Top-{k}è©•ä¾¡ä¸­",
                total=len(eval_tasks),
            ):
                try:
                    task_features = self._extract_task_features(task)
                    recommendations = self.ultra_optimized_recommendation(
                        task_features, task, k
                    )

                    recommended_agents = [agent for agent, _ in recommendations]
                    all_recommendations.extend(recommended_agents)

                    if actual_author in recommended_agents:
                        correct_predictions += 1

                except Exception:
                    continue

            accuracy = correct_predictions / len(eval_tasks) if eval_tasks else 0
            diversity_score = (
                len(set(all_recommendations)) / len(all_recommendations)
                if all_recommendations else 0
            )

            results[f"top_{k}"] = {
                "accuracy": accuracy,
                "diversity_score": diversity_score,
            }

            print(f"   Top-{k}ç²¾åº¦: {accuracy:.3f} ({accuracy*100:.1f}%)")
            print(f"   å¤šæ§˜æ€§ã‚¹ã‚³ã‚¢: {diversity_score:.3f}")

        return results


def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    print("ğŸš€ è¶…æœ€é©åŒ–ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«æ¨è–¦ã‚·ã‚¹ãƒ†ãƒ ")
    print("=" * 60)

    try:
        # ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–
        system = OptimizedEnsembleSystem(
            model_dir="models/improved_rl/final_models",
            test_data_path="data/backlog_test_2023.json",
        )

        # è¶…æœ€é©åŒ–æ‰‹æ³•ã®è©•ä¾¡
        print(f"\n## è¶…æœ€é©åŒ–æ‰‹æ³•ã®è©•ä¾¡")
        ultra_results = system.evaluate_ultra_optimized(sample_size=200)

        # å¾“æ¥æ‰‹æ³•ã¨ã®æ¯”è¼ƒ
        print(f"\n## å¾“æ¥æ‰‹æ³•ã¨ã®æ¯”è¼ƒ")
        meta_results = system.evaluate_system("meta_ensemble", sample_size=200)

        print(f"\nğŸ‰ æ¯”è¼ƒçµæœ")
        print("=" * 60)
        
        ultra_top1 = ultra_results["top_1"]["accuracy"]
        meta_top1 = meta_results["top_1"]["accuracy"]
        
        print(f"ğŸ† è¶…æœ€é©åŒ–æ‰‹æ³•:")
        print(f"   Top-1ç²¾åº¦: {ultra_top1*100:.1f}%")
        print(f"   Top-3ç²¾åº¦: {ultra_results['top_3']['accuracy']*100:.1f}%")
        print(f"   Top-5ç²¾åº¦: {ultra_results['top_5']['accuracy']*100:.1f}%")
        
        print(f"ğŸ“Š å¾“æ¥ãƒ¡ã‚¿ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«:")
        print(f"   Top-1ç²¾åº¦: {meta_top1*100:.1f}%")
        print(f"   Top-3ç²¾åº¦: {meta_results['top_3']['accuracy']*100:.1f}%")
        print(f"   Top-5ç²¾åº¦: {meta_results['top_5']['accuracy']*100:.1f}%")

        if ultra_top1 > meta_top1:
            improvement = (ultra_top1 - meta_top1) / meta_top1 * 100
            print(f"\nğŸš€ æ”¹å–„é”æˆ: +{improvement:.1f}%")
            print(f"ğŸ¯ æœ€çµ‚Top-1ç²¾åº¦: {ultra_top1*100:.1f}%")
            
            if ultra_top1 >= 0.5:
                print(f"ğŸ‰ 50%çªç ´é”æˆï¼")
            elif ultra_top1 >= 0.47:
                print(f"âœ… 47%ä»¥ä¸Šé”æˆï¼")
        else:
            print(f"ğŸ“Š å¾“æ¥æ‰‹æ³•ãŒå„ªç§€")

    except Exception as e:
        print(f"âŒ ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿ: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()