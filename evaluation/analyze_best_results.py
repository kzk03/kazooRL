#!/usr/bin/env python3
"""
æœ€å„ªç§€æ‰‹æ³•ã®è©³ç´°åˆ†æ
"""

import json
import os
from collections import Counter, defaultdict
from typing import Dict, List

from advanced_ensemble_system import AdvancedEnsembleSystem


def analyze_meta_ensemble_details():
    """ãƒ¡ã‚¿ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ã®è©³ç´°åˆ†æ"""
    print("ğŸ” ãƒ¡ã‚¿ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«æ‰‹æ³•ã®è©³ç´°åˆ†æ")
    print("=" * 50)
    
    # ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–
    system = AdvancedEnsembleSystem(
        model_dir="models/improved_rl/final_models",
        test_data_path="data/backlog_test_2023.json",
    )
    
    # è©³ç´°åˆ†æç”¨ãƒ‡ãƒ¼ã‚¿
    available_agents = set(system.models.keys())
    eval_tasks = []
    eval_ground_truth = []
    
    for task, author in zip(system.tasks[:200], system.ground_truth[:200]):
        if author in available_agents and len(eval_tasks) < 100:
            eval_tasks.append(task)
            eval_ground_truth.append(author)
    
    # è©³ç´°åˆ†æ
    correct_predictions = []
    incorrect_predictions = []
    author_performance = defaultdict(list)
    task_type_performance = defaultdict(list)
    
    for task, actual_author in zip(eval_tasks, eval_ground_truth):
        task_features = system._extract_task_features(task)
        recommendations = system.meta_ensemble_recommendation(task_features, task, 1)
        
        predicted_author = recommendations[0][0] if recommendations else None
        predicted_score = recommendations[0][1] if recommendations else 0.0
        
        is_correct = (predicted_author == actual_author)
        
        # ã‚¿ã‚¹ã‚¯ã‚¿ã‚¤ãƒ—åˆ†é¡
        title_lower = (task.get("title", "") or "").lower()
        body_lower = (task.get("body", "") or "").lower()
        full_text = f"{title_lower} {body_lower}"
        
        if any(kw in full_text for kw in ["bug", "fix", "error", "issue"]):
            task_type = "bug"
        elif any(kw in full_text for kw in ["feature", "enhancement", "new"]):
            task_type = "feature"
        elif any(kw in full_text for kw in ["doc", "readme", "guide"]):
            task_type = "doc"
        else:
            task_type = "other"
        
        result = {
            "task_title": task.get("title", "")[:60] + "...",
            "task_type": task_type,
            "actual_author": actual_author,
            "predicted_author": predicted_author,
            "predicted_score": predicted_score,
            "is_correct": is_correct,
            "contribution": system.author_contributions.get(actual_author, 0),
        }
        
        if is_correct:
            correct_predictions.append(result)
        else:
            incorrect_predictions.append(result)
        
        author_performance[actual_author].append(is_correct)
        task_type_performance[task_type].append(is_correct)
    
    # çµæœè¡¨ç¤º
    print(f"## ğŸ“Š åˆ†æçµæœ")
    print(f"   ç·è©•ä¾¡æ•°: {len(eval_tasks)}")
    print(f"   æ­£è§£æ•°: {len(correct_predictions)}")
    print(f"   ç²¾åº¦: {len(correct_predictions)/len(eval_tasks)*100:.1f}%")
    
    # ã‚¿ã‚¹ã‚¯ã‚¿ã‚¤ãƒ—åˆ¥æ€§èƒ½
    print(f"\n### ğŸ“ˆ ã‚¿ã‚¹ã‚¯ã‚¿ã‚¤ãƒ—åˆ¥æ€§èƒ½")
    for task_type, results in task_type_performance.items():
        accuracy = sum(results) / len(results) * 100
        print(f"   {task_type}: {accuracy:.1f}% ({sum(results)}/{len(results)})")
    
    # æœ€ã‚‚æˆåŠŸã—ãŸé–‹ç™ºè€…
    print(f"\n### ğŸ† é«˜ç²¾åº¦ã§äºˆæ¸¬ã•ã‚ŒãŸé–‹ç™ºè€… (Top-10)")
    author_accuracy = {}
    for author, results in author_performance.items():
        if len(results) >= 2:  # 2å›ä»¥ä¸Šå‡ºç¾
            author_accuracy[author] = sum(results) / len(results)
    
    sorted_authors = sorted(author_accuracy.items(), key=lambda x: x[1], reverse=True)
    for i, (author, accuracy) in enumerate(sorted_authors[:10]):
        contribution = system.author_contributions.get(author, 0)
        total_attempts = len(author_performance[author])
        print(f"   {i+1}. {author}: {accuracy*100:.1f}% ({int(accuracy*total_attempts)}/{total_attempts}) [è²¢çŒ®{contribution}]")
    
    # æˆåŠŸäº‹ä¾‹åˆ†æ
    print(f"\n### âœ… æˆåŠŸäº‹ä¾‹ (Top-5)")
    correct_sorted = sorted(correct_predictions, key=lambda x: x["predicted_score"], reverse=True)
    for i, case in enumerate(correct_sorted[:5]):
        print(f"   {i+1}. ã‚¿ã‚¹ã‚¯: {case['task_title']}")
        print(f"      ã‚¿ã‚¤ãƒ—: {case['task_type']}, è‘—è€…: {case['actual_author']}")
        print(f"      ã‚¹ã‚³ã‚¢: {case['predicted_score']:.3f}, è²¢çŒ®: {case['contribution']}")
        print()
    
    # å¤±æ•—äº‹ä¾‹åˆ†æ
    print(f"\n### âŒ å¤±æ•—äº‹ä¾‹ (é«˜è²¢çŒ®è€…)")
    high_contrib_errors = [p for p in incorrect_predictions if p["contribution"] >= 20]
    high_contrib_errors.sort(key=lambda x: x["contribution"], reverse=True)
    
    for i, case in enumerate(high_contrib_errors[:3]):
        print(f"   {i+1}. ã‚¿ã‚¹ã‚¯: {case['task_title']}")
        print(f"      å®Ÿéš›: {case['actual_author']} (è²¢çŒ®{case['contribution']})")
        print(f"      äºˆæ¸¬: {case['predicted_author']} (ã‚¹ã‚³ã‚¢{case['predicted_score']:.3f})")
        print()


if __name__ == "__main__":
    analyze_meta_ensemble_details()