#!/usr/bin/env python3
"""
æˆåŠŸã—ãŸæ¨è–¦ã®é–‹ç™ºè€…åˆ†æ
è²¢çŒ®é‡ã¨ã®é–¢ä¿‚ã‚’è©³ã—ãèª¿æŸ»
"""

import json
import os
from collections import Counter, defaultdict
from typing import Dict, List, Tuple

import numpy as np
import torch
import torch.nn as nn
from tqdm import tqdm


class PPOPolicyNetwork(nn.Module):
    """PPOãƒãƒªã‚·ãƒ¼ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®å†æ§‹ç¯‰"""

    def __init__(self, input_dim=64, hidden_dim=128):
        super(PPOPolicyNetwork, self).__init__()

        self.feature_extractor = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.LayerNorm(hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(hidden_dim, hidden_dim),
            nn.LayerNorm(hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.1),
        )

        self.actor = nn.Sequential(
            nn.Linear(hidden_dim, 64),
            nn.LayerNorm(64),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(64, 16),
            nn.Softmax(dim=-1),
        )

        self.critic = nn.Sequential(
            nn.Linear(hidden_dim, 64),
            nn.LayerNorm(64),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(64, 1),
        )

    def forward(self, x):
        features = self.feature_extractor(x)
        action_probs = self.actor(features)
        value = self.critic(features)
        return action_probs, value

    def get_action_score(self, x):
        with torch.no_grad():
            action_probs, value = self.forward(x)
            score = torch.max(action_probs).item()
            return score


def is_bot(username: str) -> bool:
    """ãƒ¦ãƒ¼ã‚¶ãƒ¼åãŒBotã‹ã©ã†ã‹åˆ¤å®š"""
    bot_indicators = [
        "[bot]",
        "bot",
        "dependabot",
        "renovate",
        "greenkeeper",
        "codecov",
        "travis",
        "circleci",
        "github-actions",
        "automated",
    ]
    username_lower = username.lower()
    return any(indicator in username_lower for indicator in bot_indicators)


def load_test_data_with_bot_filtering(
    test_data_path: str,
) -> Tuple[List[Dict], List[str]]:
    """ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ã€Botã‚’é™¤å»"""
    with open(test_data_path, "r", encoding="utf-8") as f:
        test_data = json.load(f)

    filtered_tasks = []
    ground_truth_authors = []

    for task in test_data:
        author = task.get("author", {})
        if author and isinstance(author, dict):
            author_login = author.get("login", "")
            if author_login and not is_bot(author_login):
                filtered_tasks.append(task)
                ground_truth_authors.append(author_login)

    return filtered_tasks, ground_truth_authors


def load_sample_models(
    model_dir: str, actual_authors: List[str], max_models: int = 50
) -> Dict[str, PPOPolicyNetwork]:
    """ã‚µãƒ³ãƒ—ãƒ«ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿"""
    model_files = [f for f in os.listdir(model_dir) if f.endswith(".pth")]
    all_trained_agents = [
        f.replace("agent_", "").replace(".pth", "") for f in model_files
    ]

    human_trained_agents = [agent for agent in all_trained_agents if not is_bot(agent)]
    actual_set = set(actual_authors)
    human_set = set(human_trained_agents)
    overlapping_agents = actual_set.intersection(human_set)

    loaded_models = {}

    for i, agent_name in enumerate(overlapping_agents):
        if i >= max_models:
            break

        model_path = os.path.join(model_dir, f"agent_{agent_name}.pth")

        try:
            model_data = torch.load(model_path, map_location="cpu", weights_only=False)
            policy_network = PPOPolicyNetwork()
            policy_network.load_state_dict(model_data["policy_state_dict"])
            policy_network.eval()
            loaded_models[agent_name] = policy_network
        except Exception as e:
            continue

    return loaded_models


def extract_task_features_for_model(task: Dict) -> torch.Tensor:
    """ãƒ¢ãƒ‡ãƒ«ç”¨ã®ã‚¿ã‚¹ã‚¯ç‰¹å¾´é‡ã‚’æŠ½å‡ºï¼ˆ64æ¬¡å…ƒï¼‰"""
    features = []

    title = task.get("title", "") or ""
    body = task.get("body", "") or ""
    labels = task.get("labels", [])

    # åŸºæœ¬ç‰¹å¾´é‡ï¼ˆ10æ¬¡å…ƒï¼‰
    basic_features = [
        len(title),
        len(body),
        len(title.split()),
        len(body.split()),
        len(labels),
        title.count("?"),
        title.count("!"),
        body.count("\n"),
        len(set(title.lower().split())),
        1 if any(kw in title.lower() for kw in ["bug", "fix", "error"]) else 0,
    ]
    features.extend(basic_features)

    # æ—¥ä»˜ç‰¹å¾´é‡ï¼ˆ3æ¬¡å…ƒï¼‰
    created_at = task.get("created_at", "")
    if created_at:
        try:
            date_parts = created_at.split("T")[0].split("-")
            year, month, day = (
                int(date_parts[0]),
                int(date_parts[1]),
                int(date_parts[2]),
            )
            features.extend([year - 2020, month, day])
        except:
            features.extend([0, 0, 0])
    else:
        features.extend([0, 0, 0])

    # ãƒ©ãƒ™ãƒ«ç‰¹å¾´é‡ï¼ˆ10æ¬¡å…ƒï¼‰
    label_text = " ".join(
        [
            str(label) if not isinstance(label, dict) else label.get("name", "")
            for label in labels
        ]
    ).lower()

    important_keywords = [
        "bug",
        "feature",
        "enhancement",
        "documentation",
        "help",
        "question",
        "performance",
        "security",
        "ui",
        "api",
    ]
    for keyword in important_keywords:
        features.append(1 if keyword in label_text else 0)

    # æ®‹ã‚Šã‚’ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°
    while len(features) < 64:
        features.append(0.0)
    features = features[:64]

    # æ­£è¦åŒ–
    features = np.array(features, dtype=np.float32)
    features = (features - np.mean(features)) / (np.std(features) + 1e-8)

    return torch.tensor(features, dtype=torch.float32)


def analyze_successful_predictions():
    """æˆåŠŸã—ãŸæ¨è–¦ã®è©³ç´°åˆ†æ"""
    print("ğŸ¯ æˆåŠŸã—ãŸæ¨è–¦ã®é–‹ç™ºè€…åˆ†æ")
    print("=" * 60)

    # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    tasks, ground_truth = load_test_data_with_bot_filtering(
        "data/backlog_test_2023.json"
    )
    trained_models = load_sample_models(
        "models/improved_rl/final_models", ground_truth, 50
    )

    print(f"ğŸ“Š åˆ†æå¯¾è±¡: {len(tasks):,}ã‚¿ã‚¹ã‚¯, {len(trained_models)}ãƒ¢ãƒ‡ãƒ«")

    # å…¨ä½“ã®è²¢çŒ®é‡åˆ†æ
    author_contribution = Counter(ground_truth)
    total_tasks = len(ground_truth)

    print(f"\n## 1. å…¨ä½“ã®é–‹ç™ºè€…è²¢çŒ®é‡åˆ†æ")
    print("-" * 40)
    print(f"   ç·ã‚¿ã‚¹ã‚¯æ•°: {total_tasks:,}")
    print(f"   ãƒ¦ãƒ‹ãƒ¼ã‚¯é–‹ç™ºè€…æ•°: {len(author_contribution)}")

    # è²¢çŒ®é‡åˆ¥ã‚«ãƒ†ã‚´ãƒªåˆ†ã‘
    high_contributors = []  # 50+ ã‚¿ã‚¹ã‚¯
    medium_contributors = []  # 10-49 ã‚¿ã‚¹ã‚¯
    low_contributors = []  # 1-9 ã‚¿ã‚¹ã‚¯

    for author, count in author_contribution.items():
        if count >= 50:
            high_contributors.append((author, count))
        elif count >= 10:
            medium_contributors.append((author, count))
        else:
            low_contributors.append((author, count))

    print(f"\n   è²¢çŒ®é‡åˆ¥åˆ†é¡:")
    print(f"     é«˜è²¢çŒ®è€… (50+ã‚¿ã‚¹ã‚¯): {len(high_contributors)}äºº")
    print(f"     ä¸­è²¢çŒ®è€… (10-49ã‚¿ã‚¹ã‚¯): {len(medium_contributors)}äºº")
    print(f"     ä½è²¢çŒ®è€… (1-9ã‚¿ã‚¹ã‚¯): {len(low_contributors)}äºº")

    # å„ã‚«ãƒ†ã‚´ãƒªã®è©³ç´°
    print(f"\n   é«˜è²¢çŒ®è€…ãƒªã‚¹ãƒˆ:")
    for author, count in sorted(high_contributors, key=lambda x: x[1], reverse=True):
        percentage = count / total_tasks * 100
        print(f"     {author}: {count:3d}ã‚¿ã‚¹ã‚¯ ({percentage:4.1f}%)")

    print(f"\n   ä¸­è²¢çŒ®è€…ãƒªã‚¹ãƒˆ:")
    for author, count in sorted(medium_contributors, key=lambda x: x[1], reverse=True)[
        :10
    ]:
        percentage = count / total_tasks * 100
        print(f"     {author}: {count:2d}ã‚¿ã‚¹ã‚¯ ({percentage:3.1f}%)")

    # æ¨è–¦æˆåŠŸåˆ†æ
    print(f"\n## 2. æ¨è–¦æˆåŠŸåˆ†æ")
    print("-" * 40)

    available_agents = set(trained_models.keys())
    sample_size = min(1000, len(tasks))  # ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚º

    successful_predictions = {"top1": [], "top3": [], "top5": []}

    print(f"   åˆ†æã‚µãƒ³ãƒ—ãƒ«æ•°: {sample_size}")

    for i, (task, actual_author) in enumerate(
        tqdm(zip(tasks[:sample_size], ground_truth[:sample_size]), desc="æ¨è–¦åˆ†æä¸­")
    ):
        if actual_author not in available_agents:
            continue

        try:
            # ã‚¿ã‚¹ã‚¯ç‰¹å¾´é‡æŠ½å‡º
            task_features = extract_task_features_for_model(task)

            # å„ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã§ã®é©åˆåº¦ã‚’è¨ˆç®—
            agent_scores = {}
            for agent_name, model in trained_models.items():
                try:
                    score = model.get_action_score(task_features)
                    agent_scores[agent_name] = score
                except:
                    agent_scores[agent_name] = 0.0

            # ã‚¹ã‚³ã‚¢é †ã«ã‚½ãƒ¼ãƒˆ
            sorted_agents = sorted(
                agent_scores.items(), key=lambda x: x[1], reverse=True
            )

            # Top-KæˆåŠŸåˆ¤å®š
            top1_agents = [agent for agent, _ in sorted_agents[:1]]
            top3_agents = [agent for agent, _ in sorted_agents[:3]]
            top5_agents = [agent for agent, _ in sorted_agents[:5]]

            if actual_author in top1_agents:
                successful_predictions["top1"].append(
                    {
                        "author": actual_author,
                        "contribution": author_contribution[actual_author],
                        "rank": 1,
                        "score": agent_scores[actual_author],
                        "task_id": i,
                    }
                )

            if actual_author in top3_agents:
                actual_rank = next(
                    rank
                    for rank, (agent, _) in enumerate(sorted_agents, 1)
                    if agent == actual_author
                )
                successful_predictions["top3"].append(
                    {
                        "author": actual_author,
                        "contribution": author_contribution[actual_author],
                        "rank": actual_rank,
                        "score": agent_scores[actual_author],
                        "task_id": i,
                    }
                )

            if actual_author in top5_agents:
                actual_rank = next(
                    rank
                    for rank, (agent, _) in enumerate(sorted_agents, 1)
                    if agent == actual_author
                )
                successful_predictions["top5"].append(
                    {
                        "author": actual_author,
                        "contribution": author_contribution[actual_author],
                        "rank": actual_rank,
                        "score": agent_scores[actual_author],
                        "task_id": i,
                    }
                )

        except Exception as e:
            continue

    # æˆåŠŸã—ãŸæ¨è–¦ã®åˆ†æ
    print(f"\n## 3. æˆåŠŸã—ãŸæ¨è–¦ã®è©³ç´°åˆ†æ")
    print("-" * 40)

    for k in ["top1", "top3", "top5"]:
        successes = successful_predictions[k]
        if not successes:
            continue

        print(f"\n### {k.upper()}æˆåŠŸåˆ†æ ({len(successes)}ä»¶)")

        # æˆåŠŸã—ãŸé–‹ç™ºè€…ã®ãƒªã‚¹ãƒˆ
        success_authors = [s["author"] for s in successes]
        success_counter = Counter(success_authors)

        print(f"   æˆåŠŸã—ãŸé–‹ç™ºè€…ãƒªã‚¹ãƒˆ:")
        for author, count in success_counter.most_common():
            total_contribution = author_contribution[author]
            success_rate = (
                count / total_contribution * 100 if total_contribution > 0 else 0
            )
            print(
                f"     {author}: {count}å›æˆåŠŸ / {total_contribution}ã‚¿ã‚¹ã‚¯ ({success_rate:.1f}%)"
            )

        # è²¢çŒ®é‡åˆ¥æˆåŠŸç‡åˆ†æ
        contribution_analysis = {
            "high": {"successes": 0, "total": 0},
            "medium": {"successes": 0, "total": 0},
            "low": {"successes": 0, "total": 0},
        }

        for success in successes:
            author = success["author"]
            contribution = success["contribution"]

            if contribution >= 50:
                contribution_analysis["high"]["successes"] += 1
            elif contribution >= 10:
                contribution_analysis["medium"]["successes"] += 1
            else:
                contribution_analysis["low"]["successes"] += 1

        # å„ã‚«ãƒ†ã‚´ãƒªã®ç·ã‚¿ã‚¹ã‚¯æ•°ã‚’è¨ˆç®—
        for author, count in author_contribution.items():
            if author in available_agents:
                if count >= 50:
                    contribution_analysis["high"]["total"] += count
                elif count >= 10:
                    contribution_analysis["medium"]["total"] += count
                else:
                    contribution_analysis["low"]["total"] += count

        print(f"\n   è²¢çŒ®é‡åˆ¥æˆåŠŸç‡:")
        for category, data in contribution_analysis.items():
            if data["total"] > 0:
                success_rate = data["successes"] / data["total"] * 100
                category_name = {
                    "high": "é«˜è²¢çŒ®è€…",
                    "medium": "ä¸­è²¢çŒ®è€…",
                    "low": "ä½è²¢çŒ®è€…",
                }[category]
                print(
                    f"     {category_name}: {data['successes']}/{data['total']} ({success_rate:.2f}%)"
                )

    # è²¢çŒ®é‡ã¨æˆåŠŸã®ç›¸é–¢åˆ†æ
    print(f"\n## 4. è²¢çŒ®é‡ã¨æ¨è–¦æˆåŠŸã®ç›¸é–¢åˆ†æ")
    print("-" * 40)

    # Top-3æˆåŠŸã§ã®è©³ç´°åˆ†æ
    top3_successes = successful_predictions["top3"]
    if top3_successes:
        contributions = [s["contribution"] for s in top3_successes]
        ranks = [s["rank"] for s in top3_successes]

        print(f"   Top-3æˆåŠŸã®çµ±è¨ˆ:")
        print(f"     å¹³å‡è²¢çŒ®é‡: {np.mean(contributions):.1f}ã‚¿ã‚¹ã‚¯")
        print(f"     ä¸­å¤®å€¤è²¢çŒ®é‡: {np.median(contributions):.1f}ã‚¿ã‚¹ã‚¯")
        print(f"     æœ€å¤§è²¢çŒ®é‡: {np.max(contributions)}ã‚¿ã‚¹ã‚¯")
        print(f"     æœ€å°è²¢çŒ®é‡: {np.min(contributions)}ã‚¿ã‚¹ã‚¯")
        print(f"     å¹³å‡ãƒ©ãƒ³ã‚¯: {np.mean(ranks):.1f}ä½")

        # è²¢çŒ®é‡ã®åˆ†å¸ƒ
        high_contrib_successes = sum(1 for c in contributions if c >= 50)
        medium_contrib_successes = sum(1 for c in contributions if 10 <= c < 50)
        low_contrib_successes = sum(1 for c in contributions if c < 10)

        total_successes = len(contributions)
        print(f"\n   æˆåŠŸã®è²¢çŒ®é‡åˆ†å¸ƒ:")
        print(
            f"     é«˜è²¢çŒ®è€…ã®æˆåŠŸ: {high_contrib_successes}/{total_successes} ({high_contrib_successes/total_successes*100:.1f}%)"
        )
        print(
            f"     ä¸­è²¢çŒ®è€…ã®æˆåŠŸ: {medium_contrib_successes}/{total_successes} ({medium_contrib_successes/total_successes*100:.1f}%)"
        )
        print(
            f"     ä½è²¢çŒ®è€…ã®æˆåŠŸ: {low_contrib_successes}/{total_successes} ({low_contrib_successes/total_successes*100:.1f}%)"
        )

    # ä»®èª¬ã®æ¤œè¨¼
    print(f"\n## 5. ä»®èª¬ã®æ¤œè¨¼")
    print("-" * 40)

    print("### ä»®èª¬: ã€Œè²¢çŒ®é‡ã®å¤šã„é–‹ç™ºè€…ã—ã‹å½“ãŸã£ã¦ã„ãªã„ã€")

    if top3_successes:
        # å…¨ä½“ã®è²¢çŒ®é‡åˆ†å¸ƒ
        total_high = sum(count for _, count in high_contributors)
        total_medium = sum(count for _, count in medium_contributors)
        total_low = sum(count for _, count in low_contributors)

        total_available_tasks = total_high + total_medium + total_low

        high_ratio_overall = total_high / total_available_tasks * 100
        medium_ratio_overall = total_medium / total_available_tasks * 100
        low_ratio_overall = total_low / total_available_tasks * 100

        print(f"\n   å…¨ä½“ã®è²¢çŒ®é‡åˆ†å¸ƒ:")
        print(f"     é«˜è²¢çŒ®è€…: {high_ratio_overall:.1f}%")
        print(f"     ä¸­è²¢çŒ®è€…: {medium_ratio_overall:.1f}%")
        print(f"     ä½è²¢çŒ®è€…: {low_ratio_overall:.1f}%")

        # æˆåŠŸã®è²¢çŒ®é‡åˆ†å¸ƒï¼ˆå†æ²ï¼‰
        contributions = [s["contribution"] for s in top3_successes]
        high_success_ratio = (
            sum(1 for c in contributions if c >= 50) / len(contributions) * 100
        )
        medium_success_ratio = (
            sum(1 for c in contributions if 10 <= c < 50) / len(contributions) * 100
        )
        low_success_ratio = (
            sum(1 for c in contributions if c < 10) / len(contributions) * 100
        )

        print(f"\n   æˆåŠŸã®è²¢çŒ®é‡åˆ†å¸ƒ:")
        print(f"     é«˜è²¢çŒ®è€…: {high_success_ratio:.1f}%")
        print(f"     ä¸­è²¢çŒ®è€…: {medium_success_ratio:.1f}%")
        print(f"     ä½è²¢çŒ®è€…: {low_success_ratio:.1f}%")

        # ãƒã‚¤ã‚¢ã‚¹ã®åˆ¤å®š
        high_bias = high_success_ratio - high_ratio_overall
        medium_bias = medium_success_ratio - medium_ratio_overall
        low_bias = low_success_ratio - low_ratio_overall

        print(f"\n   ãƒã‚¤ã‚¢ã‚¹åˆ†æï¼ˆæˆåŠŸç‡ - å…¨ä½“ç‡ï¼‰:")
        print(f"     é«˜è²¢çŒ®è€…ãƒã‚¤ã‚¢ã‚¹: {high_bias:+.1f}%ãƒã‚¤ãƒ³ãƒˆ")
        print(f"     ä¸­è²¢çŒ®è€…ãƒã‚¤ã‚¢ã‚¹: {medium_bias:+.1f}%ãƒã‚¤ãƒ³ãƒˆ")
        print(f"     ä½è²¢çŒ®è€…ãƒã‚¤ã‚¢ã‚¹: {low_bias:+.1f}%ãƒã‚¤ãƒ³ãƒˆ")

        # çµè«–
        if high_bias > 10:
            print(f"\n   ğŸ¯ çµè«–: ä»®èª¬ã¯æ­£ã—ã„ - é«˜è²¢çŒ®è€…ã«å¼·ã„ãƒã‚¤ã‚¢ã‚¹")
        elif abs(high_bias) < 5:
            print(f"\n   ğŸ¯ çµè«–: ä»®èª¬ã¯é–“é•ã„ - ãƒã‚¤ã‚¢ã‚¹ã¯è»½å¾®")
        else:
            print(f"\n   ğŸ¯ çµè«–: ä»®èª¬ã¯éƒ¨åˆ†çš„ã«æ­£ã—ã„ - ä¸­ç¨‹åº¦ã®ãƒã‚¤ã‚¢ã‚¹")

    return {
        "successful_predictions": successful_predictions,
        "author_contribution": author_contribution,
        "high_contributors": high_contributors,
        "medium_contributors": medium_contributors,
        "low_contributors": low_contributors,
    }


if __name__ == "__main__":
    results = analyze_successful_predictions()

    print(f"\nğŸ¯ ã¾ã¨ã‚:")
    top3_count = len(results["successful_predictions"]["top3"])
    total_contributors = len(results["author_contribution"])
    print(f"   Top-3æˆåŠŸæ•°: {top3_count}")
    print(f"   æˆåŠŸã—ãŸé–‹ç™ºè€…ã®è©³ç´°ãƒªã‚¹ãƒˆã‚’ä¸Šè¨˜ã«è¡¨ç¤º")
    print(f"   è²¢çŒ®é‡ãƒã‚¤ã‚¢ã‚¹ã®åˆ†æçµæœã‚’ç¢ºèª")
