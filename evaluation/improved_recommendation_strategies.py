#!/usr/bin/env python3
"""
æ¨è–¦ã‚·ã‚¹ãƒ†ãƒ ã®åã‚Šæ”¹å–„æˆ¦ç•¥
å¤šæ§˜æ€§ã¨å…¬å¹³æ€§ã‚’å‘ä¸Šã•ã›ã‚‹æ‰‹æ³•ã®å®Ÿè£…
"""

import json
import os
import random
from collections import Counter, defaultdict
from typing import Dict, List, Tuple

import numpy as np
import torch
import torch.nn as nn
from tqdm import tqdm


class PPOPolicyNetwork(nn.Module):
    """PPOãƒãƒªã‚·ãƒ¼ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®å†æ§‹ç¯‰"""
    
    def __init__(self, input_dim=64, hidden_dim=128):
        super(PPOPolicyNetwork, self).__init__()
        
        self.feature_extractor = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.LayerNorm(hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(hidden_dim, hidden_dim),
            nn.LayerNorm(hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.1)
        )
        
        self.actor = nn.Sequential(
            nn.Linear(hidden_dim, 64),
            nn.LayerNorm(64),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(64, 16),
            nn.Softmax(dim=-1)
        )
        
        self.critic = nn.Sequential(
            nn.Linear(hidden_dim, 64),
            nn.LayerNorm(64),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(64, 1)
        )
    
    def forward(self, x):
        features = self.feature_extractor(x)
        action_probs = self.actor(features)
        value = self.critic(features)
        return action_probs, value
    
    def get_action_score(self, x):
        with torch.no_grad():
            action_probs, value = self.forward(x)
            score = torch.max(action_probs).item()
            return score


def is_bot(username: str) -> bool:
    """ãƒ¦ãƒ¼ã‚¶ãƒ¼åãŒBotã‹ã©ã†ã‹åˆ¤å®š"""
    bot_indicators = [
        "[bot]", "bot", "dependabot", "renovate", "greenkeeper",
        "codecov", "travis", "circleci", "github-actions", "automated"
    ]
    username_lower = username.lower()
    return any(indicator in username_lower for indicator in bot_indicators)


def load_test_data_with_bot_filtering(test_data_path: str) -> Tuple[List[Dict], List[str]]:
    """ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ã€Botã‚’é™¤å»"""
    with open(test_data_path, "r", encoding="utf-8") as f:
        test_data = json.load(f)
    
    filtered_tasks = []
    ground_truth_authors = []
    
    for task in test_data:
        author = task.get("author", {})
        if author and isinstance(author, dict):
            author_login = author.get("login", "")
            if author_login and not is_bot(author_login):
                filtered_tasks.append(task)
                ground_truth_authors.append(author_login)
    
    return filtered_tasks, ground_truth_authors


def load_sample_models(model_dir: str, actual_authors: List[str], max_models: int = 50) -> Dict[str, PPOPolicyNetwork]:
    """ã‚µãƒ³ãƒ—ãƒ«ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿"""
    model_files = [f for f in os.listdir(model_dir) if f.endswith('.pth')]
    all_trained_agents = [f.replace('agent_', '').replace('.pth', '') for f in model_files]
    
    human_trained_agents = [agent for agent in all_trained_agents if not is_bot(agent)]
    actual_set = set(actual_authors)
    human_set = set(human_trained_agents)
    overlapping_agents = actual_set.intersection(human_set)
    
    loaded_models = {}
    
    for i, agent_name in enumerate(overlapping_agents):
        if i >= max_models:
            break
            
        model_path = os.path.join(model_dir, f"agent_{agent_name}.pth")
        
        try:
            model_data = torch.load(model_path, map_location='cpu', weights_only=False)
            policy_network = PPOPolicyNetwork()
            policy_network.load_state_dict(model_data['policy_state_dict'])
            policy_network.eval()
            loaded_models[agent_name] = policy_network
        except Exception as e:
            continue
    
    return loaded_models


def extract_task_features_for_model(task: Dict) -> torch.Tensor:
    """ãƒ¢ãƒ‡ãƒ«ç”¨ã®ã‚¿ã‚¹ã‚¯ç‰¹å¾´é‡ã‚’æŠ½å‡ºï¼ˆ64æ¬¡å…ƒï¼‰"""
    features = []
    
    title = task.get("title", "") or ""
    body = task.get("body", "") or ""
    labels = task.get("labels", [])
    
    # åŸºæœ¬ç‰¹å¾´é‡ï¼ˆ10æ¬¡å…ƒï¼‰
    basic_features = [
        len(title), len(body), len(title.split()), len(body.split()), len(labels),
        title.count('?'), title.count('!'), body.count('\n'),
        len(set(title.lower().split())),
        1 if any(kw in title.lower() for kw in ['bug', 'fix', 'error']) else 0
    ]
    features.extend(basic_features)
    
    # æ—¥ä»˜ç‰¹å¾´é‡ï¼ˆ3æ¬¡å…ƒï¼‰
    created_at = task.get("created_at", "")
    if created_at:
        try:
            date_parts = created_at.split("T")[0].split("-")
            year, month, day = int(date_parts[0]), int(date_parts[1]), int(date_parts[2])
            features.extend([year - 2020, month, day])
        except:
            features.extend([0, 0, 0])
    else:
        features.extend([0, 0, 0])
    
    # ãƒ©ãƒ™ãƒ«ç‰¹å¾´é‡ï¼ˆ10æ¬¡å…ƒï¼‰
    label_text = " ".join([str(label) if not isinstance(label, dict) else label.get("name", "") 
                          for label in labels]).lower()
    
    important_keywords = ["bug", "feature", "enhancement", "documentation", "help", 
                         "question", "performance", "security", "ui", "api"]
    for keyword in important_keywords:
        features.append(1 if keyword in label_text else 0)
    
    # æ®‹ã‚Šã‚’ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°
    while len(features) < 64:
        features.append(0.0)
    features = features[:64]
    
    # æ­£è¦åŒ–
    features = np.array(features, dtype=np.float32)
    features = (features - np.mean(features)) / (np.std(features) + 1e-8)
    
    return torch.tensor(features, dtype=torch.float32)


class ImprovedRecommendationSystem:
    """æ”¹å–„ã•ã‚ŒãŸæ¨è–¦ã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self, models: Dict[str, PPOPolicyNetwork], author_contributions: Dict[str, int]):
        self.models = models
        self.author_contributions = author_contributions
        self.recommendation_history = defaultdict(int)  # æ¨è–¦å±¥æ­´ã®è¿½è·¡
        
        # è²¢çŒ®é‡åˆ¥ã‚«ãƒ†ã‚´ãƒªåˆ†ã‘
        self.high_contributors = set()
        self.medium_contributors = set()
        self.low_contributors = set()
        
        for author, count in author_contributions.items():
            if author in models:
                if count >= 50:
                    self.high_contributors.add(author)
                elif count >= 10:
                    self.medium_contributors.add(author)
                else:
                    self.low_contributors.add(author)
    
    def original_recommendation(self, task_features: torch.Tensor, k: int = 5) -> List[Tuple[str, float]]:
        """å…ƒã®æ¨è–¦æ–¹æ³•ï¼ˆæ¯”è¼ƒç”¨ï¼‰"""
        agent_scores = {}
        for agent_name, model in self.models.items():
            try:
                score = model.get_action_score(task_features)
                agent_scores[agent_name] = score
            except:
                agent_scores[agent_name] = 0.0
        
        sorted_agents = sorted(agent_scores.items(), key=lambda x: x[1], reverse=True)
        return sorted_agents[:k]
    
    def diversity_weighted_recommendation(self, task_features: torch.Tensor, k: int = 5, 
                                        diversity_weight: float = 0.3) -> List[Tuple[str, float]]:
        """å¤šæ§˜æ€§é‡ã¿ä»˜ãæ¨è–¦"""
        agent_scores = {}
        for agent_name, model in self.models.items():
            try:
                base_score = model.get_action_score(task_features)
                
                # å¤šæ§˜æ€§ãƒœãƒ¼ãƒŠã‚¹ï¼ˆæ¨è–¦å›æ•°ãŒå°‘ãªã„ã»ã©é«˜ã„ãƒœãƒ¼ãƒŠã‚¹ï¼‰
                recommendation_count = self.recommendation_history[agent_name]
                max_recommendations = max(self.recommendation_history.values()) if self.recommendation_history else 1
                diversity_bonus = (1 - recommendation_count / (max_recommendations + 1)) * diversity_weight
                
                final_score = base_score + diversity_bonus
                agent_scores[agent_name] = final_score
            except:
                agent_scores[agent_name] = 0.0
        
        sorted_agents = sorted(agent_scores.items(), key=lambda x: x[1], reverse=True)
        return sorted_agents[:k]
    
    def contribution_balanced_recommendation(self, task_features: torch.Tensor, k: int = 5) -> List[Tuple[str, float]]:
        """è²¢çŒ®é‡ãƒãƒ©ãƒ³ã‚¹æ¨è–¦"""
        # å„ã‚«ãƒ†ã‚´ãƒªã‹ã‚‰å€™è£œã‚’é¸å‡º
        high_candidates = []
        medium_candidates = []
        low_candidates = []
        
        for agent_name, model in self.models.items():
            try:
                score = model.get_action_score(task_features)
                
                if agent_name in self.high_contributors:
                    high_candidates.append((agent_name, score))
                elif agent_name in self.medium_contributors:
                    medium_candidates.append((agent_name, score))
                else:
                    low_candidates.append((agent_name, score))
            except:
                continue
        
        # å„ã‚«ãƒ†ã‚´ãƒªã‚’ã‚¹ã‚³ã‚¢é †ã«ã‚½ãƒ¼ãƒˆ
        high_candidates.sort(key=lambda x: x[1], reverse=True)
        medium_candidates.sort(key=lambda x: x[1], reverse=True)
        low_candidates.sort(key=lambda x: x[1], reverse=True)
        
        # ãƒãƒ©ãƒ³ã‚¹è‰¯ãé¸å‡ºï¼ˆé«˜:ä¸­:ä½ = 2:2:1ã®æ¯”ç‡ï¼‰
        recommendations = []
        
        # é«˜è²¢çŒ®è€…ã‹ã‚‰2äºº
        recommendations.extend(high_candidates[:2])
        
        # ä¸­è²¢çŒ®è€…ã‹ã‚‰2äºº
        recommendations.extend(medium_candidates[:2])
        
        # ä½è²¢çŒ®è€…ã‹ã‚‰1äºº
        recommendations.extend(low_candidates[:1])
        
        # æ®‹ã‚Šã¯å…¨ä½“ã‹ã‚‰é¸å‡º
        all_candidates = high_candidates + medium_candidates + low_candidates
        all_candidates.sort(key=lambda x: x[1], reverse=True)
        
        existing_agents = set(agent for agent, _ in recommendations)
        for agent, score in all_candidates:
            if agent not in existing_agents and len(recommendations) < k:
                recommendations.append((agent, score))
        
        return recommendations[:k]
    
    def temperature_scaled_recommendation(self, task_features: torch.Tensor, k: int = 5, 
                                        temperature: float = 2.0) -> List[Tuple[str, float]]:
        """æ¸©åº¦ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°æ¨è–¦ï¼ˆç¢ºç‡çš„é¸æŠï¼‰"""
        agent_scores = {}
        for agent_name, model in self.models.items():
            try:
                score = model.get_action_score(task_features)
                agent_scores[agent_name] = score
            except:
                agent_scores[agent_name] = 0.0
        
        # æ¸©åº¦ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°
        agents = list(agent_scores.keys())
        scores = np.array(list(agent_scores.values()))
        
        # æ¸©åº¦ã‚’é©ç”¨
        scaled_scores = scores / temperature
        
        # ã‚½ãƒ•ãƒˆãƒãƒƒã‚¯ã‚¹ã§ç¢ºç‡ã«å¤‰æ›
        exp_scores = np.exp(scaled_scores - np.max(scaled_scores))  # æ•°å€¤å®‰å®šæ€§ã®ãŸã‚
        probabilities = exp_scores / np.sum(exp_scores)
        
        # ç¢ºç‡ã«åŸºã¥ã„ã¦é¸æŠï¼ˆé‡è¤‡ãªã—ï¼‰
        selected_indices = np.random.choice(
            len(agents), size=min(k, len(agents)), replace=False, p=probabilities
        )
        
        recommendations = [(agents[i], agent_scores[agents[i]]) for i in selected_indices]
        recommendations.sort(key=lambda x: x[1], reverse=True)
        
        return recommendations
    
    def hybrid_recommendation(self, task_features: torch.Tensor, k: int = 5) -> List[Tuple[str, float]]:
        """ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¨è–¦ï¼ˆè¤‡æ•°æ‰‹æ³•ã®çµ„ã¿åˆã‚ã›ï¼‰"""
        # å„æ‰‹æ³•ã§æ¨è–¦ã‚’å–å¾—
        original = self.original_recommendation(task_features, k)
        diversity = self.diversity_weighted_recommendation(task_features, k)
        balanced = self.contribution_balanced_recommendation(task_features, k)
        
        # ã‚¹ã‚³ã‚¢ã‚’çµ±åˆ
        combined_scores = defaultdict(list)
        
        for agent, score in original:
            combined_scores[agent].append(score * 0.4)  # å…ƒæ‰‹æ³•ã«40%ã®é‡ã¿
        
        for agent, score in diversity:
            combined_scores[agent].append(score * 0.3)  # å¤šæ§˜æ€§ã«30%ã®é‡ã¿
        
        for agent, score in balanced:
            combined_scores[agent].append(score * 0.3)  # ãƒãƒ©ãƒ³ã‚¹ã«30%ã®é‡ã¿
        
        # å¹³å‡ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—
        final_scores = {}
        for agent, scores in combined_scores.items():
            final_scores[agent] = np.mean(scores)
        
        sorted_agents = sorted(final_scores.items(), key=lambda x: x[1], reverse=True)
        return sorted_agents[:k]
    
    def update_recommendation_history(self, recommended_agents: List[str]):
        """æ¨è–¦å±¥æ­´ã‚’æ›´æ–°"""
        for agent in recommended_agents:
            self.recommendation_history[agent] += 1


def evaluate_improved_strategies():
    """æ”¹å–„æˆ¦ç•¥ã®è©•ä¾¡"""
    print("ğŸš€ æ¨è–¦ã‚·ã‚¹ãƒ†ãƒ æ”¹å–„æˆ¦ç•¥ã®è©•ä¾¡")
    print("=" * 60)
    
    # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    tasks, ground_truth = load_test_data_with_bot_filtering("data/backlog_test_2023.json")
    trained_models = load_sample_models("models/improved_rl/final_models", ground_truth, 50)
    
    # è²¢çŒ®é‡åˆ†æ
    author_contribution = Counter(ground_truth)
    
    # æ”¹å–„ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–
    improved_system = ImprovedRecommendationSystem(trained_models, author_contribution)
    
    print(f"ğŸ“Š è©•ä¾¡è¨­å®š:")
    print(f"   ã‚¿ã‚¹ã‚¯æ•°: {len(tasks):,}")
    print(f"   ãƒ¢ãƒ‡ãƒ«æ•°: {len(trained_models)}")
    print(f"   é«˜è²¢çŒ®è€…: {len(improved_system.high_contributors)}äºº")
    print(f"   ä¸­è²¢çŒ®è€…: {len(improved_system.medium_contributors)}äºº")
    print(f"   ä½è²¢çŒ®è€…: {len(improved_system.low_contributors)}äºº")
    
    # å„æˆ¦ç•¥ã‚’è©•ä¾¡
    strategies = {
        'original': 'å…ƒã®æ‰‹æ³•',
        'diversity_weighted': 'å¤šæ§˜æ€§é‡ã¿ä»˜ã',
        'contribution_balanced': 'è²¢çŒ®é‡ãƒãƒ©ãƒ³ã‚¹',
        'temperature_scaled': 'æ¸©åº¦ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°',
        'hybrid': 'ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰'
    }
    
    sample_size = min(200, len(tasks))
    available_agents = set(trained_models.keys())
    
    results = {}
    
    for strategy_name, strategy_desc in strategies.items():
        print(f"\n## {strategy_desc}ã®è©•ä¾¡")
        print("-" * 40)
        
        strategy_results = {
            'top3_accuracy': 0,
            'diversity_score': 0,
            'contribution_distribution': {'high': 0, 'medium': 0, 'low': 0},
            'recommendations': []
        }
        
        correct_predictions = 0
        all_recommendations = []
        
        for i, (task, actual_author) in enumerate(tqdm(zip(tasks[:sample_size], ground_truth[:sample_size]), 
                                                      desc=f"{strategy_desc}è©•ä¾¡ä¸­")):
            if actual_author not in available_agents:
                continue
            
            try:
                task_features = extract_task_features_for_model(task)
                
                # æˆ¦ç•¥ã«å¿œã˜ã¦æ¨è–¦ã‚’å®Ÿè¡Œ
                if strategy_name == 'original':
                    recommendations = improved_system.original_recommendation(task_features, 3)
                elif strategy_name == 'diversity_weighted':
                    recommendations = improved_system.diversity_weighted_recommendation(task_features, 3)
                elif strategy_name == 'contribution_balanced':
                    recommendations = improved_system.contribution_balanced_recommendation(task_features, 3)
                elif strategy_name == 'temperature_scaled':
                    recommendations = improved_system.temperature_scaled_recommendation(task_features, 3)
                elif strategy_name == 'hybrid':
                    recommendations = improved_system.hybrid_recommendation(task_features, 3)
                
                recommended_agents = [agent for agent, _ in recommendations]
                all_recommendations.extend(recommended_agents)
                
                # Top-3ç²¾åº¦
                if actual_author in recommended_agents:
                    correct_predictions += 1
                
                # æ¨è–¦å±¥æ­´æ›´æ–°ï¼ˆå¤šæ§˜æ€§é‡ã¿ä»˜ãã®ãŸã‚ï¼‰
                improved_system.update_recommendation_history(recommended_agents)
                
            except Exception as e:
                continue
        
        # çµæœè¨ˆç®—
        evaluated_tasks = len([task for task, author in zip(tasks[:sample_size], ground_truth[:sample_size]) 
                              if author in available_agents])
        
        if evaluated_tasks > 0:
            strategy_results['top3_accuracy'] = correct_predictions / evaluated_tasks
        
        # å¤šæ§˜æ€§ã‚¹ã‚³ã‚¢ï¼ˆãƒ¦ãƒ‹ãƒ¼ã‚¯æ¨è–¦è€…æ•° / ç·æ¨è–¦æ•°ï¼‰
        if all_recommendations:
            unique_recommendations = len(set(all_recommendations))
            total_recommendations = len(all_recommendations)
            strategy_results['diversity_score'] = unique_recommendations / total_recommendations
        
        # è²¢çŒ®é‡åˆ†å¸ƒ
        recommendation_counter = Counter(all_recommendations)
        for agent, count in recommendation_counter.items():
            contribution = author_contribution.get(agent, 0)
            if contribution >= 50:
                strategy_results['contribution_distribution']['high'] += count
            elif contribution >= 10:
                strategy_results['contribution_distribution']['medium'] += count
            else:
                strategy_results['contribution_distribution']['low'] += count
        
        results[strategy_name] = strategy_results
        
        # çµæœè¡¨ç¤º
        print(f"   Top-3ç²¾åº¦: {strategy_results['top3_accuracy']:.3f}")
        print(f"   å¤šæ§˜æ€§ã‚¹ã‚³ã‚¢: {strategy_results['diversity_score']:.3f}")
        
        total_recs = sum(strategy_results['contribution_distribution'].values())
        if total_recs > 0:
            high_pct = strategy_results['contribution_distribution']['high'] / total_recs * 100
            medium_pct = strategy_results['contribution_distribution']['medium'] / total_recs * 100
            low_pct = strategy_results['contribution_distribution']['low'] / total_recs * 100
            
            print(f"   æ¨è–¦åˆ†å¸ƒ:")
            print(f"     é«˜è²¢çŒ®è€…: {high_pct:.1f}%")
            print(f"     ä¸­è²¢çŒ®è€…: {medium_pct:.1f}%")
            print(f"     ä½è²¢çŒ®è€…: {low_pct:.1f}%")
    
    # æ¯”è¼ƒçµæœ
    print(f"\n## æˆ¦ç•¥æ¯”è¼ƒçµæœ")
    print("-" * 40)
    
    print("| æˆ¦ç•¥ | Top-3ç²¾åº¦ | å¤šæ§˜æ€§ | é«˜è²¢çŒ®è€…% | ä¸­è²¢çŒ®è€…% | ä½è²¢çŒ®è€…% |")
    print("|------|-----------|--------|-----------|-----------|-----------|")
    
    for strategy_name, strategy_desc in strategies.items():
        result = results[strategy_name]
        total_recs = sum(result['contribution_distribution'].values())
        
        if total_recs > 0:
            high_pct = result['contribution_distribution']['high'] / total_recs * 100
            medium_pct = result['contribution_distribution']['medium'] / total_recs * 100
            low_pct = result['contribution_distribution']['low'] / total_recs * 100
        else:
            high_pct = medium_pct = low_pct = 0
        
        print(f"| {strategy_desc} | {result['top3_accuracy']:.3f} | {result['diversity_score']:.3f} | "
              f"{high_pct:.1f}% | {medium_pct:.1f}% | {low_pct:.1f}% |")
    
    # æ¨å¥¨æˆ¦ç•¥
    print(f"\n## æ¨å¥¨æ”¹å–„æˆ¦ç•¥")
    print("-" * 40)
    
    best_diversity = max(results.values(), key=lambda x: x['diversity_score'])
    best_accuracy = max(results.values(), key=lambda x: x['top3_accuracy'])
    
    print("### ğŸ¯ æ¨å¥¨æˆ¦ç•¥:")
    print("1. **è²¢çŒ®é‡ãƒãƒ©ãƒ³ã‚¹æ¨è–¦**: å¤šæ§˜æ€§ã‚’å¤§å¹…æ”¹å–„")
    print("2. **ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¨è–¦**: ç²¾åº¦ã¨å¤šæ§˜æ€§ã®ãƒãƒ©ãƒ³ã‚¹")
    print("3. **å¤šæ§˜æ€§é‡ã¿ä»˜ãæ¨è–¦**: æ¨è–¦å±¥æ­´ã‚’è€ƒæ…®ã—ãŸå…¬å¹³æ€§")
    
    print("\n### ğŸ”§ å®Ÿè£…ã®å„ªå…ˆé †ä½:")
    print("1. çŸ­æœŸ: è²¢çŒ®é‡ãƒãƒ©ãƒ³ã‚¹æ¨è–¦ï¼ˆå³åº§ã«å¤šæ§˜æ€§æ”¹å–„ï¼‰")
    print("2. ä¸­æœŸ: ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¨è–¦ï¼ˆç·åˆæ€§èƒ½å‘ä¸Šï¼‰")
    print("3. é•·æœŸ: å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã®å†ãƒãƒ©ãƒ³ã‚·ãƒ³ã‚°")
    
    return results


if __name__ == "__main__":
    results = evaluate_improved_strategies()
    
    print(f"\nğŸ¯ ã¾ã¨ã‚:")
    print(f"   è¤‡æ•°ã®æ”¹å–„æˆ¦ç•¥ã‚’å®Ÿè£…ãƒ»è©•ä¾¡å®Œäº†")
    print(f"   å¤šæ§˜æ€§ã¨å…¬å¹³æ€§ã®å¤§å¹…æ”¹å–„ãŒå¯èƒ½")
    print(f"   å®Ÿç”¨çš„ãªæ¨è–¦ã‚·ã‚¹ãƒ†ãƒ ã¸ã®é“ç­‹ã‚’æç¤º")