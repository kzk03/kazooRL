#!/usr/bin/env python3
"""
å¼·åŒ–å­¦ç¿’ãƒ™ãƒ¼ã‚¹ã®ã‚·ãƒ³ãƒ—ãƒ«æ¨è–¦ã‚·ã‚¹ãƒ†ãƒ 

ç¾åœ¨ã®SimpleSimilarityRecommenderã®æ‰‹æ³•ã‚’å¼·åŒ–å­¦ç¿’ç’°å¢ƒã«çµ±åˆï¼š
- TF-IDFã¨RandomForestã®äºˆæ¸¬ã‚’å ±é…¬ã¨ã—ã¦æ´»ç”¨
- é¡ä¼¼åº¦ã‚¹ã‚³ã‚¢ã‚’çŠ¶æ…‹è¡¨ç¾ã«çµ„ã¿è¾¼ã¿
- PPOã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã§æ¨è–¦ãƒãƒªã‚·ãƒ¼ã‚’å­¦ç¿’
"""

import argparse
import json
import pickle
import re
from collections import Counter, defaultdict
from datetime import datetime
from pathlib import Path

import gymnasium as gym
import numpy as np
import pandas as pd
import torch
import yaml

# ã‚·ãƒ³ãƒ—ãƒ«é¡ä¼¼åº¦æ¨è–¦ã‚·ã‚¹ãƒ†ãƒ ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from simple_similarity_recommender import SimpleSimilarityRecommender
from sklearn.ensemble import RandomForestClassifier
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.preprocessing import StandardScaler
from stable_baselines3 import PPO
from stable_baselines3.common.vec_env import DummyVecEnv


class RLSimilarityEnvironment(gym.Env):
    """å¼·åŒ–å­¦ç¿’ãƒ™ãƒ¼ã‚¹ã®é¡ä¼¼åº¦æ¨è–¦ç’°å¢ƒ"""
    
    def __init__(self, similarity_recommender, training_pairs, config):
        super().__init__()
        
        self.similarity_recommender = similarity_recommender
        self.training_pairs = training_pairs
        self.config = config
        
        # é–‹ç™ºè€…ãƒªã‚¹ãƒˆ
        self.developers = list(similarity_recommender.developer_profiles.keys())
        self.num_developers = len(self.developers)
        
        # è¡Œå‹•ç©ºé–“: é–‹ç™ºè€…é¸æŠ
        self.action_space = gym.spaces.Discrete(self.num_developers)
        
        # è¦³æ¸¬ç©ºé–“: TF-IDFã‚¹ã‚³ã‚¢ + ç‰¹å¾´é‡ã‚¹ã‚³ã‚¢ + åŸºæœ¬ç‰¹å¾´é‡
        # - TF-IDFã‚¹ã‚³ã‚¢: å„é–‹ç™ºè€…ã¸ã®é¡ä¼¼åº¦ (num_developersæ¬¡å…ƒ)
        # - ç‰¹å¾´é‡ã‚¹ã‚³ã‚¢: RandomForestã®äºˆæ¸¬ç¢ºç‡ (num_developersæ¬¡å…ƒ)  
        # - åŸºæœ¬ç‰¹å¾´é‡: ã‚¿ã‚¹ã‚¯ã®åŸºæœ¬ç‰¹å¾´ (11æ¬¡å…ƒ)
        obs_dim = self.num_developers * 2 + 11
        
        self.observation_space = gym.spaces.Box(
            low=-np.inf, high=np.inf,
            shape=(obs_dim,),
            dtype=np.float32
        )
        
        # ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ç®¡ç†
        self.current_episode = 0
        self.reset_count = 0
        
        print(f"ğŸ® å¼·åŒ–å­¦ç¿’é¡ä¼¼åº¦æ¨è–¦ç’°å¢ƒåˆæœŸåŒ–å®Œäº†")
        print(f"   é–‹ç™ºè€…æ•°: {self.num_developers}")
        print(f"   è¦³æ¸¬æ¬¡å…ƒ: {obs_dim}")
        print(f"   è¨“ç·´ãƒšã‚¢æ•°: {len(training_pairs)}")
    
    def reset(self, seed=None, options=None):
        """ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã‚’ãƒªã‚»ãƒƒãƒˆ"""
        super().reset(seed=seed)
        
        # ãƒ©ãƒ³ãƒ€ãƒ ã«è¨“ç·´ãƒšã‚¢ã‚’é¸æŠ
        pair = np.random.choice(self.training_pairs)
        self.current_task_data = pair['task_data']
        self.ground_truth_dev = pair['developer']
        
        # è¦³æ¸¬ã‚’ç”Ÿæˆ
        obs = self._get_observation()
        self.reset_count += 1
        
        return obs, {}
    
    def step(self, action):
        """ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚’å®Ÿè¡Œ"""
        if action >= self.num_developers:
            # ç„¡åŠ¹ãªã‚¢ã‚¯ã‚·ãƒ§ãƒ³
            reward = -5.0
            terminated = True
            obs = np.zeros(self.observation_space.shape[0])
            return obs, reward, terminated, False, {'invalid_action': True}
        
        selected_dev = self.developers[action]
        
        # å ±é…¬è¨ˆç®—
        reward = self._calculate_reward(selected_dev)
        
        # ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰çµ‚äº†
        terminated = True
        obs = np.zeros(self.observation_space.shape[0])
        
        info = {
            'selected_dev': selected_dev,
            'ground_truth_dev': self.ground_truth_dev,
            'correct_prediction': selected_dev == self.ground_truth_dev,
            'tfidf_scores': getattr(self, 'last_tfidf_scores', {}),
            'feature_scores': getattr(self, 'last_feature_scores', {})
        }
        
        return obs, reward, terminated, False, info
    
    def _get_observation(self):
        """ç¾åœ¨ã®è¦³æ¸¬ã‚’ç”Ÿæˆ"""
        # 1. TF-IDFã«ã‚ˆã‚‹é¡ä¼¼åº¦ã‚¹ã‚³ã‚¢
        tfidf_scores = self.similarity_recommender._predict_by_text_similarity(self.current_task_data)
        tfidf_vector = []
        for dev in self.developers:
            tfidf_vector.append(tfidf_scores.get(dev, 0.0))
        self.last_tfidf_scores = tfidf_scores
        
        # 2. RandomForestã«ã‚ˆã‚‹ç‰¹å¾´é‡ã‚¹ã‚³ã‚¢
        feature_scores = self.similarity_recommender._predict_by_features(self.current_task_data)
        feature_vector = []
        for dev in self.developers:
            feature_vector.append(feature_scores.get(dev, 0.0))
        self.last_feature_scores = feature_scores
        
        # 3. ã‚¿ã‚¹ã‚¯ã®åŸºæœ¬ç‰¹å¾´é‡
        basic_features = self.similarity_recommender.extract_basic_features(self.current_task_data)
        basic_vector = [
            basic_features.get('title_length', 0),
            basic_features.get('body_length', 0),
            basic_features.get('comments_count', 0),
            basic_features.get('is_bug', 0),
            basic_features.get('is_enhancement', 0),
            basic_features.get('is_documentation', 0),
            basic_features.get('is_question', 0),
            basic_features.get('is_help_wanted', 0),
            basic_features.get('label_count', 0),
            basic_features.get('is_open', 0),
            len(basic_features)  # ç‰¹å¾´é‡ã®ç·æ•°
        ]
        
        # è¦³æ¸¬ãƒ™ã‚¯ãƒˆãƒ«ã®çµåˆ
        obs = np.concatenate([tfidf_vector, feature_vector, basic_vector])
        return obs.astype(np.float32)
    
    def _calculate_reward(self, selected_dev):
        """å¤šé¢çš„ãªå ±é…¬è¨ˆç®—"""
        total_reward = 0.0
        
        # 1. æ­£è§£å ±é…¬ (æœ€é‡è¦)
        if selected_dev == self.ground_truth_dev:
            total_reward += 10.0
        
        # 2. TF-IDFé¡ä¼¼åº¦å ±é…¬
        if hasattr(self, 'last_tfidf_scores'):
            tfidf_score = self.last_tfidf_scores.get(selected_dev, 0.0)
            total_reward += tfidf_score * 3.0
        
        # 3. RandomForestç¢ºä¿¡åº¦å ±é…¬
        if hasattr(self, 'last_feature_scores'):
            rf_score = self.last_feature_scores.get(selected_dev, 0.0)
            total_reward += rf_score * 5.0
        
        # 4. çµ±åˆã‚¹ã‚³ã‚¢å ±é…¬ï¼ˆæ—¢å­˜æ‰‹æ³•ã¨ã®ä¸€è‡´ï¼‰
        if hasattr(self, 'last_tfidf_scores') and hasattr(self, 'last_feature_scores'):
            text_score = self.last_tfidf_scores.get(selected_dev, 0.0)
            feature_score = self.last_feature_scores.get(selected_dev, 0.0)
            
            # æ—¢å­˜æ‰‹æ³•ã¨åŒã˜é‡ã¿ä»˜ã‘
            combined_score = 0.6 * text_score + 0.4 * feature_score
            total_reward += combined_score * 2.0
        
        # 5. é–‹ç™ºè€…ã®å°‚é–€æ€§å ±é…¬
        dev_profile = self.similarity_recommender.developer_profiles.get(selected_dev, {})
        task_count = dev_profile.get('task_count', 0)
        if task_count > 0:
            # ã‚ˆã‚Šå¤šãã®ã‚¿ã‚¹ã‚¯ã‚’æ‹…å½“ã—ãŸé–‹ç™ºè€…ã«ã¯å°ã•ãªãƒœãƒ¼ãƒŠã‚¹
            experience_bonus = min(task_count / 100.0, 1.0)  # æœ€å¤§1.0
            total_reward += experience_bonus
        
        # 6. ãƒ©ãƒ™ãƒ«ä¸€è‡´å ±é…¬
        task_labels = [label.get('name', '').lower() for label in self.current_task_data.get('labels', [])]
        if task_labels and selected_dev in self.similarity_recommender.developer_profiles:
            dev_label_prefs = self.similarity_recommender.developer_profiles[selected_dev].get('label_preferences', {})
            label_match_score = 0.0
            
            for label in task_labels:
                if label in dev_label_prefs:
                    label_match_score += dev_label_prefs[label] / dev_profile.get('task_count', 1)
            
            total_reward += min(label_match_score, 2.0)  # æœ€å¤§2.0ã®ãƒœãƒ¼ãƒŠã‚¹
        
        return total_reward


class RLSimilarityRecommender:
    """å¼·åŒ–å­¦ç¿’ãƒ™ãƒ¼ã‚¹ã®é¡ä¼¼åº¦æ¨è–¦ã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self, config_path):
        self.config_path = config_path
        
        # è¨­å®šèª­ã¿è¾¼ã¿
        with open(config_path, 'r', encoding='utf-8') as f:
            self.config = yaml.safe_load(f)
        
        # ãƒ™ãƒ¼ã‚¹ã¨ãªã‚‹é¡ä¼¼åº¦æ¨è–¦ã‚·ã‚¹ãƒ†ãƒ 
        self.similarity_recommender = SimpleSimilarityRecommender(config_path)
        
        print("ğŸ¤– å¼·åŒ–å­¦ç¿’é¡ä¼¼åº¦æ¨è–¦ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å®Œäº†")
    
    def train_base_models(self, data_path):
        """ãƒ™ãƒ¼ã‚¹ã¨ãªã‚‹é¡ä¼¼åº¦ãƒ¢ãƒ‡ãƒ«ã‚’è¨“ç·´"""
        print("ğŸ“š ãƒ™ãƒ¼ã‚¹é¡ä¼¼åº¦ãƒ¢ãƒ‡ãƒ«è¨“ç·´ä¸­...")
        
        # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
        training_data, test_data = self.similarity_recommender.load_data(data_path)
        
        # å­¦ç¿’ãƒšã‚¢æŠ½å‡º
        training_pairs, developer_stats = self.similarity_recommender.extract_training_pairs(training_data)
        
        if not training_pairs:
            raise ValueError("å­¦ç¿’ãƒšã‚¢ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
        
        # é–‹ç™ºè€…ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«æ§‹ç¯‰
        self.similarity_recommender.developer_profiles = self.similarity_recommender.build_developer_profiles(training_pairs)
        
        # TF-IDFãƒ¢ãƒ‡ãƒ«è¨“ç·´
        self.similarity_recommender.train_text_similarity_model(self.similarity_recommender.developer_profiles)
        
        # RandomForestãƒ¢ãƒ‡ãƒ«è¨“ç·´
        self.similarity_recommender.train_feature_model(training_pairs)
        
        print("âœ… ãƒ™ãƒ¼ã‚¹é¡ä¼¼åº¦ãƒ¢ãƒ‡ãƒ«è¨“ç·´å®Œäº†")
        return training_pairs, test_data
    
    def train_rl_policy(self, training_pairs, total_timesteps=100000):
        """å¼·åŒ–å­¦ç¿’ãƒãƒªã‚·ãƒ¼ã‚’è¨“ç·´"""
        print("ğŸ¯ å¼·åŒ–å­¦ç¿’ãƒãƒªã‚·ãƒ¼è¨“ç·´é–‹å§‹...")
        
        # ç’°å¢ƒä½œæˆ
        def make_env():
            return RLSimilarityEnvironment(
                self.similarity_recommender, training_pairs, self.config
            )
        
        env = DummyVecEnv([make_env])
        
        # PPOã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆä½œæˆ
        self.rl_agent = PPO(
            "MlpPolicy",
            env,
            verbose=1,
            learning_rate=3e-4,
            n_steps=2048,
            batch_size=64,
            n_epochs=10,
            gamma=0.99,
            gae_lambda=0.95,
            clip_range=0.2,
            ent_coef=0.01,  # æ¢ç´¢ã‚’ä¿ƒé€²
            device="auto",
            seed=42
        )
        
        # è¨“ç·´å®Ÿè¡Œ
        print(f"   è¨“ç·´ã‚¹ãƒ†ãƒƒãƒ—æ•°: {total_timesteps:,}")
        print(f"   æ¨å®šè¨“ç·´æ™‚é–“: {total_timesteps / 10000:.1f}åˆ†")
        
        # ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯è¨­å®š
        def callback(locals_, globals_):
            if locals_['self'].num_timesteps % 10000 == 0:
                print(f"   é€²æ—: {locals_['self'].num_timesteps:,}/{total_timesteps:,} ã‚¹ãƒ†ãƒƒãƒ—")
            return True
        
        self.rl_agent.learn(
            total_timesteps=total_timesteps,
            callback=callback
        )
        
        print("âœ… å¼·åŒ–å­¦ç¿’ãƒãƒªã‚·ãƒ¼è¨“ç·´å®Œäº†")
    
    def predict_with_rl(self, test_data):
        """å¼·åŒ–å­¦ç¿’ãƒãƒªã‚·ãƒ¼ã§äºˆæ¸¬"""
        print("ğŸ¤– å¼·åŒ–å­¦ç¿’äºˆæ¸¬å®Ÿè¡Œä¸­...")
        
        predictions = {}
        prediction_scores = {}
        
        # ãƒ†ã‚¹ãƒˆç’°å¢ƒè¨­å®š
        test_env = RLSimilarityEnvironment(
            self.similarity_recommender, [], self.config
        )
        
        prediction_count = 0
        
        for task_data in test_data:
            task_id = task_data.get('id') or task_data.get('number')
            if not task_id:
                continue
            
            try:
                # ã‚¿ã‚¹ã‚¯ã‚’ç’°å¢ƒã«è¨­å®š
                test_env.current_task_data = task_data
                obs = test_env._get_observation()
                
                # å¼·åŒ–å­¦ç¿’ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã§äºˆæ¸¬
                action, _ = self.rl_agent.predict(obs, deterministic=True)
                
                if action < len(test_env.developers):
                    predicted_dev = test_env.developers[action]
                    
                    # äºˆæ¸¬ä¿¡é ¼åº¦è¨ˆç®—
                    tfidf_score = test_env.last_tfidf_scores.get(predicted_dev, 0.0)
                    feature_score = test_env.last_feature_scores.get(predicted_dev, 0.0)
                    combined_score = 0.6 * tfidf_score + 0.4 * feature_score
                    
                    predictions[task_id] = predicted_dev
                    prediction_scores[task_id] = {
                        'predicted_dev': predicted_dev,
                        'rl_action': int(action),
                        'tfidf_score': tfidf_score,
                        'feature_score': feature_score,
                        'combined_score': combined_score,
                        'method': 'reinforcement_learning'
                    }
                    
                    prediction_count += 1
                    
                    if prediction_count % 100 == 0:
                        print(f"   é€²æ—: {prediction_count}/{len(test_data)}")
                
            except Exception as e:
                print(f"âš ï¸ ã‚¿ã‚¹ã‚¯ {task_id} ã®RLäºˆæ¸¬ã‚¨ãƒ©ãƒ¼: {e}")
                continue
        
        print(f"   RLäºˆæ¸¬å®Œäº†: {len(predictions)} ã‚¿ã‚¹ã‚¯")
        return predictions, prediction_scores
    
    def compare_with_baseline(self, test_data):
        """ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³æ‰‹æ³•ã¨æ¯”è¼ƒ"""
        print("ğŸ“Š ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³æ¯”è¼ƒå®Ÿè¡Œä¸­...")
        
        # 1. æ—¢å­˜ã®é¡ä¼¼åº¦ãƒ™ãƒ¼ã‚¹äºˆæ¸¬
        print("   é¡ä¼¼åº¦ãƒ™ãƒ¼ã‚¹äºˆæ¸¬...")
        similarity_predictions, similarity_scores, test_assignments = self.similarity_recommender.predict_assignments(
            test_data, self.similarity_recommender.developer_profiles
        )
        
        # 2. å¼·åŒ–å­¦ç¿’ãƒ™ãƒ¼ã‚¹äºˆæ¸¬
        print("   å¼·åŒ–å­¦ç¿’ãƒ™ãƒ¼ã‚¹äºˆæ¸¬...")
        rl_predictions, rl_scores = self.predict_with_rl(test_data)
        
        # 3. æ¯”è¼ƒè©•ä¾¡
        comparison_results = {
            'similarity_method': {
                'predictions': similarity_predictions,
                'scores': similarity_scores,
                'metrics': self._evaluate_predictions(similarity_predictions, test_assignments)
            },
            'rl_method': {
                'predictions': rl_predictions,
                'scores': rl_scores,
                'metrics': self._evaluate_predictions(rl_predictions, test_assignments)
            },
            'test_assignments': test_assignments
        }
        
        return comparison_results
    
    def _evaluate_predictions(self, predictions, test_assignments):
        """äºˆæ¸¬ã®è©•ä¾¡"""
        if not predictions or not test_assignments:
            return {'accuracy': 0.0, 'correct': 0, 'total': 0}
        
        common_tasks = set(predictions.keys()) & set(test_assignments.keys())
        if not common_tasks:
            return {'accuracy': 0.0, 'correct': 0, 'total': 0}
        
        correct = sum(1 for task_id in common_tasks if predictions[task_id] == test_assignments[task_id])
        total = len(common_tasks)
        accuracy = correct / total
        
        return {
            'accuracy': accuracy,
            'correct': correct,
            'total': total
        }
    
    def save_rl_model(self, output_dir="models"):
        """å¼·åŒ–å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜"""
        output_dir = Path(output_dir)
        output_dir.mkdir(exist_ok=True)
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # RLã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆä¿å­˜
        rl_model_path = output_dir / f"rl_similarity_recommender_{timestamp}.zip"
        self.rl_agent.save(rl_model_path)
        
        # ãƒ™ãƒ¼ã‚¹é¡ä¼¼åº¦ãƒ¢ãƒ‡ãƒ«ä¿å­˜
        base_model_path = self.similarity_recommender.save_model(output_dir)
        
        print(f"âœ… RLé¡ä¼¼åº¦ãƒ¢ãƒ‡ãƒ«ä¿å­˜: {rl_model_path}")
        return rl_model_path, base_model_path
    
    def run_full_pipeline(self, data_path, output_dir="outputs"):
        """å®Œå…¨ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè¡Œ"""
        print("ğŸš€ å¼·åŒ–å­¦ç¿’é¡ä¼¼åº¦æ¨è–¦ã‚·ã‚¹ãƒ†ãƒ å®Ÿè¡Œé–‹å§‹")
        print("=" * 70)
        
        # 1. ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«è¨“ç·´
        training_pairs, test_data = self.train_base_models(data_path)
        
        # 2. å¼·åŒ–å­¦ç¿’ãƒãƒªã‚·ãƒ¼è¨“ç·´
        self.train_rl_policy(training_pairs)
        
        # 3. æ¯”è¼ƒè©•ä¾¡
        comparison_results = self.compare_with_baseline(test_data)
        
        # 4. çµæœä¿å­˜
        self.save_comparison_results(comparison_results, output_dir)
        
        # 5. ãƒ¢ãƒ‡ãƒ«ä¿å­˜
        model_paths = self.save_rl_model()
        
        print("âœ… å¼·åŒ–å­¦ç¿’é¡ä¼¼åº¦æ¨è–¦ã‚·ã‚¹ãƒ†ãƒ å®Œäº†")
        return comparison_results
    
    def save_comparison_results(self, comparison_results, output_dir):
        """æ¯”è¼ƒçµæœã®ä¿å­˜"""
        output_dir = Path(output_dir)
        output_dir.mkdir(exist_ok=True)
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # ãƒ¡ãƒˆãƒªã‚¯ã‚¹æ¯”è¼ƒ
        comparison_metrics = {
            'similarity_accuracy': comparison_results['similarity_method']['metrics']['accuracy'],
            'rl_accuracy': comparison_results['rl_method']['metrics']['accuracy'],
            'similarity_correct': comparison_results['similarity_method']['metrics']['correct'],
            'rl_correct': comparison_results['rl_method']['metrics']['correct'],
            'total_tasks': comparison_results['similarity_method']['metrics']['total'],
            'improvement': (comparison_results['rl_method']['metrics']['accuracy'] - 
                          comparison_results['similarity_method']['metrics']['accuracy'])
        }
        
        # JSONä¿å­˜
        metrics_path = output_dir / f"rl_similarity_comparison_{timestamp}.json"
        with open(metrics_path, 'w', encoding='utf-8') as f:
            json.dump(comparison_metrics, f, indent=2, ensure_ascii=False, default=str)
        
        # è©³ç´°çµæœCSVä¿å­˜
        results = []
        all_task_ids = (set(comparison_results['similarity_method']['predictions'].keys()) | 
                       set(comparison_results['rl_method']['predictions'].keys()) |
                       set(comparison_results['test_assignments'].keys()))
        
        for task_id in all_task_ids:
            result = {
                'task_id': task_id,
                'actual_developer': comparison_results['test_assignments'].get(task_id),
                'similarity_prediction': comparison_results['similarity_method']['predictions'].get(task_id),
                'rl_prediction': comparison_results['rl_method']['predictions'].get(task_id),
                'similarity_correct': (comparison_results['similarity_method']['predictions'].get(task_id) == 
                                     comparison_results['test_assignments'].get(task_id)),
                'rl_correct': (comparison_results['rl_method']['predictions'].get(task_id) == 
                              comparison_results['test_assignments'].get(task_id))
            }
            
            # ã‚¹ã‚³ã‚¢æƒ…å ±è¿½åŠ 
            if task_id in comparison_results['similarity_method']['scores']:
                sim_score = comparison_results['similarity_method']['scores'][task_id]
                result['similarity_combined_score'] = sim_score.get('combined_score', 0.0)
                result['similarity_text_score'] = sim_score.get('text_score', 0.0)
                result['similarity_feature_score'] = sim_score.get('feature_score', 0.0)
            
            if task_id in comparison_results['rl_method']['scores']:
                rl_score = comparison_results['rl_method']['scores'][task_id]
                result['rl_combined_score'] = rl_score.get('combined_score', 0.0)
                result['rl_tfidf_score'] = rl_score.get('tfidf_score', 0.0)
                result['rl_feature_score'] = rl_score.get('feature_score', 0.0)
            
            results.append(result)
        
        results_df = pd.DataFrame(results)
        results_path = output_dir / f"rl_similarity_detailed_{timestamp}.csv"
        results_df.to_csv(results_path, index=False, encoding='utf-8')
        
        print(f"âœ… æ¯”è¼ƒãƒ¡ãƒˆãƒªã‚¯ã‚¹ä¿å­˜: {metrics_path}")
        print(f"âœ… è©³ç´°çµæœä¿å­˜: {results_path}")
        
        # çµæœã‚µãƒãƒªãƒ¼è¡¨ç¤º
        print("\nğŸ“Š æ¯”è¼ƒçµæœã‚µãƒãƒªãƒ¼:")
        print(f"   é¡ä¼¼åº¦ãƒ™ãƒ¼ã‚¹ç²¾åº¦: {comparison_metrics['similarity_accuracy']:.3f}")
        print(f"   å¼·åŒ–å­¦ç¿’ç²¾åº¦: {comparison_metrics['rl_accuracy']:.3f}")
        print(f"   æ”¹å–„åº¦: {comparison_metrics['improvement']:+.3f}")
        print(f"   è©•ä¾¡ã‚¿ã‚¹ã‚¯æ•°: {comparison_metrics['total_tasks']}")


def main():
    parser = argparse.ArgumentParser(description='å¼·åŒ–å­¦ç¿’é¡ä¼¼åº¦æ¨è–¦ã‚·ã‚¹ãƒ†ãƒ ')
    parser.add_argument('--config', default='configs/unified_rl.yaml',
                       help='è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹')
    parser.add_argument('--data', default='data/backlog.json',
                       help='çµ±åˆãƒ‡ãƒ¼ã‚¿ãƒ‘ã‚¹')
    parser.add_argument('--output', default='outputs',
                       help='å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª')
    parser.add_argument('--timesteps', type=int, default=50000,
                       help='å¼·åŒ–å­¦ç¿’ã®è¨“ç·´ã‚¹ãƒ†ãƒƒãƒ—æ•°')
    
    args = parser.parse_args()
    
    # å¼·åŒ–å­¦ç¿’é¡ä¼¼åº¦æ¨è–¦ã‚·ã‚¹ãƒ†ãƒ å®Ÿè¡Œ
    rl_recommender = RLSimilarityRecommender(args.config)
    comparison_results = rl_recommender.run_full_pipeline(args.data, args.output)
    
    return 0


if __name__ == "__main__":
    exit(main())
