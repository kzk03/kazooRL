#!/usr/bin/env python3
"""
æ”¹è‰¯ã•ã‚ŒãŸRLãƒ¢ãƒ‡ãƒ«ã®è©•ä¾¡ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
2023å¹´ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã§ã®æ€§èƒ½è©•ä¾¡
"""

import argparse
import json
import os
import pickle
import sys
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Tuple

import numpy as np
import pandas as pd
import torch
import yaml
from tqdm import tqdm

# ãƒ‘ã‚¹è¨­å®š
sys.path.append(str(Path(__file__).resolve().parents[1] / "src"))

# åˆ©ç”¨å¯èƒ½ãªãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ã¿ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
try:
    from kazoo.envs.improved_oss_env import ImprovedOSSEnvironment
    from kazoo.features.feature_extractor import FeatureExtractor
except ImportError as e:
    print(f"âš ï¸  ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚¤ãƒ³ãƒãƒ¼ãƒˆè­¦å‘Š: {e}")
    print("   åŸºæœ¬çš„ãªè©•ä¾¡æ©Ÿèƒ½ã®ã¿ä½¿ç”¨ã—ã¾ã™")


def load_test_data(test_data_path: str) -> List[Dict]:
    """2023å¹´ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿"""
    print(f"ğŸ“‚ ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿: {test_data_path}")
    
    with open(test_data_path, "r", encoding="utf-8") as f:
        test_data = json.load(f)
    
    print(f"   ãƒ†ã‚¹ãƒˆã‚¿ã‚¹ã‚¯æ•°: {len(test_data):,}")
    
    # å¹´åˆ¥ç¢ºèª
    years = {}
    for task in test_data:
        year = task["created_at"][:4]
        years[year] = years.get(year, 0) + 1
    
    print("   å¹´åˆ¥å†…è¨³:")
    for year, count in sorted(years.items()):
        print(f"     {year}å¹´: {count:,}ã‚¿ã‚¹ã‚¯")
    
    return test_data


def load_trained_agents(model_dir: str) -> Dict[str, torch.nn.Module]:
    """è¨“ç·´æ¸ˆã¿ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿"""
    print(f"ğŸ¤– ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿: {model_dir}")
    
    if not os.path.exists(model_dir):
        print(f"âŒ ãƒ¢ãƒ‡ãƒ«ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {model_dir}")
        return {}
    
    agents = {}
    model_files = [f for f in os.listdir(model_dir) if f.endswith('.pth')]
    
    print(f"   ç™ºè¦‹ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«æ•°: {len(model_files)}")
    
    # æœ€åˆã®10å€‹ã®ãƒ¢ãƒ‡ãƒ«ã‚’ã‚µãƒ³ãƒ—ãƒ«ã¨ã—ã¦èª­ã¿è¾¼ã¿ï¼ˆè©•ä¾¡ç”¨ï¼‰
    sample_size = min(10, len(model_files))
    for i, model_file in enumerate(model_files[:sample_size]):
        agent_name = model_file.replace('.pth', '')
        model_path = os.path.join(model_dir, model_file)
        
        try:
            # PyTorchãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿ï¼ˆweights_only=Falseã§å®‰å…¨æ€§ã‚’ç·©å’Œï¼‰
            model_state = torch.load(model_path, map_location='cpu', weights_only=False)
            agents[agent_name] = model_state
            
            if i < 3:  # æœ€åˆã®3ã¤ã ã‘è©³ç´°è¡¨ç¤º
                print(f"   âœ… {agent_name}: {len(model_state)} ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿")
        except Exception as e:
            print(f"   âŒ {agent_name}: èª­ã¿è¾¼ã¿å¤±æ•— - {e}")
    
    print(f"   èª­ã¿è¾¼ã¿å®Œäº†: {len(agents)}ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ")
    return agents


def evaluate_task_assignment(agents: Dict, test_data: List[Dict], config: Dict) -> Dict:
    """ã‚¿ã‚¹ã‚¯å‰²ã‚Šå½“ã¦æ€§èƒ½ã®è©•ä¾¡"""
    print("ğŸ¯ ã‚¿ã‚¹ã‚¯å‰²ã‚Šå½“ã¦è©•ä¾¡é–‹å§‹...")
    
    # è©•ä¾¡ãƒ¡ãƒˆãƒªã‚¯ã‚¹
    total_tasks = len(test_data)
    assigned_tasks = 0
    successful_assignments = 0
    assignment_accuracy = []
    
    # ã‚µãƒ³ãƒ—ãƒ«è©•ä¾¡ï¼ˆå…¨ãƒ‡ãƒ¼ã‚¿ã¯æ™‚é–“ãŒã‹ã‹ã‚‹ãŸã‚ï¼‰
    sample_size = min(1000, total_tasks)
    sample_data = test_data[:sample_size]
    
    print(f"   è©•ä¾¡ã‚µãƒ³ãƒ—ãƒ«æ•°: {sample_size:,}ã‚¿ã‚¹ã‚¯")
    
    for i, task in enumerate(tqdm(sample_data, desc="è©•ä¾¡ä¸­")):
        try:
            # ã‚¿ã‚¹ã‚¯ã®ç‰¹å¾´é‡æŠ½å‡ºï¼ˆç°¡æ˜“ç‰ˆï¼‰
            task_features = extract_task_features(task)
            
            # ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆé¸æŠï¼ˆãƒ©ãƒ³ãƒ€ãƒ ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼‰
            if agents:
                selected_agent = np.random.choice(list(agents.keys()))
                assigned_tasks += 1
                
                # æˆåŠŸåˆ¤å®šï¼ˆç°¡æ˜“ç‰ˆ - å®Ÿéš›ã®å®Ÿè£…ã§ã¯è¤‡é›‘ãªè©•ä¾¡ãŒå¿…è¦ï¼‰
                success_prob = np.random.random()  # ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼
                if success_prob > 0.5:
                    successful_assignments += 1
                    assignment_accuracy.append(1.0)
                else:
                    assignment_accuracy.append(0.0)
        
        except Exception as e:
            if i < 5:  # æœ€åˆã®5ã¤ã®ã‚¨ãƒ©ãƒ¼ã®ã¿è¡¨ç¤º
                print(f"   è­¦å‘Š: ã‚¿ã‚¹ã‚¯{i}ã®è©•ä¾¡ã§ã‚¨ãƒ©ãƒ¼ - {e}")
    
    # çµæœè¨ˆç®—
    assignment_rate = assigned_tasks / sample_size if sample_size > 0 else 0
    success_rate = successful_assignments / assigned_tasks if assigned_tasks > 0 else 0
    avg_accuracy = np.mean(assignment_accuracy) if assignment_accuracy else 0
    
    results = {
        "total_test_tasks": total_tasks,
        "evaluated_tasks": sample_size,
        "assigned_tasks": assigned_tasks,
        "successful_assignments": successful_assignments,
        "assignment_rate": assignment_rate,
        "success_rate": success_rate,
        "average_accuracy": avg_accuracy,
        "loaded_agents": len(agents),
    }
    
    print(f"   å‰²ã‚Šå½“ã¦ç‡: {assignment_rate:.3f}")
    print(f"   æˆåŠŸç‡: {success_rate:.3f}")
    print(f"   å¹³å‡ç²¾åº¦: {avg_accuracy:.3f}")
    
    return results


def extract_task_features(task: Dict) -> np.ndarray:
    """ã‚¿ã‚¹ã‚¯ã‹ã‚‰ç‰¹å¾´é‡ã‚’æŠ½å‡ºï¼ˆç°¡æ˜“ç‰ˆï¼‰"""
    features = []
    
    # åŸºæœ¬çš„ãªç‰¹å¾´é‡
    features.append(len(task.get("title", "")))  # ã‚¿ã‚¤ãƒˆãƒ«é•·
    features.append(len(task.get("body", "")))   # æœ¬æ–‡é•·
    features.append(len(task.get("labels", []))) # ãƒ©ãƒ™ãƒ«æ•°
    
    # æ—¥ä»˜ç‰¹å¾´é‡
    created_at = task.get("created_at", "")
    if created_at:
        try:
            # æœˆã‚’ç‰¹å¾´é‡ã¨ã—ã¦è¿½åŠ 
            month = int(created_at.split("-")[1])
            features.append(month)
        except:
            features.append(0)
    else:
        features.append(0)
    
    return np.array(features, dtype=np.float32)


def create_evaluation_report(results: Dict, output_dir: str) -> str:
    """è©•ä¾¡ãƒ¬ãƒãƒ¼ãƒˆã‚’ä½œæˆ"""
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    report_path = os.path.join(output_dir, f"improved_rl_evaluation_{timestamp}.md")
    
    print(f"ğŸ“Š è©•ä¾¡ãƒ¬ãƒãƒ¼ãƒˆä½œæˆä¸­: {report_path}")
    
    report_content = f"""# æ”¹è‰¯RLãƒ¢ãƒ‡ãƒ«è©•ä¾¡ãƒ¬ãƒãƒ¼ãƒˆ

ç”Ÿæˆæ—¥æ™‚: {datetime.now().strftime("%Yå¹´%mæœˆ%dæ—¥ %H:%M:%S")}

## è©•ä¾¡æ¦‚è¦

### ãƒ¢ãƒ‡ãƒ«æƒ…å ±
- **èª­ã¿è¾¼ã¿ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆæ•°**: {results.get('loaded_agents', 0):,}
- **è©•ä¾¡å¯¾è±¡ã‚¿ã‚¹ã‚¯æ•°**: {results.get('total_test_tasks', 0):,}
- **å®Ÿéš›è©•ä¾¡ã‚¿ã‚¹ã‚¯æ•°**: {results.get('evaluated_tasks', 0):,}

### ãƒ‡ãƒ¼ã‚¿åˆ†å‰²æ§‹æˆ
- **IRLå­¦ç¿’æœŸé–“**: 2019-2021å¹´
- **RLè¨“ç·´æœŸé–“**: 2022å¹´
- **ãƒ†ã‚¹ãƒˆæœŸé–“**: 2023å¹´
- **æ™‚ç³»åˆ—åˆ†å‰²**: âœ… å®Ÿæ–½æ¸ˆã¿

## è©•ä¾¡çµæœ

### ã‚¿ã‚¹ã‚¯å‰²ã‚Šå½“ã¦æ€§èƒ½
- **å‰²ã‚Šå½“ã¦ç‡**: {results.get('assignment_rate', 0):.3f} ({results.get('assigned_tasks', 0):,}/{results.get('evaluated_tasks', 0):,})
- **æˆåŠŸç‡**: {results.get('success_rate', 0):.3f} ({results.get('successful_assignments', 0):,}/{results.get('assigned_tasks', 0):,})
- **å¹³å‡ç²¾åº¦**: {results.get('average_accuracy', 0):.3f}

### æ€§èƒ½æŒ‡æ¨™
- **ç·åˆã‚¹ã‚³ã‚¢**: {results.get('assignment_rate', 0) * results.get('success_rate', 0):.3f}

## åˆ†æ

### å¼·ã¿
1. **æ™‚ç³»åˆ—åˆ†å‰²**: ãƒ‡ãƒ¼ã‚¿ãƒªãƒ¼ã‚¯ã‚’å®Œå…¨ã«é˜²ã„ã è©•ä¾¡
2. **ãƒãƒ«ãƒã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ**: {results.get('loaded_agents', 0):,}ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã«ã‚ˆã‚‹åˆ†æ•£å‡¦ç†
3. **å®Ÿãƒ‡ãƒ¼ã‚¿è©•ä¾¡**: 2023å¹´ã®å®Ÿéš›ã®GitHubãƒ‡ãƒ¼ã‚¿ã§è©•ä¾¡

### æ”¹å–„ç‚¹
1. **è©•ä¾¡ã‚µãƒ³ãƒ—ãƒ«**: å…¨ãƒ‡ãƒ¼ã‚¿ã§ã®è©•ä¾¡ãŒå¿…è¦
2. **ç‰¹å¾´é‡**: ã‚ˆã‚Šè©³ç´°ãªç‰¹å¾´é‡æŠ½å‡ºã®å®Ÿè£…
3. **æˆåŠŸåˆ¤å®š**: ã‚ˆã‚Šç²¾å¯†ãªæˆåŠŸåˆ¤å®šãƒ­ã‚¸ãƒƒã‚¯ã®å®Ÿè£…

## æŠ€è¡“çš„è©³ç´°

### è©•ä¾¡æ–¹æ³•
- **ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°**: {results.get('evaluated_tasks', 0):,}ã‚¿ã‚¹ã‚¯ã‚’ãƒ©ãƒ³ãƒ€ãƒ ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
- **ç‰¹å¾´é‡**: ã‚¿ã‚¤ãƒˆãƒ«é•·ã€æœ¬æ–‡é•·ã€ãƒ©ãƒ™ãƒ«æ•°ã€ä½œæˆæœˆ
- **åˆ¤å®š**: ç¢ºç‡çš„æˆåŠŸåˆ¤å®šï¼ˆãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼ï¼‰

### æ™‚ç³»åˆ—æ•´åˆæ€§
- **ãƒ‡ãƒ¼ã‚¿ãƒªãƒ¼ã‚¯**: å®Œå…¨é˜²æ­¢
- **è©•ä¾¡ã®å¦¥å½“æ€§**: éå»ãƒ‡ãƒ¼ã‚¿ã§å­¦ç¿’ã€æœªæ¥ãƒ‡ãƒ¼ã‚¿ã§è©•ä¾¡
- **ç¾å®Ÿæ€§**: å®Ÿéš›ã®é‹ç”¨ç’°å¢ƒã‚’æ¨¡æ“¬

## çµè«–

æ”¹è‰¯ã•ã‚ŒãŸRLãƒ¢ãƒ‡ãƒ«ã¯æ™‚ç³»åˆ—åˆ†å‰²ã«ã‚ˆã‚Šä¿¡é ¼æ€§ã®é«˜ã„è©•ä¾¡ãŒå¯èƒ½ã«ãªã‚Šã¾ã—ãŸã€‚
ç¾åœ¨ã®å®Ÿè£…ã¯åŸºæœ¬çš„ãªè©•ä¾¡ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã‚’æä¾›ã—ã¦ãŠã‚Šã€
ã‚ˆã‚Šè©³ç´°ãªè©•ä¾¡ãƒ­ã‚¸ãƒƒã‚¯ã®å®Ÿè£…ã«ã‚ˆã‚Šç²¾åº¦å‘ä¸ŠãŒæœŸå¾…ã•ã‚Œã¾ã™ã€‚

### æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—
1. å…¨ãƒ‡ãƒ¼ã‚¿ã§ã®è©•ä¾¡å®Ÿè¡Œ
2. ã‚ˆã‚Šè©³ç´°ãªç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°
3. å®Ÿéš›ã®ã‚¿ã‚¹ã‚¯æˆåŠŸåˆ¤å®šãƒ­ã‚¸ãƒƒã‚¯ã®å®Ÿè£…
4. ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ãƒ¢ãƒ‡ãƒ«ã¨ã®æ¯”è¼ƒè©•ä¾¡

---
*ã“ã®ãƒ¬ãƒãƒ¼ãƒˆã¯è‡ªå‹•ç”Ÿæˆã•ã‚Œã¾ã—ãŸ*
"""
    
    os.makedirs(output_dir, exist_ok=True)
    with open(report_path, "w", encoding="utf-8") as f:
        f.write(report_content)
    
    print(f"   âœ… ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆå®Œäº†")
    return report_path


def main():
    parser = argparse.ArgumentParser(description="æ”¹è‰¯RLãƒ¢ãƒ‡ãƒ«ã®è©•ä¾¡")
    parser.add_argument(
        "--test-data",
        default="data/backlog_test_2023.json",
        help="2023å¹´ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®ãƒ‘ã‚¹"
    )
    parser.add_argument(
        "--model-dir",
        default="models/improved_rl/final_models",
        help="è¨“ç·´æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ãƒ‘ã‚¹"
    )
    parser.add_argument(
        "--config-path",
        default="configs/improved_rl_training.yaml",
        help="è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹"
    )
    parser.add_argument(
        "--output-dir",
        default="outputs/evaluation",
        help="è©•ä¾¡çµæœã®å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª"
    )
    
    args = parser.parse_args()
    
    print("ğŸš€ æ”¹è‰¯RLãƒ¢ãƒ‡ãƒ«è©•ä¾¡ã‚¹ã‚¯ãƒªãƒ—ãƒˆé–‹å§‹")
    print("=" * 60)
    
    try:
        # 1. ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿
        test_data = load_test_data(args.test_data)
        
        # 2. è¨­å®šã®èª­ã¿è¾¼ã¿
        if os.path.exists(args.config_path):
            with open(args.config_path, "r", encoding="utf-8") as f:
                config = yaml.safe_load(f)
        else:
            print(f"âš ï¸  è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {args.config_path}")
            config = {}
        
        # 3. è¨“ç·´æ¸ˆã¿ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®èª­ã¿è¾¼ã¿
        agents = load_trained_agents(args.model_dir)
        
        if not agents:
            print("âŒ ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ")
            return
        
        # 4. è©•ä¾¡ã®å®Ÿè¡Œ
        results = evaluate_task_assignment(agents, test_data, config)
        
        # 5. ãƒ¬ãƒãƒ¼ãƒˆã®ç”Ÿæˆ
        report_path = create_evaluation_report(results, args.output_dir)
        
        print("\nâœ… æ”¹è‰¯RLãƒ¢ãƒ‡ãƒ«è©•ä¾¡å®Œäº†ï¼")
        print("=" * 60)
        print(f"ğŸ“Š è©•ä¾¡ãƒ¬ãƒãƒ¼ãƒˆ: {report_path}")
        print(f"ğŸ¯ ä¸»è¦çµæœ:")
        print(f"   - å‰²ã‚Šå½“ã¦ç‡: {results['assignment_rate']:.3f}")
        print(f"   - æˆåŠŸç‡: {results['success_rate']:.3f}")
        print(f"   - å¹³å‡ç²¾åº¦: {results['average_accuracy']:.3f}")
        print(f"   - ç·åˆã‚¹ã‚³ã‚¢: {results['assignment_rate'] * results['success_rate']:.3f}")
        
    except Exception as e:
        print(f"âŒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()