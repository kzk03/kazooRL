#!/usr/bin/env python3
"""
ãƒ©ãƒ³ã‚­ãƒ³ã‚°é¡ä¼¼åº¦è©•ä¾¡ã‚·ã‚¹ãƒ†ãƒ 

é–‹ç™ºè€…ãƒ—ãƒ¼ãƒ«ãŒç•°ãªã£ã¦ã‚‚ã€ç‰¹å¾´ãƒ™ãƒ¼ã‚¹ã®é¡ä¼¼åº¦ã§ãƒ©ãƒ³ã‚­ãƒ³ã‚°è©•ä¾¡ã‚’å®Ÿè¡Œ
"""

import argparse
import json

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
import sys
from collections import Counter, defaultdict
from datetime import datetime
from pathlib import Path

import numpy as np
import pandas as pd
import torch
import yaml

# Stable-Baselines3
from stable_baselines3 import PPO

sys.path.append('/Users/kazuki-h/rl/kazoo')
sys.path.append('/Users/kazuki-h/rl/kazoo/src')

from scripts.train_simple_unified_rl import SimpleTaskAssignmentEnv
from src.kazoo.envs.task import Task


class RankingSimilarityEvaluator:
    """ãƒ©ãƒ³ã‚­ãƒ³ã‚°é¡ä¼¼åº¦ã§ã®è©•ä¾¡ã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self, config_path, model_path, test_data_path):
        """
        Args:
            config_path: è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
            model_path: å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ãƒ‘ã‚¹
            test_data_path: ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ãƒ‘ã‚¹
        """
        self.config_path = config_path
        self.model_path = model_path
        self.test_data_path = test_data_path
        
        # è¨­å®šèª­ã¿è¾¼ã¿
        with open(config_path, 'r', encoding='utf-8') as f:
            self.config = yaml.safe_load(f)
        
        # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ï¼ˆ2023å¹´ã®ã¿ï¼‰
        with open(test_data_path, 'r', encoding='utf-8') as f:
            all_data = json.load(f)
        
        self.test_data = []
        for task in all_data:
            created_at = task.get('created_at', '')
            if created_at.startswith('2023'):
                self.test_data.append(task)
        
        print(f"ğŸ“Š å…¨ãƒ‡ãƒ¼ã‚¿: {len(all_data):,} ã‚¿ã‚¹ã‚¯")
        print(f"ğŸ“Š 2023å¹´ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿: {len(self.test_data):,} ã‚¿ã‚¹ã‚¯")
        
        # ç’°å¢ƒåˆæœŸåŒ–
        self._setup_environment()
        
        # ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿
        self.model = PPO.load(model_path)
        print(f"âœ… ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿å®Œäº†: {model_path}")
    
    def _setup_environment(self):
        """ç’°å¢ƒã®åˆæœŸåŒ–"""
        print("ğŸ® ç’°å¢ƒåˆæœŸåŒ–ä¸­...")
        
        # é–‹ç™ºè€…ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿
        dev_profiles_path = self.config['env']['dev_profiles_path']
        with open(dev_profiles_path, 'r', encoding='utf-8') as f:
            self.dev_profiles = yaml.safe_load(f)
        
        # å­¦ç¿’ã«ä½¿ç”¨ã—ãŸãƒãƒƒã‚¯ãƒ­ã‚°ãƒ‡ãƒ¼ã‚¿ã‚‚èª­ã¿è¾¼ã¿
        backlog_path = self.config['env']['backlog_path']
        with open(backlog_path, 'r', encoding='utf-8') as f:
            training_backlog = json.load(f)
        
        # è¨­å®šã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®ä½œæˆ
        class DictObj:
            def __init__(self, d):
                for k, v in d.items():
                    if isinstance(v, dict):
                        setattr(self, k, DictObj(v))
                    else:
                        setattr(self, k, v)
            
            def get(self, key, default=None):
                return getattr(self, key, default)
        
        cfg = DictObj(self.config)
        
        # ç’°å¢ƒä½œæˆï¼ˆå­¦ç¿’æ™‚ã®ãƒ‡ãƒ¼ã‚¿æ§‹æˆã‚’ä½¿ç”¨ï¼‰
        self.env = SimpleTaskAssignmentEnv(
            cfg=cfg,
            backlog_data=training_backlog,
            dev_profiles_data=self.dev_profiles
        )
        
        print(f"   é–‹ç™ºè€…æ•°: {self.env.num_developers}")
        print(f"   ã‚¿ã‚¹ã‚¯æ•°: {len(self.env.tasks)}")
        print(f"   ç‰¹å¾´é‡æ¬¡å…ƒ: {self.env.observation_space.shape[0]}")
    
    def extract_actual_assignments(self):
        """å®Ÿéš›ã®é–‹ç™ºè€…å‰²ã‚Šå½“ã¦ã‚’æŠ½å‡º"""
        print("ğŸ“‹ å®Ÿéš›ã®å‰²ã‚Šå½“ã¦æŠ½å‡ºä¸­...")
        
        actual_assignments = {}
        actual_dev_profiles = {}
        assignment_stats = defaultdict(int)
        developer_stats = Counter()
        
        for task_data in self.test_data:
            task_id = task_data.get('id') or task_data.get('number')
            if not task_id:
                continue
            
            # å®Ÿéš›ã®æ‹…å½“è€…ã‚’æŠ½å‡º
            assignee = None
            
            if 'assignees' in task_data and task_data['assignees']:
                assignee = task_data['assignees'][0].get('login')
            elif 'events' in task_data:
                for event in task_data['events']:
                    if event.get('event') == 'assigned' and event.get('assignee'):
                        assignee = event['assignee'].get('login')
                        break
            
            if assignee:
                actual_assignments[task_id] = assignee
                assignment_stats['assigned'] += 1
                developer_stats[assignee] += 1
                
                # å®Ÿéš›ã®é–‹ç™ºè€…ã®ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆç°¡æ˜“ç‰ˆï¼‰ã‚’ä½œæˆ
                if assignee not in actual_dev_profiles:
                    actual_dev_profiles[assignee] = {
                        'name': assignee,
                        'task_count': 0,
                        'skill_areas': set(),
                        'collaboration_history': []
                    }
                
                actual_dev_profiles[assignee]['task_count'] += 1
                
                # ãƒ©ãƒ™ãƒ«ã‹ã‚‰ã‚¹ã‚­ãƒ«æ¨å®š
                for label in task_data.get('labels', []):
                    if isinstance(label, dict):
                        label_name = label.get('name', '')
                    else:
                        label_name = str(label)
                    actual_dev_profiles[assignee]['skill_areas'].add(label_name)
            else:
                assignment_stats['unassigned'] += 1
        
        print(f"   å‰²ã‚Šå½“ã¦æ¸ˆã¿: {assignment_stats['assigned']:,} ã‚¿ã‚¹ã‚¯")
        print(f"   æœªå‰²ã‚Šå½“ã¦: {assignment_stats['unassigned']:,} ã‚¿ã‚¹ã‚¯")
        print(f"   ãƒ¦ãƒ‹ãƒ¼ã‚¯é–‹ç™ºè€…: {len(developer_stats)} äºº")
        
        # ä¸Šä½é–‹ç™ºè€…è¡¨ç¤º
        top_devs = developer_stats.most_common(10)
        print("   ä¸Šä½é–‹ç™ºè€…:")
        for dev, count in top_devs:
            print(f"     {dev}: {count} ã‚¿ã‚¹ã‚¯")
        
        return actual_assignments, actual_dev_profiles
    
    def calculate_developer_similarity(self, actual_dev_profiles):
        """å®Ÿéš›ã®é–‹ç™ºè€…ã¨ç’°å¢ƒã®é–‹ç™ºè€…é–“ã®é¡ä¼¼åº¦ã‚’è¨ˆç®—"""
        print("ğŸ” é–‹ç™ºè€…é¡ä¼¼åº¦è¨ˆç®—ä¸­...")
        
        # å®Ÿéš›ã®é–‹ç™ºè€…ã®ã‚¹ã‚­ãƒ«ãƒ™ã‚¯ãƒˆãƒ«ã‚’ä½œæˆ
        all_skills = set()
        for profile in actual_dev_profiles.values():
            all_skills.update(profile['skill_areas'])
        
        skill_list = sorted(list(all_skills))
        print(f"   è­˜åˆ¥ã•ã‚ŒãŸã‚¹ã‚­ãƒ«: {len(skill_list)} ç¨®é¡")
        
        # å®Ÿéš›ã®é–‹ç™ºè€…ã®ã‚¹ã‚­ãƒ«ãƒ™ã‚¯ãƒˆãƒ«
        actual_vectors = {}
        for dev_name, profile in actual_dev_profiles.items():
            vector = np.zeros(len(skill_list))
            for i, skill in enumerate(skill_list):
                if skill in profile['skill_areas']:
                    vector[i] = 1.0
            actual_vectors[dev_name] = vector
        
        # ç’°å¢ƒã®é–‹ç™ºè€…ã®ã‚¹ã‚­ãƒ«ãƒ™ã‚¯ãƒˆãƒ«ï¼ˆãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ï¼‰
        env_vectors = {}
        for dev_name in self.env.developers:
            dev_profile = self.dev_profiles.get(dev_name, {})
            vector = np.zeros(len(skill_list))
            
            # ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ã‚¹ã‚­ãƒ«ã‚’æŠ½å‡º
            dev_skills = set()
            if 'skills' in dev_profile:
                dev_skills.update(dev_profile['skills'])
            if 'expertise' in dev_profile:
                dev_skills.update(dev_profile['expertise'])
            
            for i, skill in enumerate(skill_list):
                if skill in dev_skills:
                    vector[i] = 1.0
            
            env_vectors[dev_name] = vector
        
        # é¡ä¼¼åº¦ãƒãƒˆãƒªãƒƒã‚¯ã‚¹è¨ˆç®—
        similarity_matrix = {}
        for actual_dev, actual_vec in actual_vectors.items():
            similarities = {}
            for env_dev, env_vec in env_vectors.items():
                # ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦
                if np.linalg.norm(actual_vec) > 0 and np.linalg.norm(env_vec) > 0:
                    cosine_sim = np.dot(actual_vec, env_vec) / (
                        np.linalg.norm(actual_vec) * np.linalg.norm(env_vec)
                    )
                else:
                    cosine_sim = 0.0
                
                # ã‚¸ãƒ£ãƒƒã‚«ãƒ¼ãƒ‰é¡ä¼¼åº¦
                actual_skills = set(skill for i, skill in enumerate(skill_list) if actual_vec[i] > 0)
                env_skills = set(skill for i, skill in enumerate(skill_list) if env_vec[i] > 0)
                
                if len(actual_skills) > 0 or len(env_skills) > 0:
                    jaccard_sim = len(actual_skills & env_skills) / len(actual_skills | env_skills)
                else:
                    jaccard_sim = 0.0
                
                # è¤‡åˆé¡ä¼¼åº¦ï¼ˆã‚³ã‚µã‚¤ãƒ³ + ã‚¸ãƒ£ãƒƒã‚«ãƒ¼ãƒ‰ï¼‰
                combined_sim = (cosine_sim + jaccard_sim) / 2.0
                similarities[env_dev] = {
                    'cosine': cosine_sim,
                    'jaccard': jaccard_sim,
                    'combined': combined_sim
                }
            
            similarity_matrix[actual_dev] = similarities
        
        return similarity_matrix
    
    def predict_with_ranking(self, actual_assignments, actual_dev_profiles):
        """å„ã‚¿ã‚¹ã‚¯ã«å¯¾ã—ã¦é–‹ç™ºè€…ã‚’ãƒ©ãƒ³ã‚­ãƒ³ã‚°äºˆæ¸¬"""
        print("ğŸ¤– ãƒ©ãƒ³ã‚­ãƒ³ã‚°äºˆæ¸¬ä¸­...")
        
        predictions = {}
        ranking_results = {}
        
        # é¡ä¼¼åº¦ãƒãƒˆãƒªãƒƒã‚¯ã‚¹è¨ˆç®—
        similarity_matrix = self.calculate_developer_similarity(actual_dev_profiles)
        
        # å®Ÿéš›ã«å‰²ã‚Šå½“ã¦ã‚‰ã‚ŒãŸ2023å¹´ã®ã‚¿ã‚¹ã‚¯ã‚’å¯¾è±¡ã«äºˆæ¸¬
        test_tasks_with_assignments = []
        for task in self.test_data:
            task_id = task.get('id') or task.get('number')
            if task_id and task_id in actual_assignments:
                test_tasks_with_assignments.append(task)
        
        print(f"   äºˆæ¸¬å¯¾è±¡: {len(test_tasks_with_assignments)} ã‚¿ã‚¹ã‚¯ï¼ˆ2023å¹´ï¼‰")
        
        for task_data in test_tasks_with_assignments:
            try:
                task_obj = Task(task_data)
                task_id = task_obj.id if hasattr(task_obj, 'id') else task_data.get('id', task_data.get('number'))
                actual_dev = actual_assignments[task_id]
                
                # ãƒ€ãƒŸãƒ¼ç’°å¢ƒ
                dummy_env = type('DummyEnv', (), {
                    'backlog': self.env.tasks,
                    'dev_profiles': self.dev_profiles,
                    'assignments': {},
                    'dev_action_history': {}
                })()
                
                # å„ç’°å¢ƒé–‹ç™ºè€…ã«å¯¾ã™ã‚‹äºˆæ¸¬ç¢ºç‡ã‚’è¨ˆç®—
                dev_predictions = []
                for dev_idx, env_dev_name in enumerate(self.env.developers):
                    env_dev_profile = self.dev_profiles.get(env_dev_name, {})
                    dev_obj = {"name": env_dev_name, "profile": env_dev_profile}
                    
                    # ç‰¹å¾´é‡ã‚’æŠ½å‡º
                    features = self.env.feature_extractor.get_features(task_obj, dev_obj, dummy_env)
                    obs = features.astype(np.float32)
                    
                    # ãƒ¢ãƒ‡ãƒ«ã§äºˆæ¸¬ç¢ºç‡ã‚’å–å¾—
                    obs_tensor = torch.tensor(obs).unsqueeze(0).float()
                    with torch.no_grad():
                        logits = self.model.policy.mlp_extractor.policy_net(
                            self.model.policy.features_extractor(obs_tensor)
                        )
                        probs = torch.softmax(logits, dim=-1).numpy()[0]
                        dev_prob = probs[dev_idx] if dev_idx < len(probs) else 0.0
                    
                    dev_predictions.append((dev_idx, env_dev_name, dev_prob))
                
                # äºˆæ¸¬ç¢ºç‡ã§ã‚½ãƒ¼ãƒˆ
                dev_predictions.sort(key=lambda x: x[2], reverse=True)
                
                # ãƒ©ãƒ³ã‚­ãƒ³ã‚°é¡ä¼¼åº¦è¨ˆç®—
                ranking_scores = {}
                
                if actual_dev in similarity_matrix:
                    # å®Ÿéš›ã®é–‹ç™ºè€…ã«æœ€ã‚‚é¡ä¼¼ã—ãŸç’°å¢ƒé–‹ç™ºè€…ã®ãƒ©ãƒ³ã‚­ãƒ³ã‚°ä½ç½®
                    similarities = similarity_matrix[actual_dev]
                    
                    # æœ€ã‚‚é¡ä¼¼ã—ãŸé–‹ç™ºè€…ã‚’è¦‹ã¤ã‘ã‚‹
                    best_match = max(similarities.items(), key=lambda x: x[1]['combined'])
                    best_env_dev = best_match[0]
                    best_similarity = best_match[1]['combined']
                    
                    # ãƒ©ãƒ³ã‚­ãƒ³ã‚°ä½ç½®ã‚’è¦‹ã¤ã‘ã‚‹
                    for rank, (idx, env_dev, prob) in enumerate(dev_predictions):
                        if env_dev == best_env_dev:
                            ranking_position = rank + 1  # 1-based
                            ranking_scores['best_match_rank'] = ranking_position
                            ranking_scores['best_match_similarity'] = best_similarity
                            ranking_scores['best_match_dev'] = best_env_dev
                            break
                    
                    # ãƒˆãƒƒãƒ—5å†…ã®é¡ä¼¼åº¦é‡ã¿ä»˜ãã‚¹ã‚³ã‚¢
                    top5_score = 0.0
                    for rank, (idx, env_dev, prob) in enumerate(dev_predictions[:5]):
                        if env_dev in similarities:
                            weight = 1.0 / (rank + 1)  # ä½ç½®ã«ã‚ˆã‚‹é‡ã¿
                            similarity = similarities[env_dev]['combined']
                            top5_score += weight * similarity
                    
                    ranking_scores['top5_weighted_similarity'] = top5_score
                
                predictions[task_id] = dev_predictions[0][1]  # ãƒˆãƒƒãƒ—äºˆæ¸¬
                ranking_results[task_id] = {
                    'actual_developer': actual_dev,
                    'predicted_ranking': [(env_dev, prob) for _, env_dev, prob in dev_predictions[:10]],
                    'ranking_scores': ranking_scores
                }
                
            except Exception as e:
                print(f"âš ï¸ ã‚¿ã‚¹ã‚¯ {task_id} ã®äºˆæ¸¬ã§ã‚¨ãƒ©ãƒ¼: {e}")
                continue
        
        print(f"   äºˆæ¸¬å®Œäº†: {len(predictions)} ã‚¿ã‚¹ã‚¯")
        return predictions, ranking_results
    
    def calculate_ranking_metrics(self, ranking_results):
        """ãƒ©ãƒ³ã‚­ãƒ³ã‚°é¡ä¼¼åº¦ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’è¨ˆç®—"""
        print("ğŸ“Š ãƒ©ãƒ³ã‚­ãƒ³ã‚°é¡ä¼¼åº¦è©•ä¾¡ä¸­...")
        
        metrics = {}
        
        # ãƒ©ãƒ³ã‚­ãƒ³ã‚°ä½ç½®ã®çµ±è¨ˆ
        best_match_ranks = []
        best_match_similarities = []
        top5_weighted_similarities = []
        
        for task_id, result in ranking_results.items():
            scores = result.get('ranking_scores', {})
            
            if 'best_match_rank' in scores:
                best_match_ranks.append(scores['best_match_rank'])
                best_match_similarities.append(scores['best_match_similarity'])
            
            if 'top5_weighted_similarity' in scores:
                top5_weighted_similarities.append(scores['top5_weighted_similarity'])
        
        if best_match_ranks:
            metrics['avg_best_match_rank'] = np.mean(best_match_ranks)
            metrics['median_best_match_rank'] = np.median(best_match_ranks)
            metrics['top5_rank_ratio'] = sum(1 for r in best_match_ranks if r <= 5) / len(best_match_ranks)
            metrics['top10_rank_ratio'] = sum(1 for r in best_match_ranks if r <= 10) / len(best_match_ranks)
            
            print(f"   æœ€é¡ä¼¼é–‹ç™ºè€…ã®å¹³å‡ãƒ©ãƒ³ã‚¯: {metrics['avg_best_match_rank']:.2f}")
            print(f"   æœ€é¡ä¼¼é–‹ç™ºè€…ã®ä¸­å¤®å€¤ãƒ©ãƒ³ã‚¯: {metrics['median_best_match_rank']:.1f}")
            print(f"   Top-5ãƒ©ãƒ³ã‚¯ç‡: {metrics['top5_rank_ratio']:.3f}")
            print(f"   Top-10ãƒ©ãƒ³ã‚¯ç‡: {metrics['top10_rank_ratio']:.3f}")
        
        if best_match_similarities:
            metrics['avg_best_match_similarity'] = np.mean(best_match_similarities)
            metrics['similarity_std'] = np.std(best_match_similarities)
            
            print(f"   å¹³å‡æœ€é¡ä¼¼åº¦: {metrics['avg_best_match_similarity']:.3f} Â± {metrics['similarity_std']:.3f}")
        
        if top5_weighted_similarities:
            metrics['avg_top5_weighted_similarity'] = np.mean(top5_weighted_similarities)
            print(f"   Top-5é‡ã¿ä»˜ãé¡ä¼¼åº¦: {metrics['avg_top5_weighted_similarity']:.3f}")
        
        # ãƒ©ãƒ³ã‚­ãƒ³ã‚°é¡ä¼¼åº¦ç·åˆã‚¹ã‚³ã‚¢
        if best_match_ranks and best_match_similarities:
            # æ­£è¦åŒ–ã•ã‚ŒãŸãƒ©ãƒ³ã‚¯ã‚¹ã‚³ã‚¢ï¼ˆ1ä½ãªã‚‰1.0ã€ä¸‹ä½ã»ã©0ã«è¿‘ã¥ãï¼‰
            normalized_ranks = [1.0 - (r - 1) / 149 for r in best_match_ranks]  # 150äººä¸­
            combined_scores = [r * s for r, s in zip(normalized_ranks, best_match_similarities)]
            
            metrics['ranking_similarity_score'] = np.mean(combined_scores)
            print(f"   ãƒ©ãƒ³ã‚­ãƒ³ã‚°é¡ä¼¼åº¦ç·åˆã‚¹ã‚³ã‚¢: {metrics['ranking_similarity_score']:.3f}")
        
        return metrics
    
    def run_evaluation(self, output_dir="outputs"):
        """è©•ä¾¡å®Ÿè¡Œ"""
        print("ğŸš€ ãƒ©ãƒ³ã‚­ãƒ³ã‚°é¡ä¼¼åº¦è©•ä¾¡é–‹å§‹")
        print("=" * 70)
        
        # 1. å®Ÿéš›ã®å‰²ã‚Šå½“ã¦æŠ½å‡º
        actual_assignments, actual_dev_profiles = self.extract_actual_assignments()
        
        # 2. ãƒ©ãƒ³ã‚­ãƒ³ã‚°äºˆæ¸¬
        predictions, ranking_results = self.predict_with_ranking(actual_assignments, actual_dev_profiles)
        
        # 3. ãƒ©ãƒ³ã‚­ãƒ³ã‚°é¡ä¼¼åº¦ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨ˆç®—
        ranking_metrics = self.calculate_ranking_metrics(ranking_results)
        
        # 4. çµæœä¿å­˜
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_dir = Path(output_dir)
        output_dir.mkdir(exist_ok=True)
        
        # ãƒ¡ãƒˆãƒªã‚¯ã‚¹ä¿å­˜
        metrics_path = output_dir / f"ranking_similarity_metrics_{timestamp}.json"
        with open(metrics_path, 'w', encoding='utf-8') as f:
            json.dump(ranking_metrics, f, indent=2, ensure_ascii=False)
        
        # è©³ç´°çµæœä¿å­˜
        results_path = output_dir / f"ranking_similarity_results_{timestamp}.json"
        with open(results_path, 'w', encoding='utf-8') as f:
            # numpyé…åˆ—ã‚’å¯¾å¿œ
            def convert_numpy(obj):
                if isinstance(obj, np.ndarray):
                    return obj.tolist()
                elif isinstance(obj, (np.integer, np.floating)):
                    return float(obj)
                return obj
            
            json.dump(ranking_results, f, indent=2, ensure_ascii=False, default=convert_numpy)
        
        print(f"âœ… ãƒ¡ãƒˆãƒªã‚¯ã‚¹ä¿å­˜: {metrics_path}")
        print(f"âœ… è©³ç´°çµæœä¿å­˜: {results_path}")
        
        return ranking_metrics


def main():
    parser = argparse.ArgumentParser(description='ãƒ©ãƒ³ã‚­ãƒ³ã‚°é¡ä¼¼åº¦è©•ä¾¡')
    parser.add_argument('--config', default='configs/unified_rl.yaml',
                       help='è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹')
    parser.add_argument('--model', default='models/simple_unified_rl_agent.zip',
                       help='å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ãƒ‘ã‚¹')
    parser.add_argument('--test-data', default='data/backlog.json',
                       help='ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ãƒ‘ã‚¹ï¼ˆçµ±åˆæ¸ˆã¿ï¼‰')
    parser.add_argument('--output', default='outputs',
                       help='å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª')
    
    args = parser.parse_args()
    
    # è©•ä¾¡å®Ÿè¡Œ
    evaluator = RankingSimilarityEvaluator(args.config, args.model, args.test_data)
    metrics = evaluator.run_evaluation(args.output)
    
    if metrics:
        print("\nğŸ¯ ä¸»è¦çµæœ:")
        for key, value in metrics.items():
            if isinstance(value, (int, float)):
                print(f"   {key}: {value:.3f}")
    
    return 0


if __name__ == "__main__":
    exit(main())
