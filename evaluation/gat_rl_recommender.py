#!/usr/bin/env python3
"""
GATç‰¹å¾´é‡çµ±åˆå¼·åŒ–å­¦ç¿’æ¨è–¦ã‚·ã‚¹ãƒ†ãƒ 

- æ—¢å­˜ã®FeatureExtractorï¼ˆGAT+IRLé‡ã¿ï¼‰ã‚’æ´»ç”¨
- 62æ¬¡å…ƒã®é«˜æ¬¡å…ƒç‰¹å¾´é‡ã§å¼·åŒ–å­¦ç¿’
- ã‚·ãƒ³ãƒ—ãƒ«é¡ä¼¼åº¦ãƒ™ãƒ¼ã‚¹ã¨ã®æ€§èƒ½æ¯”è¼ƒ
"""

import argparse
import json
import pickle
import sys
from collections import Counter, defaultdict
from datetime import datetime
from pathlib import Path

import gymnasium as gym
import numpy as np
import pandas as pd
import torch
import yaml
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import StandardScaler
from stable_baselines3 import DQN, PPO
from stable_baselines3.common.callbacks import EvalCallback
from stable_baselines3.common.vec_env import DummyVecEnv

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
sys.path.append('/Users/kazuki-h/rl/kazoo')
sys.path.append('/Users/kazuki-h/rl/kazoo/src')

from src.kazoo.envs.task import Task
from src.kazoo.features.feature_extractor import FeatureExtractor


class GATEnhancedRLEnvironment(gym.Env):
    """
    GATç‰¹å¾´é‡çµ±åˆå¼·åŒ–å­¦ç¿’ç’°å¢ƒ
    - 62æ¬¡å…ƒGATç‰¹å¾´é‡ã‚’çŠ¶æ…‹è¡¨ç¾ã«ä½¿ç”¨
    - è¤‡é›‘ãªé–‹ç™ºè€…-ã‚¿ã‚¹ã‚¯é–¢ä¿‚ã‚’ãƒ¢ãƒ‡ãƒ«åŒ–
    - IRLé‡ã¿ã‚’å ±é…¬è¨ˆç®—ã«æ´»ç”¨
    """
    
    def __init__(self, config, training_data, dev_profiles):
        super().__init__()
        
        self.config = config
        self.training_data = training_data
        self.dev_profiles = dev_profiles
        
        # ç‰¹å¾´é‡æŠ½å‡ºå™¨ã®åˆæœŸåŒ–
        self.setup_feature_extractor()
        
        # é–‹ç™ºè€…ãƒªã‚¹ãƒˆ
        self.developers = list(dev_profiles.keys())
        self.num_developers = len(self.developers)
        
        # è¡Œå‹•ç©ºé–“: é–‹ç™ºè€…é¸æŠ
        self.action_space = gym.spaces.Discrete(self.num_developers)
        
        # è¦³æ¸¬ç©ºé–“: GATç‰¹å¾´é‡ (62æ¬¡å…ƒ)
        feature_dim = len(self.feature_extractor.feature_names)
        self.observation_space = gym.spaces.Box(
            low=-np.inf, high=np.inf,
            shape=(feature_dim,),
            dtype=np.float32
        )
        
        # å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ãƒšã‚¢ã‚’æŠ½å‡º
        self.training_pairs = self._extract_training_pairs()
        
        # ç¾åœ¨ã®ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰çŠ¶æ…‹
        self.current_pair = None
        self.episode_count = 0
        
        print(f"ğŸ¤– GATçµ±åˆå¼·åŒ–å­¦ç¿’ç’°å¢ƒåˆæœŸåŒ–å®Œäº†")
        print(f"   é–‹ç™ºè€…æ•°: {self.num_developers}")
        print(f"   ç‰¹å¾´é‡æ¬¡å…ƒ: {feature_dim} (GATå«ã‚€)")
        print(f"   å­¦ç¿’ãƒšã‚¢æ•°: {len(self.training_pairs)}")
        print(f"   è¡Œå‹•ç©ºé–“: {self.action_space}")
    
    def setup_feature_extractor(self):
        """GATç‰¹å¾´é‡æŠ½å‡ºå™¨ã®åˆæœŸåŒ–"""
        print("ğŸ”§ GATç‰¹å¾´é‡æŠ½å‡ºå™¨åˆæœŸåŒ–ä¸­...")
        
        # è¨­å®šã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®ä½œæˆ
        class DictObj:
            def __init__(self, d):
                for k, v in d.items():
                    if isinstance(v, dict):
                        setattr(self, k, DictObj(v))
                    else:
                        setattr(self, k, v)
            
            def get(self, key, default=None):
                return getattr(self, key, default)
        
        cfg = DictObj(self.config)
        self.feature_extractor = FeatureExtractor(cfg)
        
        # IRLé‡ã¿ã‚’èª­ã¿è¾¼ã¿
        self.irl_weights = self._load_irl_weights()
        
        print(f"   ç‰¹å¾´é‡æ¬¡å…ƒ: {len(self.feature_extractor.feature_names)}")
        print(f"   GATç‰¹å¾´é‡: {sum(1 for name in self.feature_extractor.feature_names if 'gat_' in name)}")
    
    def _load_irl_weights(self):
        """IRLå­¦ç¿’æ¸ˆã¿é‡ã¿ã‚’èª­ã¿è¾¼ã¿"""
        weights_path = self.config.get('irl', {}).get('output_weights_path')
        
        if weights_path and Path(weights_path).exists():
            try:
                weights = np.load(weights_path)
                print(f"âœ… IRLé‡ã¿èª­ã¿è¾¼ã¿: {weights_path} ({weights.shape})")
                return torch.tensor(weights, dtype=torch.float32)
            except Exception as e:
                print(f"âš ï¸ IRLé‡ã¿èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
        
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ãƒ©ãƒ³ãƒ€ãƒ é‡ã¿
        feature_dim = len(self.feature_extractor.feature_names)
        weights = torch.randn(feature_dim, dtype=torch.float32)
        print(f"âš ï¸ ãƒ©ãƒ³ãƒ€ãƒ é‡ã¿ã‚’ä½¿ç”¨: {weights.shape}")
        return weights
    
    def _extract_training_pairs(self):
        """å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ã‚¿ã‚¹ã‚¯-é–‹ç™ºè€…ãƒšã‚¢ã‚’æŠ½å‡º"""
        pairs = []
        
        for task_data in self.training_data:
            task_id = task_data.get('id') or task_data.get('number')
            if not task_id:
                continue
            
            # å®Ÿéš›ã®æ‹…å½“è€…ã‚’æŠ½å‡º
            assignee = None
            if 'assignees' in task_data and task_data['assignees']:
                assignee = task_data['assignees'][0].get('login')
            elif 'events' in task_data:
                for event in task_data['events']:
                    if event.get('event') == 'assigned' and event.get('assignee'):
                        assignee = event['assignee'].get('login')
                        break
            
            if assignee and assignee in self.developers:
                pairs.append({
                    'task_data': task_data,
                    'developer': assignee,
                    'task_id': task_id
                })
        
        return pairs
    
    def reset(self, seed=None, options=None):
        """ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã‚’ãƒªã‚»ãƒƒãƒˆ"""
        super().reset(seed=seed)
        
        # ãƒ©ãƒ³ãƒ€ãƒ ã«ãƒšã‚¢ã‚’é¸æŠ
        if self.training_pairs:
            self.current_pair = np.random.choice(self.training_pairs)
        else:
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            self.current_pair = {
                'task_data': self.training_data[0] if self.training_data else {},
                'developer': self.developers[0] if self.developers else 'unknown',
                'task_id': 'dummy'
            }
        
        # è¦³æ¸¬ã‚’ç”Ÿæˆ
        obs = self._get_observation()
        
        self.episode_count += 1
        return obs, {}
    
    def step(self, action):
        """ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚’å®Ÿè¡Œ"""
        if action >= self.num_developers:
            # ç„¡åŠ¹ãªã‚¢ã‚¯ã‚·ãƒ§ãƒ³
            reward = -10.0
            terminated = True
            obs = np.zeros(self.observation_space.shape[0], dtype=np.float32)
            return obs, reward, terminated, False, {}
        
        selected_dev = self.developers[action]
        actual_dev = self.current_pair['developer']
        
        # å ±é…¬è¨ˆç®—ï¼ˆGATç‰¹å¾´é‡ãƒ™ãƒ¼ã‚¹ï¼‰
        reward = self._calculate_gat_reward(selected_dev, actual_dev)
        
        # ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰çµ‚äº†
        terminated = True
        obs = np.zeros(self.observation_space.shape[0], dtype=np.float32)
        
        info = {
            'selected_dev': selected_dev,
            'actual_dev': actual_dev,
            'task_id': self.current_pair['task_id'],
            'correct': selected_dev == actual_dev,
            'reward_breakdown': getattr(self, 'last_reward_breakdown', {})
        }
        
        return obs, reward, terminated, False, info
    
    def _get_observation(self):
        """GATç‰¹å¾´é‡ã«ã‚ˆã‚‹è¦³æ¸¬ã‚’å–å¾—"""
        try:
            # ã‚¿ã‚¹ã‚¯ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆä½œæˆ
            task_obj = Task(self.current_pair['task_data'])
            
            # å®Ÿéš›ã®é–‹ç™ºè€…ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ
            actual_dev_name = self.current_pair['developer']
            dev_profile = self.dev_profiles.get(actual_dev_name, {})
            developer_obj = {"name": actual_dev_name, "profile": dev_profile}
            
            # ãƒ€ãƒŸãƒ¼ç’°å¢ƒä½œæˆ
            dummy_env = type('DummyEnv', (), {
                'backlog': [task_obj],
                'dev_profiles': self.dev_profiles,
                'assignments': {},
                'dev_action_history': {}
            })()
            
            # GATç‰¹å¾´é‡æŠ½å‡º
            features = self.feature_extractor.get_features(
                task_obj, developer_obj, dummy_env
            )
            
            return features.astype(np.float32)
            
        except Exception as e:
            print(f"âš ï¸ è¦³æ¸¬å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ã‚¼ãƒ­ãƒ™ã‚¯ãƒˆãƒ«
            return np.zeros(self.observation_space.shape[0], dtype=np.float32)
    
    def _calculate_gat_reward(self, selected_dev, actual_dev):
        """GATç‰¹å¾´é‡ã¨IRLé‡ã¿ã‚’æ´»ç”¨ã—ãŸå ±é…¬è¨ˆç®—"""
        total_reward = 0.0
        reward_breakdown = {}
        
        # 1. æ­£è§£å ±é…¬ (æœ€é‡è¦)
        if selected_dev == actual_dev:
            correct_reward = 10.0
            total_reward += correct_reward
            reward_breakdown['correct'] = correct_reward
        else:
            reward_breakdown['correct'] = 0.0
        
        # 2. GATç‰¹å¾´é‡ãƒ™ãƒ¼ã‚¹å ±é…¬
        try:
            # é¸æŠã•ã‚ŒãŸé–‹ç™ºè€…ã§ã®ç‰¹å¾´é‡æŠ½å‡º
            task_obj = Task(self.current_pair['task_data'])
            selected_dev_profile = self.dev_profiles.get(selected_dev, {})
            selected_dev_obj = {"name": selected_dev, "profile": selected_dev_profile}
            
            dummy_env = type('DummyEnv', (), {
                'backlog': [task_obj],
                'dev_profiles': self.dev_profiles,
                'assignments': {},
                'dev_action_history': {}
            })()
            
            # GATç‰¹å¾´é‡æŠ½å‡º
            features = self.feature_extractor.get_features(
                task_obj, selected_dev_obj, dummy_env
            )
            features_tensor = torch.tensor(features, dtype=torch.float32)
            
            # IRLé‡ã¿ã¨ã®å†…ç©ã§é©åˆåº¦è¨ˆç®—
            compatibility_score = torch.dot(self.irl_weights, features_tensor).item()
            
            # æ­£è¦åŒ–ã—ã¦å ±é…¬ã«å¤‰æ›
            gat_reward = np.tanh(compatibility_score) * 5.0  # -5.0 to +5.0
            total_reward += gat_reward
            reward_breakdown['gat_compatibility'] = gat_reward
            
            # 3. GATç‰¹å®šç‰¹å¾´é‡ãƒœãƒ¼ãƒŠã‚¹
            gat_indices = [i for i, name in enumerate(self.feature_extractor.feature_names) 
                          if 'gat_' in name]
            
            if gat_indices:
                gat_features = features[gat_indices]
                
                # GATé¡ä¼¼åº¦ç‰¹å¾´é‡
                gat_similarity_idx = [i for i, name in enumerate(self.feature_extractor.feature_names) 
                                    if name == 'gat_similarity']
                if gat_similarity_idx:
                    similarity_score = features[gat_similarity_idx[0]]
                    similarity_reward = similarity_score * 3.0
                    total_reward += similarity_reward
                    reward_breakdown['gat_similarity'] = similarity_reward
                
                # GATå°‚é–€æ€§ç‰¹å¾´é‡
                expertise_idx = [i for i, name in enumerate(self.feature_extractor.feature_names) 
                               if name == 'gat_dev_expertise']
                if expertise_idx:
                    expertise_score = features[expertise_idx[0]]
                    expertise_reward = expertise_score * 2.0
                    total_reward += expertise_reward
                    reward_breakdown['gat_expertise'] = expertise_reward
                
                # GATå”åŠ›å¼·åº¦ç‰¹å¾´é‡
                collaboration_idx = [i for i, name in enumerate(self.feature_extractor.feature_names) 
                                   if name == 'gat_collaboration_strength']
                if collaboration_idx:
                    collab_score = features[collaboration_idx[0]]
                    collab_reward = collab_score * 2.0
                    total_reward += collab_reward
                    reward_breakdown['gat_collaboration'] = collab_reward
        
        except Exception as e:
            print(f"âš ï¸ GATå ±é…¬è¨ˆç®—ã‚¨ãƒ©ãƒ¼: {e}")
            reward_breakdown['gat_compatibility'] = 0.0
        
        # 4. é–‹ç™ºè€…çµŒé¨“å ±é…¬
        dev_profile = self.dev_profiles.get(selected_dev, {})
        if 'expertise' in dev_profile:
            expertise_count = len(dev_profile['expertise'])
            exp_reward = min(expertise_count * 0.1, 1.0)  # æœ€å¤§1.0
            total_reward += exp_reward
            reward_breakdown['dev_experience'] = exp_reward
        
        # å ±é…¬è©³ç´°ã‚’ä¿å­˜
        self.last_reward_breakdown = reward_breakdown
        
        return total_reward


class GATRLRecommender:
    """GATç‰¹å¾´é‡çµ±åˆå¼·åŒ–å­¦ç¿’æ¨è–¦ã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self, config_path):
        self.config_path = config_path
        
        with open(config_path, 'r', encoding='utf-8') as f:
            self.config = yaml.safe_load(f)
        
        print("ğŸš€ GATçµ±åˆå¼·åŒ–å­¦ç¿’æ¨è–¦ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å®Œäº†")
    
    def load_data(self, data_path):
        """ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã‚“ã§æ™‚ç³»åˆ—åˆ†å‰²"""
        print("ğŸ“Š ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ä¸­...")
        
        with open(data_path, 'r', encoding='utf-8') as f:
            all_data = json.load(f)
        
        # æ™‚ç³»åˆ—åˆ†å‰²
        training_data = []  # 2022å¹´ä»¥å‰
        test_data = []      # 2023å¹´
        
        for task in all_data:
            created_at = task.get('created_at', '')
            if created_at.startswith('2022'):
                training_data.append(task)
            elif created_at.startswith('2023'):
                test_data.append(task)
        
        print(f"   å­¦ç¿’ãƒ‡ãƒ¼ã‚¿: {len(training_data):,} ã‚¿ã‚¹ã‚¯ (2022å¹´)")
        print(f"   ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿: {len(test_data):,} ã‚¿ã‚¹ã‚¯ (2023å¹´)")
        
        return training_data, test_data
    
    def load_dev_profiles(self):
        """é–‹ç™ºè€…ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿"""
        dev_profiles_path = self.config['env']['dev_profiles_path']
        with open(dev_profiles_path, 'r', encoding='utf-8') as f:
            dev_profiles = yaml.safe_load(f)
        
        print(f"ğŸ“‹ é–‹ç™ºè€…ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿: {len(dev_profiles)} äºº")
        return dev_profiles
    
    def train_rl_agent(self, training_data, dev_profiles, total_timesteps=100000):
        """GATç‰¹å¾´é‡ã§å¼·åŒ–å­¦ç¿’ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’è¨“ç·´"""
        print("ğŸ“ GATçµ±åˆå¼·åŒ–å­¦ç¿’ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆè¨“ç·´é–‹å§‹...")
        
        # ç’°å¢ƒä½œæˆ
        def make_env():
            return GATEnhancedRLEnvironment(self.config, training_data, dev_profiles)
        
        env = DummyVecEnv([make_env])
        
        # PPOã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆï¼ˆé«˜æ¬¡å…ƒç‰¹å¾´é‡å¯¾å¿œï¼‰
        self.rl_agent = PPO(
            "MlpPolicy",
            env,
            verbose=1,
            learning_rate=3e-4,
            n_steps=2048,
            batch_size=64,
            n_epochs=10,
            gamma=0.99,
            gae_lambda=0.95,
            clip_range=0.2,
            ent_coef=0.01,  # æ¢ç´¢ä¿ƒé€²
            vf_coef=0.5,
            max_grad_norm=0.5,
            device="auto",
            policy_kwargs=dict(
                net_arch=[256, 256, 128],  # å¤§ããªãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯
                activation_fn=torch.nn.ReLU
            )
        )
        
        # è©•ä¾¡ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯
        eval_env = DummyVecEnv([make_env])
        eval_callback = EvalCallback(
            eval_env,
            best_model_save_path="./models/gat_rl_best/",
            log_path="./logs/gat_rl_eval/",
            eval_freq=5000,
            deterministic=True,
            render=False,
            verbose=1
        )
        
        # è¨“ç·´å®Ÿè¡Œ
        print(f"   è¨“ç·´ã‚¹ãƒ†ãƒƒãƒ—: {total_timesteps:,}")
        print(f"   æ¨å®šæ™‚é–“: {total_timesteps / 1000:.1f}åˆ†")
        
        self.rl_agent.learn(
            total_timesteps=total_timesteps,
            callback=eval_callback,
            progress_bar=False  # ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼ç„¡åŠ¹åŒ–
        )
        
        print("âœ… GATçµ±åˆå¼·åŒ–å­¦ç¿’ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆè¨“ç·´å®Œäº†")
    
    def predict_with_gat_rl(self, test_data, dev_profiles):
        """GATç‰¹å¾´é‡å¼·åŒ–å­¦ç¿’ã§äºˆæ¸¬"""
        print("ğŸ¤– GATå¼·åŒ–å­¦ç¿’äºˆæ¸¬å®Ÿè¡Œä¸­...")
        
        predictions = {}
        prediction_scores = {}
        
        # ãƒ†ã‚¹ãƒˆç’°å¢ƒ
        test_env = GATEnhancedRLEnvironment(self.config, test_data, dev_profiles)
        
        # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®å®Ÿéš›ã®å‰²ã‚Šå½“ã¦ã‚’æŠ½å‡º
        test_assignments = {}
        for task_data in test_data:
            task_id = task_data.get('id') or task_data.get('number')
            if not task_id:
                continue
                
            assignee = None
            if 'assignees' in task_data and task_data['assignees']:
                assignee = task_data['assignees'][0].get('login')
            elif 'events' in task_data:
                for event in task_data['events']:
                    if event.get('event') == 'assigned' and event.get('assignee'):
                        assignee = event['assignee'].get('login')
                        break
            
            if assignee:
                test_assignments[task_id] = assignee
        
        print(f"   äºˆæ¸¬å¯¾è±¡: {len(test_assignments)} ã‚¿ã‚¹ã‚¯")
        
        prediction_count = 0
        
        for task_data in test_data:
            task_id = task_data.get('id') or task_data.get('number')
            if task_id not in test_assignments:
                continue
            
            try:
                # ãƒ†ã‚¹ãƒˆç’°å¢ƒã§ã‚¿ã‚¹ã‚¯ã‚’è¨­å®š
                test_env.current_pair = {
                    'task_data': task_data,
                    'developer': test_assignments[task_id],
                    'task_id': task_id
                }
                
                # è¦³æ¸¬å–å¾—
                obs = test_env._get_observation()
                
                # å¼·åŒ–å­¦ç¿’ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã§äºˆæ¸¬
                action, _ = self.rl_agent.predict(obs, deterministic=True)
                
                if action < len(test_env.developers):
                    predicted_dev = test_env.developers[action]
                    
                    predictions[task_id] = predicted_dev
                    prediction_scores[task_id] = {
                        'predicted_dev': predicted_dev,
                        'action': int(action),
                        'method': 'gat_reinforcement_learning'
                    }
                    
                    prediction_count += 1
                    
                    if prediction_count % 50 == 0:
                        print(f"   é€²æ—: {prediction_count}/{len(test_assignments)}")
            
            except Exception as e:
                print(f"âš ï¸ ã‚¿ã‚¹ã‚¯ {task_id} ã®GAT-RLäºˆæ¸¬ã‚¨ãƒ©ãƒ¼: {e}")
                continue
        
        print(f"   GAT-RLäºˆæ¸¬å®Œäº†: {len(predictions)} ã‚¿ã‚¹ã‚¯")
        return predictions, prediction_scores, test_assignments
    
    def compare_with_baseline(self, gat_predictions, test_assignments, baseline_path=None):
        """ã‚·ãƒ³ãƒ—ãƒ«é¡ä¼¼åº¦ãƒ™ãƒ¼ã‚¹ã¨ã®æ¯”è¼ƒ"""
        print("ğŸ“Š ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³æ¯”è¼ƒå®Ÿè¡Œä¸­...")
        
        # GAT-RLè©•ä¾¡
        gat_common_tasks = set(gat_predictions.keys()) & set(test_assignments.keys())
        gat_correct = sum(1 for task_id in gat_common_tasks 
                         if gat_predictions[task_id] == test_assignments[task_id])
        gat_accuracy = gat_correct / len(gat_common_tasks) if gat_common_tasks else 0.0
        
        print(f"ğŸ¤– GATå¼·åŒ–å­¦ç¿’ã‚·ã‚¹ãƒ†ãƒ :")
        print(f"   ç²¾åº¦: {gat_accuracy:.3f} ({gat_correct}/{len(gat_common_tasks)})")
        
        # ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³æ¯”è¼ƒï¼ˆå¯èƒ½ã§ã‚ã‚Œã°ï¼‰
        if baseline_path and Path(baseline_path).exists():
            baseline_results = pd.read_csv(baseline_path)
            baseline_accuracy = baseline_results['correct'].mean()
            
            print(f"ğŸ“ ã‚·ãƒ³ãƒ—ãƒ«é¡ä¼¼åº¦ãƒ™ãƒ¼ã‚¹:")
            print(f"   ç²¾åº¦: {baseline_accuracy:.3f}")
            
            improvement = gat_accuracy - baseline_accuracy
            print(f"ğŸ“ˆ æ”¹å–„åº¦: {improvement:+.3f} ({improvement/baseline_accuracy*100:+.1f}%)")
        
        return {
            'gat_rl_accuracy': gat_accuracy,
            'gat_rl_correct': gat_correct,
            'gat_rl_total': len(gat_common_tasks)
        }
    
    def save_gat_rl_model(self, output_dir="models"):
        """GATçµ±åˆå¼·åŒ–å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜"""
        output_dir = Path(output_dir)
        output_dir.mkdir(exist_ok=True)
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # RLã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆä¿å­˜
        rl_model_path = output_dir / f"gat_rl_recommender_{timestamp}.zip"
        self.rl_agent.save(rl_model_path)
        
        print(f"âœ… GATçµ±åˆRLãƒ¢ãƒ‡ãƒ«ä¿å­˜: {rl_model_path}")
        return rl_model_path
    
    def run_full_pipeline(self, data_path, baseline_path=None, total_timesteps=100000):
        """å®Œå…¨ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè¡Œ"""
        print("ğŸš€ GATçµ±åˆå¼·åŒ–å­¦ç¿’æ¨è–¦ã‚·ã‚¹ãƒ†ãƒ å®Ÿè¡Œé–‹å§‹")
        print("=" * 70)
        
        # 1. ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
        training_data, test_data = self.load_data(data_path)
        
        # 2. é–‹ç™ºè€…ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿
        dev_profiles = self.load_dev_profiles()
        
        # 3. å¼·åŒ–å­¦ç¿’ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆè¨“ç·´
        self.train_rl_agent(training_data, dev_profiles, total_timesteps)
        
        # 4. äºˆæ¸¬å®Ÿè¡Œ
        predictions, scores, test_assignments = self.predict_with_gat_rl(test_data, dev_profiles)
        
        # 5. ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã¨ã®æ¯”è¼ƒ
        metrics = self.compare_with_baseline(predictions, test_assignments, baseline_path)
        
        # 6. ãƒ¢ãƒ‡ãƒ«ä¿å­˜
        self.save_gat_rl_model()
        
        print("âœ… GATçµ±åˆå¼·åŒ–å­¦ç¿’æ¨è–¦ã‚·ã‚¹ãƒ†ãƒ å®Œäº†")
        return metrics


def main():
    parser = argparse.ArgumentParser(description='GATçµ±åˆå¼·åŒ–å­¦ç¿’æ¨è–¦ã‚·ã‚¹ãƒ†ãƒ ')
    parser.add_argument('--config', default='configs/unified_rl.yaml',
                       help='è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹')
    parser.add_argument('--data', default='data/backlog.json',
                       help='çµ±åˆãƒ‡ãƒ¼ã‚¿ãƒ‘ã‚¹')
    parser.add_argument('--baseline', 
                       help='ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³çµæœCSVãƒ‘ã‚¹')
    parser.add_argument('--timesteps', type=int, default=100000,
                       help='è¨“ç·´ã‚¹ãƒ†ãƒƒãƒ—æ•°')
    parser.add_argument('--output', default='outputs',
                       help='å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª')
    
    args = parser.parse_args()
    
    # GATçµ±åˆå¼·åŒ–å­¦ç¿’ã‚·ã‚¹ãƒ†ãƒ å®Ÿè¡Œ
    recommender = GATRLRecommender(args.config)
    metrics = recommender.run_full_pipeline(
        args.data, args.baseline, args.timesteps
    )
    
    print("\nğŸ¯ æœ€çµ‚çµæœ:")
    for key, value in metrics.items():
        if isinstance(value, (int, float)):
            print(f"   {key}: {value:.3f}")
    
    return 0


if __name__ == "__main__":
    exit(main())
