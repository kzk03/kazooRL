#!/usr/bin/env python3
"""
ndeloofãŒæ¨è–¦ã‹ã‚‰é™¤å¤–ã•ã‚ŒãŸç†ç”±ã®è©³ç´°èª¿æŸ»
æœ€é«˜è²¢çŒ®è€…ãŒæ¨è–¦ã•ã‚Œãªã„åŸå› ã‚’è§£æ˜
"""

import json
import os
from collections import Counter, defaultdict
from typing import Dict, List, Tuple

import numpy as np
import torch
import torch.nn as nn
from tqdm import tqdm


class PPOPolicyNetwork(nn.Module):
    """PPOãƒãƒªã‚·ãƒ¼ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®å†æ§‹ç¯‰"""

    def __init__(self, input_dim=64, hidden_dim=128):
        super(PPOPolicyNetwork, self).__init__()

        self.feature_extractor = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.LayerNorm(hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(hidden_dim, hidden_dim),
            nn.LayerNorm(hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.1),
        )

        self.actor = nn.Sequential(
            nn.Linear(hidden_dim, 64),
            nn.LayerNorm(64),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(64, 16),
            nn.Softmax(dim=-1),
        )

        self.critic = nn.Sequential(
            nn.Linear(hidden_dim, 64),
            nn.LayerNorm(64),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(64, 1),
        )

    def forward(self, x):
        features = self.feature_extractor(x)
        action_probs = self.actor(features)
        value = self.critic(features)
        return action_probs, value

    def get_action_score(self, x):
        with torch.no_grad():
            action_probs, value = self.forward(x)
            score = torch.max(action_probs).item()
            return score


def is_bot(username: str) -> bool:
    """ãƒ¦ãƒ¼ã‚¶ãƒ¼åãŒBotã‹ã©ã†ã‹åˆ¤å®š"""
    bot_indicators = [
        "[bot]",
        "bot",
        "dependabot",
        "renovate",
        "greenkeeper",
        "codecov",
        "travis",
        "circleci",
        "github-actions",
        "automated",
    ]
    username_lower = username.lower()
    return any(indicator in username_lower for indicator in bot_indicators)


def load_test_data_with_bot_filtering(
    test_data_path: str,
) -> Tuple[List[Dict], List[str]]:
    """ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ã€Botã‚’é™¤å»"""
    with open(test_data_path, "r", encoding="utf-8") as f:
        test_data = json.load(f)

    filtered_tasks = []
    ground_truth_authors = []

    for task in test_data:
        author = task.get("author", {})
        if author and isinstance(author, dict):
            author_login = author.get("login", "")
            if author_login and not is_bot(author_login):
                filtered_tasks.append(task)
                ground_truth_authors.append(author_login)

    return filtered_tasks, ground_truth_authors


def load_sample_models(
    model_dir: str, actual_authors: List[str], max_models: int = 50
) -> Dict[str, PPOPolicyNetwork]:
    """ã‚µãƒ³ãƒ—ãƒ«ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿"""
    model_files = [f for f in os.listdir(model_dir) if f.endswith(".pth")]
    all_trained_agents = [
        f.replace("agent_", "").replace(".pth", "") for f in model_files
    ]

    human_trained_agents = [agent for agent in all_trained_agents if not is_bot(agent)]
    actual_set = set(actual_authors)
    human_set = set(human_trained_agents)
    overlapping_agents = actual_set.intersection(human_set)

    loaded_models = {}

    for i, agent_name in enumerate(overlapping_agents):
        if i >= max_models:
            break

        model_path = os.path.join(model_dir, f"agent_{agent_name}.pth")

        try:
            model_data = torch.load(model_path, map_location="cpu", weights_only=False)
            policy_network = PPOPolicyNetwork()
            policy_network.load_state_dict(model_data["policy_state_dict"])
            policy_network.eval()
            loaded_models[agent_name] = policy_network
        except Exception as e:
            continue

    return loaded_models


def extract_task_features_for_model(task: Dict) -> torch.Tensor:
    """ãƒ¢ãƒ‡ãƒ«ç”¨ã®ã‚¿ã‚¹ã‚¯ç‰¹å¾´é‡ã‚’æŠ½å‡ºï¼ˆ64æ¬¡å…ƒï¼‰"""
    features = []

    title = task.get("title", "") or ""
    body = task.get("body", "") or ""
    labels = task.get("labels", [])

    # åŸºæœ¬ç‰¹å¾´é‡ï¼ˆ10æ¬¡å…ƒï¼‰
    basic_features = [
        len(title),
        len(body),
        len(title.split()),
        len(body.split()),
        len(labels),
        title.count("?"),
        title.count("!"),
        body.count("\n"),
        len(set(title.lower().split())),
        1 if any(kw in title.lower() for kw in ["bug", "fix", "error"]) else 0,
    ]
    features.extend(basic_features)

    # æ—¥ä»˜ç‰¹å¾´é‡ï¼ˆ3æ¬¡å…ƒï¼‰
    created_at = task.get("created_at", "")
    if created_at:
        try:
            date_parts = created_at.split("T")[0].split("-")
            year, month, day = (
                int(date_parts[0]),
                int(date_parts[1]),
                int(date_parts[2]),
            )
            features.extend([year - 2020, month, day])
        except:
            features.extend([0, 0, 0])
    else:
        features.extend([0, 0, 0])

    # ãƒ©ãƒ™ãƒ«ç‰¹å¾´é‡ï¼ˆ10æ¬¡å…ƒï¼‰
    label_text = " ".join(
        [
            str(label) if not isinstance(label, dict) else label.get("name", "")
            for label in labels
        ]
    ).lower()

    important_keywords = [
        "bug",
        "feature",
        "enhancement",
        "documentation",
        "help",
        "question",
        "performance",
        "security",
        "ui",
        "api",
    ]
    for keyword in important_keywords:
        features.append(1 if keyword in label_text else 0)

    # æ®‹ã‚Šã‚’ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°
    while len(features) < 64:
        features.append(0.0)
    features = features[:64]

    # æ­£è¦åŒ–
    features = np.array(features, dtype=np.float32)
    features = (features - np.mean(features)) / (np.std(features) + 1e-8)

    return torch.tensor(features, dtype=torch.float32)


def investigate_ndeloof_exclusion():
    """ndeloofãŒé™¤å¤–ã•ã‚ŒãŸç†ç”±ã‚’è©³ç´°èª¿æŸ»"""
    print("ğŸ” ndeloofé™¤å¤–ã®è©³ç´°èª¿æŸ»")
    print("=" * 60)

    # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    tasks, ground_truth = load_test_data_with_bot_filtering(
        "data/backlog_test_2023.json"
    )
    trained_models = load_sample_models(
        "models/improved_rl/final_models", ground_truth, 50
    )

    # è²¢çŒ®é‡åˆ†æ
    author_contribution = Counter(ground_truth)

    print(f"## 1. åŸºæœ¬æƒ…å ±ç¢ºèª")
    print("-" * 40)
    print(f"   ndeloofã®è²¢çŒ®é‡: {author_contribution.get('ndeloof', 0)}ã‚¿ã‚¹ã‚¯")
    print(
        f"   ndeloofãŒè¨“ç·´æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã«å«ã¾ã‚Œã‚‹ã‹: {'âœ…' if 'ndeloof' in trained_models else 'âŒ'}"
    )

    if "ndeloof" not in trained_models:
        print(f"\nğŸš¨ é‡å¤§ãªç™ºè¦‹: ndeloofãŒè¨“ç·´æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã«å«ã¾ã‚Œã¦ã„ã¾ã›ã‚“ï¼")

        # åˆ©ç”¨å¯èƒ½ãªãƒ¢ãƒ‡ãƒ«ã‚’ç¢ºèª
        print(f"\n   åˆ©ç”¨å¯èƒ½ãªãƒ¢ãƒ‡ãƒ«ä¸€è¦§:")
        for i, agent in enumerate(sorted(trained_models.keys()), 1):
            contribution = author_contribution.get(agent, 0)
            print(f"     {i:2d}. {agent}: {contribution}ã‚¿ã‚¹ã‚¯")

        # ndeloofã®ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã™ã‚‹ã‹ç¢ºèª
        model_dir = "models/improved_rl/final_models"
        ndeloof_model_path = os.path.join(model_dir, "agent_ndeloof.pth")

        print(f"\n   ndeloofã®ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ç¢ºèª:")
        print(f"     ãƒ‘ã‚¹: {ndeloof_model_path}")
        print(f"     å­˜åœ¨: {'âœ…' if os.path.exists(ndeloof_model_path) else 'âŒ'}")

        if os.path.exists(ndeloof_model_path):
            print(f"\n   ğŸ” ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã¯å­˜åœ¨ã™ã‚‹ãŒèª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¦ã„ã‚‹å¯èƒ½æ€§")

            # ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ã‚’è©¦è¡Œ
            try:
                model_data = torch.load(
                    ndeloof_model_path, map_location="cpu", weights_only=False
                )
                print(f"     ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿: âœ… æˆåŠŸ")
                print(f"     ãƒ¢ãƒ‡ãƒ«ã‚­ãƒ¼: {list(model_data.keys())}")

                # ãƒãƒªã‚·ãƒ¼ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯å†æ§‹ç¯‰ã‚’è©¦è¡Œ
                try:
                    policy_network = PPOPolicyNetwork()
                    policy_network.load_state_dict(model_data["policy_state_dict"])
                    policy_network.eval()
                    print(f"     ãƒãƒªã‚·ãƒ¼ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ§‹ç¯‰: âœ… æˆåŠŸ")

                    # æ‰‹å‹•ã§ãƒ¢ãƒ‡ãƒ«ã‚’è¿½åŠ 
                    trained_models["ndeloof"] = policy_network
                    print(f"     æ‰‹å‹•è¿½åŠ : âœ… å®Œäº†")

                except Exception as e:
                    print(f"     ãƒãƒªã‚·ãƒ¼ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ§‹ç¯‰: âŒ å¤±æ•— - {e}")

            except Exception as e:
                print(f"     ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿: âŒ å¤±æ•— - {e}")

        else:
            print(f"\n   ğŸš¨ ndeloofã®ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ã¾ã›ã‚“")

            # å…¨ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç¢ºèª
            all_model_files = [f for f in os.listdir(model_dir) if f.endswith(".pth")]
            print(f"     ç·ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {len(all_model_files)}")

            # ndeloofã«é¡ä¼¼ã™ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«åã‚’æ¤œç´¢
            ndeloof_like = [f for f in all_model_files if "ndeloof" in f.lower()]
            print(f"     ndeloofé¡ä¼¼ãƒ•ã‚¡ã‚¤ãƒ«: {ndeloof_like}")

    # ndeloofãŒå«ã¾ã‚Œã¦ã„ã‚‹å ´åˆã®åˆ†æ
    if "ndeloof" in trained_models:
        print(f"\n## 2. ndeloofã®ã‚¹ã‚³ã‚¢åˆ†æ")
        print("-" * 40)

        # ndeloofã®ã‚¿ã‚¹ã‚¯ã§ã®ã‚¹ã‚³ã‚¢åˆ†æ
        ndeloof_tasks = [
            (i, task)
            for i, (task, author) in enumerate(zip(tasks, ground_truth))
            if author == "ndeloof"
        ]

        print(f"   ndeloofã®ã‚¿ã‚¹ã‚¯æ•°: {len(ndeloof_tasks)}")

        if ndeloof_tasks:
            sample_tasks = ndeloof_tasks[:5]  # æœ€åˆã®5ã¤ã®ã‚¿ã‚¹ã‚¯ã‚’åˆ†æ

            for i, (task_idx, task) in enumerate(sample_tasks, 1):
                print(f"\n   ã‚¿ã‚¹ã‚¯{i}: {task.get('title', '')[:50]}...")

                try:
                    task_features = extract_task_features_for_model(task)

                    # å…¨ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—
                    agent_scores = {}
                    for agent_name, model in trained_models.items():
                        try:
                            score = model.get_action_score(task_features)
                            agent_scores[agent_name] = score
                        except:
                            agent_scores[agent_name] = 0.0

                    # ã‚¹ã‚³ã‚¢é †ã«ã‚½ãƒ¼ãƒˆ
                    sorted_agents = sorted(
                        agent_scores.items(), key=lambda x: x[1], reverse=True
                    )

                    # ndeloofã®é †ä½ã‚’ç¢ºèª
                    ndeloof_rank = None
                    ndeloof_score = agent_scores.get("ndeloof", 0.0)

                    for rank, (agent, score) in enumerate(sorted_agents, 1):
                        if agent == "ndeloof":
                            ndeloof_rank = rank
                            break

                    print(f"     ndeloofã®ã‚¹ã‚³ã‚¢: {ndeloof_score:.3f}")
                    print(f"     ndeloofã®é †ä½: {ndeloof_rank}/{len(sorted_agents)}")

                    # ä¸Šä½5ä½ã‚’è¡¨ç¤º
                    print(f"     ä¸Šä½5ä½:")
                    for rank, (agent, score) in enumerate(sorted_agents[:5], 1):
                        marker = "ğŸ‘‘" if agent == "ndeloof" else "  "
                        contribution = author_contribution.get(agent, 0)
                        print(
                            f"       {rank}. {marker} {agent}: {score:.3f} ({contribution}ã‚¿ã‚¹ã‚¯)"
                        )

                    # è²¢çŒ®é‡ãƒãƒ©ãƒ³ã‚¹æ¨è–¦ã§ã®çµæœ
                    print(f"\n     è²¢çŒ®é‡ãƒãƒ©ãƒ³ã‚¹æ¨è–¦çµæœ:")

                    # é«˜è²¢çŒ®è€…ã‚«ãƒ†ã‚´ãƒª
                    high_contributors = set()
                    medium_contributors = set()
                    low_contributors = set()

                    for author, count in author_contribution.items():
                        if author in trained_models:
                            if count >= 50:
                                high_contributors.add(author)
                            elif count >= 10:
                                medium_contributors.add(author)
                            else:
                                low_contributors.add(author)

                    print(
                        f"       ndeloofã®ã‚«ãƒ†ã‚´ãƒª: {'é«˜è²¢çŒ®è€…' if 'ndeloof' in high_contributors else 'ä¸­è²¢çŒ®è€…' if 'ndeloof' in medium_contributors else 'ä½è²¢çŒ®è€…'}"
                    )

                    # å„ã‚«ãƒ†ã‚´ãƒªã§ã®é †ä½
                    high_candidates = [
                        (agent, score)
                        for agent, score in agent_scores.items()
                        if agent in high_contributors
                    ]
                    high_candidates.sort(key=lambda x: x[1], reverse=True)

                    ndeloof_high_rank = None
                    for rank, (agent, score) in enumerate(high_candidates, 1):
                        if agent == "ndeloof":
                            ndeloof_high_rank = rank
                            break

                    print(
                        f"       é«˜è²¢çŒ®è€…å†…ã§ã®é †ä½: {ndeloof_high_rank}/{len(high_candidates)}"
                    )
                    print(
                        f"       é«˜è²¢çŒ®è€…ä¸Šä½2ä½: {[agent for agent, _ in high_candidates[:2]]}"
                    )

                    # ãªãœé™¤å¤–ã•ã‚ŒãŸã‹ã®åˆ†æ
                    if ndeloof_high_rank and ndeloof_high_rank > 2:
                        print(
                            f"       ğŸ¯ é™¤å¤–ç†ç”±: é«˜è²¢çŒ®è€…å†…ã§{ndeloof_high_rank}ä½ã®ãŸã‚ã€ä¸Šä½2ä½ã«å…¥ã‚‰ãªã‹ã£ãŸ"
                        )

                except Exception as e:
                    print(f"     ã‚¨ãƒ©ãƒ¼: {e}")

    # æ ¹æœ¬åŸå› ã®åˆ†æ
    print(f"\n## 3. æ ¹æœ¬åŸå› ã®åˆ†æ")
    print("-" * 40)

    if "ndeloof" not in trained_models:
        print("### ğŸš¨ ä¸»è¦åŸå› : ndeloofãŒè¨“ç·´æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã«å«ã¾ã‚Œã¦ã„ãªã„")
        print("   è€ƒãˆã‚‰ã‚Œã‚‹ç†ç”±:")
        print("   1. ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ãªã„")
        print("   2. ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿æ™‚ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ")
        print("   3. max_modelsåˆ¶é™ã«ã‚ˆã‚Šé™¤å¤–")
        print("   4. é‡è¤‡ãƒã‚§ãƒƒã‚¯ã§é™¤å¤–")

        # max_modelsåˆ¶é™ã®ç¢ºèª
        print(f"\n   max_modelsåˆ¶é™ã®ç¢ºèª:")
        print(f"     ç¾åœ¨ã®åˆ¶é™: 50ãƒ¢ãƒ‡ãƒ«")
        print(
            f"     å®Ÿéš›ã®é‡è¤‡é–‹ç™ºè€…æ•°: {len(set(ground_truth).intersection(set([f.replace('agent_', '').replace('.pth', '') for f in os.listdir('models/improved_rl/final_models') if f.endswith('.pth')])))}äºº"
        )

    else:
        print("### ğŸ¯ ä¸»è¦åŸå› : é«˜è²¢çŒ®è€…ã‚«ãƒ†ã‚´ãƒªå†…ã§ã®ç«¶äº‰ã«æ•—åŒ—")
        print("   ndeloofã¯é«˜è²¢çŒ®è€…ã ãŒã€åŒã‚«ãƒ†ã‚´ãƒªå†…ã§ä¸Šä½2ä½ã«å…¥ã‚Œãªã„")
        print("   ä»–ã®é«˜è²¢çŒ®è€…ï¼ˆmilas, gloursï¼‰ãŒã‚ˆã‚Šé«˜ã„ã‚¹ã‚³ã‚¢ã‚’ç²å¾—")

    # è§£æ±ºç­–ã®ææ¡ˆ
    print(f"\n## 4. è§£æ±ºç­–ã®ææ¡ˆ")
    print("-" * 40)

    if "ndeloof" not in trained_models:
        print("### å³åº§ã®è§£æ±ºç­–:")
        print("   1. max_modelsåˆ¶é™ã‚’å¢—åŠ ï¼ˆ50 â†’ 100ï¼‰")
        print("   2. ndeloofã®ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼ã‚’ä¿®æ­£")
        print("   3. é‡è¦é–‹ç™ºè€…ã®å„ªå…ˆèª­ã¿è¾¼ã¿å®Ÿè£…")

    else:
        print("### æ¨è–¦ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®æ”¹å–„:")
        print("   1. é«˜è²¢çŒ®è€…æ ã‚’3äººã«å¢—åŠ ï¼ˆ2 â†’ 3ï¼‰")
        print("   2. è²¢çŒ®é‡é‡ã¿ä»˜ãã‚¹ã‚³ã‚¢èª¿æ•´")
        print("   3. æœ€é«˜è²¢çŒ®è€…ã®å„ªå…ˆé¸å‡º")

    print("\n### é•·æœŸçš„æ”¹å–„:")
    print("   1. å…¨ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãƒ¢ãƒ‡ãƒ«ã®ä½¿ç”¨ï¼ˆ7,001 â†’ å…¨ã¦ï¼‰")
    print("   2. å‹•çš„ã‚«ãƒ†ã‚´ãƒªèª¿æ•´")
    print("   3. å€‹åˆ¥é‡è¦åº¦ã«ã‚ˆã‚‹é‡ã¿ä»˜ã‘")

    return {
        "ndeloof_in_models": "ndeloof" in trained_models,
        "ndeloof_contribution": author_contribution.get("ndeloof", 0),
        "total_models": len(trained_models),
        "available_agents": list(trained_models.keys()),
    }


if __name__ == "__main__":
    results = investigate_ndeloof_exclusion()

    print(f"\nğŸ¯ èª¿æŸ»çµæœã¾ã¨ã‚:")
    print(
        f"   ndeloofãŒãƒ¢ãƒ‡ãƒ«ã«å«ã¾ã‚Œã‚‹: {'âœ…' if results['ndeloof_in_models'] else 'âŒ'}"
    )
    print(f"   ndeloofã®è²¢çŒ®é‡: {results['ndeloof_contribution']}ã‚¿ã‚¹ã‚¯")
    print(f"   åˆ©ç”¨å¯èƒ½ãƒ¢ãƒ‡ãƒ«æ•°: {results['total_models']}")

    if not results["ndeloof_in_models"]:
        print(f"   ğŸš¨ ç·Šæ€¥å¯¾å¿œãŒå¿…è¦: æœ€é«˜è²¢çŒ®è€…ãŒæ¨è–¦å¯¾è±¡å¤–")
