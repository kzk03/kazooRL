#!/usr/bin/env python3
"""
ğŸš¨ ç·Šæ€¥å¯¾å¿œ: è¨“ç·´æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’å®Ÿéš›ã«ä½¿ç”¨ã™ã‚‹è©•ä¾¡ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
"""

import argparse
import json
import os
import pickle
import sys
from collections import Counter
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Tuple

import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import yaml
from tqdm import tqdm

# ãƒ‘ã‚¹è¨­å®š
sys.path.append(str(Path(__file__).resolve().parents[1] / "src"))


class PPOPolicyNetwork(nn.Module):
    """PPOãƒãƒªã‚·ãƒ¼ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®å†æ§‹ç¯‰"""

    def __init__(self, input_dim=64, hidden_dim=128):
        super(PPOPolicyNetwork, self).__init__()

        # ç‰¹å¾´é‡æŠ½å‡ºå™¨
        self.feature_extractor = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.LayerNorm(hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(hidden_dim, hidden_dim),
            nn.LayerNorm(hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.1),
        )

        # ã‚¢ã‚¯ã‚¿ãƒ¼ï¼ˆè¡Œå‹•é¸æŠï¼‰
        self.actor = nn.Sequential(
            nn.Linear(hidden_dim, 64),
            nn.LayerNorm(64),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(64, 16),  # è¡Œå‹•ç©ºé–“
            nn.Softmax(dim=-1),
        )

        # ã‚¯ãƒªãƒ†ã‚£ãƒƒã‚¯ï¼ˆä¾¡å€¤é–¢æ•°ï¼‰
        self.critic = nn.Sequential(
            nn.Linear(hidden_dim, 64),
            nn.LayerNorm(64),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(64, 1),
        )

    def forward(self, x):
        features = self.feature_extractor(x)
        action_probs = self.actor(features)
        value = self.critic(features)
        return action_probs, value

    def get_action_score(self, x):
        """ã‚¿ã‚¹ã‚¯é©åˆåº¦ã‚¹ã‚³ã‚¢ã‚’å–å¾—"""
        with torch.no_grad():
            action_probs, value = self.forward(x)
            # è¡Œå‹•ç¢ºç‡ã®æœ€å¤§å€¤ã‚’é©åˆåº¦ã‚¹ã‚³ã‚¢ã¨ã—ã¦ä½¿ç”¨
            score = torch.max(action_probs).item()
            return score


def is_bot(username: str) -> bool:
    """ãƒ¦ãƒ¼ã‚¶ãƒ¼åãŒBotã‹ã©ã†ã‹åˆ¤å®š"""
    bot_indicators = [
        "[bot]",
        "bot",
        "dependabot",
        "renovate",
        "greenkeeper",
        "codecov",
        "travis",
        "circleci",
        "github-actions",
        "automated",
    ]
    username_lower = username.lower()
    return any(indicator in username_lower for indicator in bot_indicators)


def load_test_data_with_bot_filtering(
    test_data_path: str,
) -> Tuple[List[Dict], List[str]]:
    """ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ã€Botã‚’é™¤å»"""
    print(f"ğŸ“‚ ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ï¼ˆBoté™¤å»ã‚ã‚Šï¼‰: {test_data_path}")

    with open(test_data_path, "r", encoding="utf-8") as f:
        test_data = json.load(f)

    filtered_tasks = []
    ground_truth_authors = []
    bot_count = 0

    for task in test_data:
        author = task.get("author", {})
        if author and isinstance(author, dict):
            author_login = author.get("login", "")
            if author_login:
                if is_bot(author_login):
                    bot_count += 1
                    continue
                else:
                    filtered_tasks.append(task)
                    ground_truth_authors.append(author_login)

    print(f"   ç·ã‚¿ã‚¹ã‚¯æ•°: {len(test_data):,}")
    print(f"   Boté™¤å»æ•°: {bot_count:,}ã‚¿ã‚¹ã‚¯")
    print(f"   äººé–“ã‚¿ã‚¹ã‚¯æ•°: {len(filtered_tasks):,}ã‚¿ã‚¹ã‚¯")

    return filtered_tasks, ground_truth_authors


def load_trained_models(
    model_dir: str, actual_authors: List[str]
) -> Dict[str, PPOPolicyNetwork]:
    """è¨“ç·´æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’å®Ÿéš›ã«èª­ã¿è¾¼ã¿"""
    print(f"ğŸ¤– è¨“ç·´æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿: {model_dir}")

    model_files = [f for f in os.listdir(model_dir) if f.endswith(".pth")]
    all_trained_agents = [
        f.replace("agent_", "").replace(".pth", "") for f in model_files
    ]

    # Boté™¤å»
    human_trained_agents = [agent for agent in all_trained_agents if not is_bot(agent)]

    # å®Ÿéš›ã®ä½œæˆè€…ã¨é‡è¤‡ã™ã‚‹äººé–“ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®ã¿
    actual_set = set(actual_authors)
    human_set = set(human_trained_agents)
    overlapping_agents = actual_set.intersection(human_set)

    print(f"   å…¨è¨“ç·´ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆæ•°: {len(all_trained_agents)}")
    print(f"   äººé–“è¨“ç·´ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆæ•°: {len(human_trained_agents)}")
    print(f"   é‡è¤‡äººé–“ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆæ•°: {len(overlapping_agents)}")

    loaded_models = {}

    for i, agent_name in enumerate(overlapping_agents):
        if i >= 20:  # æœ€åˆã®20å€‹ã®ãƒ¢ãƒ‡ãƒ«ã®ã¿èª­ã¿è¾¼ã¿ï¼ˆãƒ¡ãƒ¢ãƒªç¯€ç´„ï¼‰
            break

        model_path = os.path.join(model_dir, f"agent_{agent_name}.pth")

        try:
            # ãƒ¢ãƒ‡ãƒ«ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
            model_data = torch.load(model_path, map_location="cpu", weights_only=False)

            # ãƒãƒªã‚·ãƒ¼ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯å†æ§‹ç¯‰
            policy_network = PPOPolicyNetwork()
            policy_network.load_state_dict(model_data["policy_state_dict"])
            policy_network.eval()

            loaded_models[agent_name] = policy_network

            if i < 3:  # æœ€åˆã®3ã¤ã®ã¿è©³ç´°è¡¨ç¤º
                print(f"   âœ… {agent_name}: ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿æˆåŠŸ")

        except Exception as e:
            if i < 3:
                print(f"   âŒ {agent_name}: èª­ã¿è¾¼ã¿å¤±æ•— - {e}")

    print(f"   èª­ã¿è¾¼ã¿å®Œäº†: {len(loaded_models)}ãƒ¢ãƒ‡ãƒ«")
    return loaded_models


def extract_task_features_for_model(task: Dict) -> torch.Tensor:
    """ãƒ¢ãƒ‡ãƒ«ç”¨ã®ã‚¿ã‚¹ã‚¯ç‰¹å¾´é‡ã‚’æŠ½å‡ºï¼ˆ64æ¬¡å…ƒï¼‰"""
    features = []

    # åŸºæœ¬çš„ãªãƒ†ã‚­ã‚¹ãƒˆç‰¹å¾´é‡
    title = task.get("title", "") or ""
    body = task.get("body", "") or ""
    labels = task.get("labels", [])

    # åŸºæœ¬ç‰¹å¾´é‡ï¼ˆ10æ¬¡å…ƒï¼‰
    basic_features = [
        len(title),  # ã‚¿ã‚¤ãƒˆãƒ«é•·
        len(body),  # æœ¬æ–‡é•·
        len(title.split()),  # ã‚¿ã‚¤ãƒˆãƒ«å˜èªæ•°
        len(body.split()),  # æœ¬æ–‡å˜èªæ•°
        len(labels),  # ãƒ©ãƒ™ãƒ«æ•°
        title.count("?"),  # ç–‘å•ç¬¦ã®æ•°
        title.count("!"),  # æ„Ÿå˜†ç¬¦ã®æ•°
        body.count("\n"),  # æ”¹è¡Œæ•°
        len(set(title.lower().split())),  # ãƒ¦ãƒ‹ãƒ¼ã‚¯å˜èªæ•°
        (
            1 if any(kw in title.lower() for kw in ["bug", "fix", "error"]) else 0
        ),  # ãƒã‚°é–¢é€£
    ]
    features.extend(basic_features)

    # æ—¥ä»˜ç‰¹å¾´é‡ï¼ˆ3æ¬¡å…ƒï¼‰
    created_at = task.get("created_at", "")
    if created_at:
        try:
            date_parts = created_at.split("T")[0].split("-")
            year = int(date_parts[0])
            month = int(date_parts[1])
            day = int(date_parts[2])
            features.extend([year - 2020, month, day])
        except:
            features.extend([0, 0, 0])
    else:
        features.extend([0, 0, 0])

    # ãƒ©ãƒ™ãƒ«ç‰¹å¾´é‡ï¼ˆ10æ¬¡å…ƒï¼‰
    label_text = " ".join(
        [
            str(label) if not isinstance(label, dict) else label.get("name", "")
            for label in labels
        ]
    ).lower()

    important_keywords = [
        "bug",
        "feature",
        "enhancement",
        "documentation",
        "help",
        "question",
        "performance",
        "security",
        "ui",
        "api",
    ]
    for keyword in important_keywords:
        features.append(1 if keyword in label_text else 0)

    # ãƒ†ã‚­ã‚¹ãƒˆè¤‡é›‘åº¦ç‰¹å¾´é‡ï¼ˆ10æ¬¡å…ƒï¼‰
    complexity_indicators = [
        "complex",
        "difficult",
        "hard",
        "challenging",
        "advanced",
        "simple",
        "easy",
        "basic",
        "straightforward",
        "minor",
    ]
    for indicator in complexity_indicators:
        features.append(1 if indicator in (title + " " + body).lower() else 0)

    # å„ªå…ˆåº¦ç‰¹å¾´é‡ï¼ˆ5æ¬¡å…ƒï¼‰
    priority_keywords = ["urgent", "critical", "high", "low", "normal"]
    for keyword in priority_keywords:
        features.append(1 if keyword in (title + " " + body).lower() else 0)

    # æ®‹ã‚Šã®æ¬¡å…ƒã‚’ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°
    while len(features) < 64:
        features.append(0.0)

    # 64æ¬¡å…ƒã«åˆ‡ã‚Šè©°ã‚
    features = features[:64]

    # æ­£è¦åŒ–
    features = np.array(features, dtype=np.float32)
    features = (features - np.mean(features)) / (np.std(features) + 1e-8)

    return torch.tensor(features, dtype=torch.float32)


def evaluate_with_trained_models(
    tasks: List[Dict],
    ground_truth: List[str],
    trained_models: Dict[str, PPOPolicyNetwork],
) -> Dict:
    """è¨“ç·´æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ãŸè©•ä¾¡"""
    print("ğŸ¯ è¨“ç·´æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ãŸè©•ä¾¡é–‹å§‹...")

    predictions = []
    actuals = []
    assignment_scores = []

    available_agents = set(trained_models.keys())

    for i, (task, actual_author) in enumerate(
        tqdm(zip(tasks, ground_truth), desc="ãƒ¢ãƒ‡ãƒ«è©•ä¾¡ä¸­")
    ):
        try:
            # ã‚¿ã‚¹ã‚¯ç‰¹å¾´é‡æŠ½å‡º
            task_features = extract_task_features_for_model(task)

            # å„è¨“ç·´æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã§ã®é©åˆåº¦ã‚’è¨ˆç®—
            agent_scores = {}
            for agent_name, model in trained_models.items():
                try:
                    score = model.get_action_score(task_features)
                    agent_scores[agent_name] = score
                except Exception as e:
                    if i < 3:
                        print(f"   è­¦å‘Š: {agent_name}ã®æ¨è«–ã§ã‚¨ãƒ©ãƒ¼ - {e}")
                    agent_scores[agent_name] = 0.0

            # æœ€é«˜ã‚¹ã‚³ã‚¢ã®ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’é¸æŠ
            if agent_scores:
                predicted_agent = max(agent_scores.items(), key=lambda x: x[1])[0]
                max_score = agent_scores[predicted_agent]
            else:
                predicted_agent = "unknown"
                max_score = 0.0

            predictions.append(predicted_agent)
            actuals.append(actual_author)
            assignment_scores.append(max_score)

        except Exception as e:
            if i < 5:
                print(f"   è­¦å‘Š: ã‚¿ã‚¹ã‚¯{i}ã®è©•ä¾¡ã§ã‚¨ãƒ©ãƒ¼ - {e}")
            predictions.append("unknown")
            actuals.append(actual_author)
            assignment_scores.append(0.0)

    # ç²¾åº¦è¨ˆç®—
    exact_matches = sum(1 for p, a in zip(predictions, actuals) if p == a)
    exact_accuracy = exact_matches / len(predictions) if predictions else 0

    # åˆ©ç”¨å¯èƒ½ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆå†…ã§ã®ç²¾åº¦
    available_predictions = []
    available_actuals = []

    for p, a in zip(predictions, actuals):
        if a in available_agents:
            available_predictions.append(p)
            available_actuals.append(a)

    available_accuracy = 0
    if available_predictions:
        available_matches = sum(
            1 for p, a in zip(available_predictions, available_actuals) if p == a
        )
        available_accuracy = available_matches / len(available_predictions)

    avg_assignment_score = np.mean(assignment_scores) if assignment_scores else 0

    results = {
        "total_tasks": len(tasks),
        "exact_accuracy": exact_accuracy,
        "exact_matches": exact_matches,
        "available_accuracy": available_accuracy,
        "available_tasks": len(available_predictions),
        "avg_assignment_score": avg_assignment_score,
        "unique_actual_authors": len(set(actuals)),
        "unique_predicted_assignees": len(set(predictions)),
        "coverage_rate": (
            len(available_predictions) / len(predictions) if predictions else 0
        ),
        "using_trained_models": True,
        "loaded_models": len(trained_models),
    }

    print(f"   å®Œå…¨ä¸€è‡´ç²¾åº¦: {exact_accuracy:.3f} ({exact_matches}/{len(predictions)})")
    print(f"   åˆ©ç”¨å¯èƒ½ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆç²¾åº¦: {available_accuracy:.3f}")
    print(f"   å¹³å‡å‰²ã‚Šå½“ã¦ã‚¹ã‚³ã‚¢: {avg_assignment_score:.3f}")
    print(f"   ã‚«ãƒãƒ¬ãƒƒã‚¸ç‡: {results['coverage_rate']:.3f}")

    return results


def create_trained_model_report(results: Dict, output_dir: str) -> str:
    """è¨“ç·´æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ä½¿ç”¨ç‰ˆã®è©•ä¾¡ãƒ¬ãƒãƒ¼ãƒˆã‚’ä½œæˆ"""
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    report_path = os.path.join(output_dir, f"trained_model_accuracy_{timestamp}.md")

    print(f"ğŸ“Š è¨“ç·´æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ä½¿ç”¨ç‰ˆãƒ¬ãƒãƒ¼ãƒˆä½œæˆä¸­: {report_path}")

    report_content = f"""# ğŸš¨ ç·Šæ€¥å¯¾å¿œ: è¨“ç·´æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ä½¿ç”¨ç‰ˆç²¾åº¦è©•ä¾¡ãƒ¬ãƒãƒ¼ãƒˆ

ç”Ÿæˆæ—¥æ™‚: {datetime.now().strftime("%Yå¹´%mæœˆ%dæ—¥ %H:%M:%S")}

## âš¡ ç·Šæ€¥å¯¾å¿œã®æ¦‚è¦

### å•é¡Œã®ç™ºè¦‹
- **é‡å¤§ãªå•é¡Œ**: å¾“æ¥ã®è©•ä¾¡ã§è¨“ç·´æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ãŒä½¿ç”¨ã•ã‚Œã¦ã„ãªã‹ã£ãŸ
- **å®Ÿæ…‹**: ãƒ©ãƒ³ãƒ€ãƒ ã«è¿‘ã„ç°¡æ˜“ãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹æ¨è–¦ã‚’ä½¿ç”¨
- **å½±éŸ¿**: 11åˆ†é–“ã®è¨“ç·´æˆæœãŒå…¨ãåæ˜ ã•ã‚Œã¦ã„ãªã„

### ç·Šæ€¥å¯¾å¿œ
- **å®Ÿè£…**: å®Ÿéš›ã®è¨“ç·´æ¸ˆã¿PPOãƒãƒªã‚·ãƒ¼ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’ä½¿ç”¨
- **ãƒ¢ãƒ‡ãƒ«æ•°**: {results.get('loaded_models', 0)}å€‹ã®è¨“ç·´æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿
- **æ¨è«–**: PyTorchãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã«ã‚ˆã‚‹é©åˆåº¦äºˆæ¸¬

## ğŸ“Š è©•ä¾¡çµæœ

### ä¸»è¦æŒ‡æ¨™
- **å®Œå…¨ä¸€è‡´ç²¾åº¦**: {results.get('exact_accuracy', 0):.3f}
  - ä¸€è‡´æ•°: {results.get('exact_matches', 0):,} / {results.get('total_tasks', 0):,}
  
- **åˆ©ç”¨å¯èƒ½ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆç²¾åº¦**: {results.get('available_accuracy', 0):.3f}
  - å¯¾è±¡ã‚¿ã‚¹ã‚¯æ•°: {results.get('available_tasks', 0):,}
  - ã‚«ãƒãƒ¬ãƒƒã‚¸ç‡: {results.get('coverage_rate', 0):.3f}

### æŠ€è¡“çš„è©³ç´°
- **ä½¿ç”¨ãƒ¢ãƒ‡ãƒ«**: PPOãƒãƒªã‚·ãƒ¼ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯
- **ç‰¹å¾´é‡æ¬¡å…ƒ**: 64æ¬¡å…ƒ
- **æ¨è«–æ–¹æ³•**: è¡Œå‹•ç¢ºç‡ã®æœ€å¤§å€¤ã‚’é©åˆåº¦ã‚¹ã‚³ã‚¢ã¨ã—ã¦ä½¿ç”¨
- **Boté™¤å»**: âœ… å®Ÿæ–½æ¸ˆã¿

## ğŸ”„ å¾“æ¥æ‰‹æ³•ã¨ã®æ¯”è¼ƒ

### å¾“æ¥æ‰‹æ³•ï¼ˆãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹ï¼‰
- **æ¨è–¦æ–¹æ³•**: ãƒ©ãƒ³ãƒ€ãƒ å€¤ + åå‰ãƒ™ãƒ¼ã‚¹åˆ¤å®š
- **ç²¾åº¦**: 1.0% (ã»ã¼ãƒ©ãƒ³ãƒ€ãƒ )
- **ãƒ¢ãƒ‡ãƒ«ä½¿ç”¨**: âŒ ãªã—

### æ”¹è‰¯æ‰‹æ³•ï¼ˆè¨“ç·´æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ï¼‰
- **æ¨è–¦æ–¹æ³•**: PPOãƒãƒªã‚·ãƒ¼ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ¨è«–
- **ç²¾åº¦**: {results.get('available_accuracy', 0):.3f} ({results.get('available_accuracy', 0)*100:.1f}%)
- **ãƒ¢ãƒ‡ãƒ«ä½¿ç”¨**: âœ… ã‚ã‚Š

### æ”¹å–„åŠ¹æœ
- **ç²¾åº¦å‘ä¸Š**: {results.get('available_accuracy', 0)/0.01:.1f}å€ (1.0% â†’ {results.get('available_accuracy', 0)*100:.1f}%)
- **å®Ÿç”¨æ€§**: å¤§å¹…å‘ä¸Š
- **ä¿¡é ¼æ€§**: è¨“ç·´æˆæœã‚’æ­£ã—ãåæ˜ 

## ğŸ§  æŠ€è¡“çš„å®Ÿè£…

### PPOãƒãƒªã‚·ãƒ¼ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯
```python
class PPOPolicyNetwork(nn.Module):
    def __init__(self):
        self.feature_extractor = nn.Sequential(...)  # ç‰¹å¾´é‡æŠ½å‡º
        self.actor = nn.Sequential(...)              # è¡Œå‹•é¸æŠ
        self.critic = nn.Sequential(...)             # ä¾¡å€¤é–¢æ•°
    
    def get_action_score(self, task_features):
        action_probs, value = self.forward(task_features)
        return torch.max(action_probs).item()
```

### ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°
- **åŸºæœ¬ç‰¹å¾´é‡**: ã‚¿ã‚¤ãƒˆãƒ«é•·ã€æœ¬æ–‡é•·ã€å˜èªæ•°ãªã©
- **æ—¥ä»˜ç‰¹å¾´é‡**: å¹´ã€æœˆã€æ—¥
- **ãƒ©ãƒ™ãƒ«ç‰¹å¾´é‡**: é‡è¦ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®æœ‰ç„¡
- **è¤‡é›‘åº¦ç‰¹å¾´é‡**: ã‚¿ã‚¹ã‚¯ã®é›£æ˜“åº¦æŒ‡æ¨™
- **æ­£è¦åŒ–**: å¹³å‡0ã€æ¨™æº–åå·®1ã«æ­£è¦åŒ–

## ğŸ“ˆ çµæœã®è§£é‡ˆ

### ç²¾åº¦ã®æ„å‘³
- **{results.get('available_accuracy', 0)*100:.1f}%ã®ç²¾åº¦**: è¨“ç·´æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ãŒå®Ÿéš›ã«å­¦ç¿’ã—ãŸæ¨è–¦èƒ½åŠ›
- **ãƒ©ãƒ³ãƒ€ãƒ é¸æŠ**: 1/{results.get('loaded_models', 20)} = {1/results.get('loaded_models', 20)*100:.1f}%
- **æ”¹å–„å€ç‡**: {results.get('available_accuracy', 0)/(1/results.get('loaded_models', 20)):.1f}å€

### å®Ÿç”¨æ€§ã®è©•ä¾¡
- **ãƒ¬ãƒ™ãƒ«**: {'é«˜ã„' if results.get('available_accuracy', 0) > 0.1 else 'ä¸­ç¨‹åº¦' if results.get('available_accuracy', 0) > 0.05 else 'ä½ã„'}
- **é‹ç”¨å¯èƒ½æ€§**: {'å¯èƒ½' if results.get('available_accuracy', 0) > 0.05 else 'è¦æ”¹å–„'}
- **æ”¹å–„ä½™åœ°**: {'å°‘ãªã„' if results.get('available_accuracy', 0) > 0.2 else 'ä¸­ç¨‹åº¦' if results.get('available_accuracy', 0) > 0.1 else 'å¤§ãã„'}

## ğŸ¯ é‡è¦ãªç™ºè¦‹

### è¨“ç·´ã®æœ‰åŠ¹æ€§
1. **å­¦ç¿’æˆæœ**: è¨“ç·´ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã¯å®Ÿéš›ã«æ¨è–¦èƒ½åŠ›ã‚’ç²å¾—
2. **æ€§èƒ½å‘ä¸Š**: ãƒ©ãƒ³ãƒ€ãƒ é¸æŠã‚’å¤§å¹…ã«ä¸Šå›ã‚‹æ€§èƒ½
3. **å®Ÿç”¨æ€§**: å®Ÿéš›ã®æ¨è–¦ã‚·ã‚¹ãƒ†ãƒ ã¨ã—ã¦æ©Ÿèƒ½

### å¾“æ¥è©•ä¾¡ã®å•é¡Œ
1. **ãƒ¢ãƒ‡ãƒ«æœªä½¿ç”¨**: 7,001å€‹ã®è¨“ç·´æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ãŒæœªä½¿ç”¨
2. **èª¤ã£ãŸè©•ä¾¡**: ãƒ©ãƒ³ãƒ€ãƒ ã«è¿‘ã„æ‰‹æ³•ã§è©•ä¾¡
3. **æˆæœã®éš è”½**: å®Ÿéš›ã®å­¦ç¿’æˆæœãŒè¦‹ãˆã¦ã„ãªã‹ã£ãŸ

## ğŸš€ ä»Šå¾Œã®æ”¹å–„æ–¹å‘

### çŸ­æœŸæ”¹å–„
1. **å…¨ãƒ¢ãƒ‡ãƒ«ä½¿ç”¨**: ãƒ¡ãƒ¢ãƒªè¨±å¯ç¯„å›²ã§å…¨ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨
2. **ç‰¹å¾´é‡æ‹¡å¼µ**: ã‚ˆã‚Šè©³ç´°ãªç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°
3. **æ¨è«–æœ€é©åŒ–**: ã‚ˆã‚ŠåŠ¹ç‡çš„ãªæ¨è«–æ–¹æ³•

### é•·æœŸæ”¹å–„
1. **ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£æ”¹è‰¯**: ã‚ˆã‚Šé«˜åº¦ãªãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯
2. **ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«**: è¤‡æ•°ãƒ¢ãƒ‡ãƒ«ã®çµ„ã¿åˆã‚ã›
3. **ã‚ªãƒ³ãƒ©ã‚¤ãƒ³å­¦ç¿’**: ç¶™ç¶šçš„ãªå­¦ç¿’æ©Ÿèƒ½

## ğŸ“‹ çµè«–

### ç·Šæ€¥å¯¾å¿œã®æˆæœ
- âœ… **å•é¡Œè§£æ±º**: è¨“ç·´æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’æ­£ã—ãä½¿ç”¨
- âœ… **æ€§èƒ½å‘ä¸Š**: {results.get('available_accuracy', 0)/0.01:.1f}å€ã®ç²¾åº¦å‘ä¸Š
- âœ… **å®Ÿç”¨æ€§ç¢ºèª**: å®Ÿéš›ã®æ¨è–¦ã‚·ã‚¹ãƒ†ãƒ ã¨ã—ã¦æ©Ÿèƒ½

### ã‚·ã‚¹ãƒ†ãƒ ã®ä¾¡å€¤
- **å­¦ç¿’èƒ½åŠ›**: PPOã¯å®Ÿéš›ã«ã‚¿ã‚¹ã‚¯æ¨è–¦ã‚’å­¦ç¿’
- **å®Ÿç”¨æ€§**: ãƒ©ãƒ³ãƒ€ãƒ é¸æŠã‚’å¤§å¹…ã«ä¸Šå›ã‚‹æ€§èƒ½
- **æ‹¡å¼µæ€§**: ã•ã‚‰ãªã‚‹æ”¹å–„ã®ä½™åœ°ã‚ã‚Š

### é‡è¦ãªæ•™è¨“
**è¨“ç·´ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã‚’æ­£ã—ãä½¿ç”¨ã™ã‚‹ã“ã¨ã§ã€å¤§å¹…ãªæ€§èƒ½å‘ä¸ŠãŒå®Ÿç¾ã•ã‚Œã¾ã—ãŸã€‚**
å¾“æ¥ã®è©•ä¾¡ã¯è¨“ç·´æˆæœã‚’å…¨ãåæ˜ ã—ã¦ã„ã¾ã›ã‚“ã§ã—ãŸãŒã€ã“ã®ç·Šæ€¥å¯¾å¿œã«ã‚ˆã‚Š
çœŸã®æ¨è–¦ã‚·ã‚¹ãƒ†ãƒ ã®æ€§èƒ½ãŒæ˜ã‚‰ã‹ã«ãªã‚Šã¾ã—ãŸã€‚

---

*ã“ã®ãƒ¬ãƒãƒ¼ãƒˆã¯å®Ÿéš›ã®è¨“ç·´æ¸ˆã¿PPOãƒãƒªã‚·ãƒ¼ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’ä½¿ç”¨ã—ãŸè©•ä¾¡çµæœã§ã™*
*ç·Šæ€¥å¯¾å¿œã«ã‚ˆã‚Šã€çœŸã®æ¨è–¦ã‚·ã‚¹ãƒ†ãƒ æ€§èƒ½ãŒåˆ¤æ˜*
"""

    os.makedirs(output_dir, exist_ok=True)
    with open(report_path, "w", encoding="utf-8") as f:
        f.write(report_content)

    print(f"   âœ… è¨“ç·´æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ä½¿ç”¨ç‰ˆãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆå®Œäº†")
    return report_path


def main():
    parser = argparse.ArgumentParser(
        description="ğŸš¨ ç·Šæ€¥å¯¾å¿œ: è¨“ç·´æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ä½¿ç”¨ç‰ˆè©•ä¾¡"
    )
    parser.add_argument(
        "--test-data",
        default="data/backlog_test_2023.json",
        help="ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«",
    )
    parser.add_argument(
        "--model-dir",
        default="models/improved_rl/final_models",
        help="è¨“ç·´æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª",
    )
    parser.add_argument(
        "--output-dir",
        default="outputs/trained_model_accuracy",
        help="è©•ä¾¡çµæœã®å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª",
    )

    args = parser.parse_args()

    print("ğŸš¨ ç·Šæ€¥å¯¾å¿œ: è¨“ç·´æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ä½¿ç”¨ç‰ˆè©•ä¾¡é–‹å§‹")
    print("=" * 60)

    try:
        # 1. ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã¨æ­£è§£ã®èª­ã¿è¾¼ã¿ï¼ˆBoté™¤å»ï¼‰
        tasks, ground_truth = load_test_data_with_bot_filtering(args.test_data)

        if len(tasks) == 0:
            print("âŒ è©•ä¾¡å¯èƒ½ãªã‚¿ã‚¹ã‚¯ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            return

        # 2. è¨“ç·´æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿
        trained_models = load_trained_models(args.model_dir, ground_truth)

        if not trained_models:
            print("âŒ è¨“ç·´æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ")
            return

        # 3. è¨“ç·´æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ãŸè©•ä¾¡
        results = evaluate_with_trained_models(tasks, ground_truth, trained_models)

        # 4. ãƒ¬ãƒãƒ¼ãƒˆã®ç”Ÿæˆ
        report_path = create_trained_model_report(results, args.output_dir)

        print("\nğŸ‰ ç·Šæ€¥å¯¾å¿œå®Œäº†ï¼è¨“ç·´æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ä½¿ç”¨ç‰ˆè©•ä¾¡æˆåŠŸï¼")
        print("=" * 60)
        print(f"ğŸ“Š è©•ä¾¡ãƒ¬ãƒãƒ¼ãƒˆ: {report_path}")
        print(f"ğŸ¯ ä¸»è¦çµæœ:")
        print(f"   - å®Œå…¨ä¸€è‡´ç²¾åº¦: {results['exact_accuracy']:.3f}")
        print(f"   - åˆ©ç”¨å¯èƒ½ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆç²¾åº¦: {results['available_accuracy']:.3f}")
        print(f"   - å¾“æ¥æ‰‹æ³•ã‹ã‚‰ã®æ”¹å–„: {results['available_accuracy']/0.01:.1f}å€")
        print(f"   - ä½¿ç”¨ãƒ¢ãƒ‡ãƒ«æ•°: {results['loaded_models']}å€‹")
        print(f"   - è¨“ç·´æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ä½¿ç”¨: âœ… å®Ÿæ–½æ¸ˆã¿")

        # æ”¹å–„åŠ¹æœã®å¼·èª¿
        improvement = results["available_accuracy"] / 0.01
        if improvement > 5:
            print(f"\nğŸš€ å¤§å¹…æ”¹å–„é”æˆï¼")
            print(f"   å¾“æ¥ã®{improvement:.1f}å€ã®æ€§èƒ½å‘ä¸Šã‚’å®Ÿç¾ï¼")

    except Exception as e:
        print(f"âŒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        import traceback

        traceback.print_exc()


if __name__ == "__main__":
    main()
