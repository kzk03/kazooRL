#!/usr/bin/env python3
"""
å¼·åŒ–å­¦ç¿’ãƒ™ãƒ¼ã‚¹ã®ã‚¿ã‚¹ã‚¯æ¨è–¦ã‚·ã‚¹ãƒ†ãƒ 

- æ—¢å­˜ã®é¡ä¼¼åº¦ãƒ™ãƒ¼ã‚¹ã‚·ã‚¹ãƒ†ãƒ ã‚’å¼·åŒ–å­¦ç¿’ã§æ”¹è‰¯
- RandomForestã®äºˆæ¸¬ã‚’å ±é…¬ã¨ã—ã¦æ´»ç”¨
- ã‚ªãƒ•ãƒ©ã‚¤ãƒ³å¼·åŒ–å­¦ç¿’ã§ãƒãƒªã‚·ãƒ¼æœ€é©åŒ–
"""

import argparse
import json
import pickle

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
import sys
from collections import Counter, defaultdict
from datetime import datetime
from pathlib import Path

# å¼·åŒ–å­¦ç¿’é–¢é€£
import gymnasium as gym
import numpy as np
import pandas as pd
import torch
import yaml
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import StandardScaler
from stable_baselines3 import DQN, PPO
from stable_baselines3.common.env_util import make_vec_env
from stable_baselines3.common.vec_env import DummyVecEnv

sys.path.append('/Users/kazuki-h/rl/kazoo')
sys.path.append('/Users/kazuki-h/rl/kazoo/src')

from src.kazoo.envs.task import Task
from src.kazoo.features.feature_extractor import FeatureExtractor


class HybridRecommenderEnvironment(gym.Env):
    """
    ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¨è–¦ç’°å¢ƒ
    - é¡ä¼¼åº¦ãƒ™ãƒ¼ã‚¹ã®æ¨è–¦ã‚’å¼·åŒ–å­¦ç¿’ã§æ”¹è‰¯
    - ã‚ªãƒ•ãƒ©ã‚¤ãƒ³å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã§ãƒãƒªã‚·ãƒ¼ã‚’è¨“ç·´
    """
    
    def __init__(self, similarity_model, training_data, config):
        super().__init__()
        
        self.similarity_model = similarity_model
        self.training_data = training_data
        self.config = config
        
        # é–‹ç™ºè€…ãƒªã‚¹ãƒˆ
        self.developers = list(similarity_model.developer_embeddings.keys())
        self.num_developers = len(self.developers)
        
        # è¡Œå‹•ç©ºé–“: é–‹ç™ºè€…é¸æŠ + ä¿¡é ¼åº¦èª¿æ•´
        self.action_space = gym.spaces.MultiDiscrete([
            self.num_developers,  # é–‹ç™ºè€…é¸æŠ
            5                     # ä¿¡é ¼åº¦ãƒ¬ãƒ™ãƒ« (0-4)
        ])
        
        # è¦³æ¸¬ç©ºé–“: ç‰¹å¾´é‡ + é¡ä¼¼åº¦ã‚¹ã‚³ã‚¢
        feature_dim = len(similarity_model.feature_extractor.feature_names)
        self.observation_space = gym.spaces.Box(
            low=-np.inf, high=np.inf,
            shape=(feature_dim + self.num_developers,),  # ç‰¹å¾´é‡ + å„é–‹ç™ºè€…ã¸ã®é¡ä¼¼åº¦
            dtype=np.float32
        )
        
        # ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ç®¡ç†
        self.current_episode = 0
        self.current_task_idx = 0
        self.tasks = []
        self.ground_truth = {}
        
        print(f"ğŸ¤– ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¨è–¦ç’°å¢ƒåˆæœŸåŒ–")
        print(f"   é–‹ç™ºè€…æ•°: {self.num_developers}")
        print(f"   è¦³æ¸¬æ¬¡å…ƒ: {self.observation_space.shape[0]}")
        print(f"   è¡Œå‹•ç©ºé–“: {self.action_space}")
    
    def reset(self, seed=None, options=None):
        """ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã‚’ãƒªã‚»ãƒƒãƒˆ"""
        super().reset(seed=seed)
        
        # ãƒ©ãƒ³ãƒ€ãƒ ã«ã‚¿ã‚¹ã‚¯ã‚’é¸æŠ
        task_data = np.random.choice(self.training_data)
        self.current_task = Task(task_data)
        
        # å®Ÿéš›ã®æ‹…å½“è€…ã‚’å–å¾—
        self.ground_truth_dev = None
        if 'assignees' in task_data and task_data['assignees']:
            self.ground_truth_dev = task_data['assignees'][0].get('login')
        
        # è¦³æ¸¬ã‚’ç”Ÿæˆ
        obs = self._get_observation()
        
        return obs, {}
    
    def step(self, action):
        """ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚’å®Ÿè¡Œ"""
        dev_idx, confidence_level = action
        
        if dev_idx >= len(self.developers):
            # ç„¡åŠ¹ãªã‚¢ã‚¯ã‚·ãƒ§ãƒ³
            reward = -1.0
            terminated = True
            obs = np.zeros(self.observation_space.shape[0])
            return obs, reward, terminated, False, {}
        
        selected_dev = self.developers[dev_idx]
        confidence_weight = (confidence_level + 1) / 5.0  # 0.2-1.0
        
        # å ±é…¬è¨ˆç®—
        reward = self._calculate_reward(selected_dev, confidence_weight)
        
        # ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰çµ‚äº†
        terminated = True
        obs = np.zeros(self.observation_space.shape[0])
        
        info = {
            'selected_dev': selected_dev,
            'ground_truth_dev': self.ground_truth_dev,
            'confidence': confidence_weight,
            'similarity_scores': getattr(self, 'last_similarity_scores', {})
        }
        
        return obs, reward, terminated, False, info
    
    def _get_observation(self):
        """ç¾åœ¨ã®è¦³æ¸¬ã‚’å–å¾—"""
        # ã‚¿ã‚¹ã‚¯ã®ç‰¹å¾´é‡
        dummy_env = type('DummyEnv', (), {
            'backlog': [],
            'dev_profiles': {},
            'assignments': {},
            'dev_action_history': {}
        })()
        
        # ä»£è¡¨é–‹ç™ºè€…ã§ã®ç‰¹å¾´é‡æŠ½å‡º
        representative_dev = {"name": self.developers[0], "profile": {}}
        task_features = self.similarity_model.feature_extractor.get_features(
            self.current_task, representative_dev, dummy_env
        )
        
        # å„é–‹ç™ºè€…ã¸ã®é¡ä¼¼åº¦ã‚¹ã‚³ã‚¢
        similarity_scores = []
        self.last_similarity_scores = {}
        
        for dev_name in self.developers:
            # é–‹ç™ºè€…ã®å¹³å‡ç‰¹å¾´é‡
            dev_embedding = self.similarity_model.developer_embeddings[dev_name]
            
            # ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦
            similarity = np.dot(task_features, dev_embedding) / (
                np.linalg.norm(task_features) * np.linalg.norm(dev_embedding)
            )
            similarity_scores.append(similarity)
            self.last_similarity_scores[dev_name] = similarity
        
        # è¦³æ¸¬ãƒ™ã‚¯ãƒˆãƒ«: ç‰¹å¾´é‡ + é¡ä¼¼åº¦ã‚¹ã‚³ã‚¢
        obs = np.concatenate([task_features, similarity_scores])
        return obs.astype(np.float32)
    
    def _calculate_reward(self, selected_dev, confidence_weight):
        """å ±é…¬ã‚’è¨ˆç®—"""
        base_reward = 0.0
        
        # 1. æ­£è§£å ±é…¬
        if selected_dev == self.ground_truth_dev:
            base_reward += 10.0
        
        # 2. é¡ä¼¼åº¦å ±é…¬
        if hasattr(self, 'last_similarity_scores'):
            similarity = self.last_similarity_scores.get(selected_dev, 0.0)
            base_reward += similarity * 5.0
        
        # 3. RandomForestäºˆæ¸¬å ±é…¬
        try:
            dummy_env = type('DummyEnv', (), {
                'backlog': [],
                'dev_profiles': {},
                'assignments': {},
                'dev_action_history': {}
            })()
            
            dev_obj = {"name": selected_dev, "profile": {}}
            features = self.similarity_model.feature_extractor.get_features(
                self.current_task, dev_obj, dummy_env
            )
            features_scaled = self.similarity_model.scaler.transform([features])
            
            # äºˆæ¸¬ç¢ºç‡
            proba = self.similarity_model.rf_classifier.predict_proba(features_scaled)[0]
            dev_classes = self.similarity_model.rf_classifier.classes_
            
            if selected_dev in dev_classes:
                dev_idx = list(dev_classes).index(selected_dev)
                rf_score = proba[dev_idx]
                base_reward += rf_score * 3.0
        
        except Exception:
            pass
        
        # 4. ä¿¡é ¼åº¦ãƒœãƒ¼ãƒŠã‚¹
        base_reward *= confidence_weight
        
        return base_reward


class RLEnhancedRecommender:
    """å¼·åŒ–å­¦ç¿’ã§æ‹¡å¼µã•ã‚ŒãŸæ¨è–¦ã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self, similarity_model, config_path):
        self.similarity_model = similarity_model
        self.config_path = config_path
        
        with open(config_path, 'r', encoding='utf-8') as f:
            self.config = yaml.safe_load(f)
        
        print("ğŸš€ å¼·åŒ–å­¦ç¿’æ‹¡å¼µæ¨è–¦ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å®Œäº†")
    
    def train_rl_policy(self, training_data, total_timesteps=50000):
        """å¼·åŒ–å­¦ç¿’ãƒãƒªã‚·ãƒ¼ã‚’è¨“ç·´"""
        print("ğŸ“ å¼·åŒ–å­¦ç¿’ãƒãƒªã‚·ãƒ¼è¨“ç·´é–‹å§‹...")
        
        # ç’°å¢ƒä½œæˆ
        def make_env():
            return HybridRecommenderEnvironment(
                self.similarity_model, training_data, self.config
            )
        
        env = DummyVecEnv([make_env])
        
        # PPOã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ
        self.rl_agent = PPO(
            "MlpPolicy",
            env,
            verbose=1,
            learning_rate=3e-4,
            n_steps=2048,
            batch_size=64,
            n_epochs=10,
            gamma=0.99,
            device="auto"
        )
        
        # è¨“ç·´å®Ÿè¡Œ
        print(f"   è¨“ç·´ã‚¹ãƒ†ãƒƒãƒ—: {total_timesteps:,}")
        self.rl_agent.learn(total_timesteps=total_timesteps)
        
        print("âœ… å¼·åŒ–å­¦ç¿’ãƒãƒªã‚·ãƒ¼è¨“ç·´å®Œäº†")
    
    def predict_with_rl(self, test_data):
        """å¼·åŒ–å­¦ç¿’ãƒãƒªã‚·ãƒ¼ã§äºˆæ¸¬"""
        print("ğŸ¤– å¼·åŒ–å­¦ç¿’äºˆæ¸¬å®Ÿè¡Œä¸­...")
        
        predictions = {}
        prediction_scores = {}
        
        # ãƒ†ã‚¹ãƒˆç’°å¢ƒ
        test_env = HybridRecommenderEnvironment(
            self.similarity_model, test_data, self.config
        )
        
        for i, task_data in enumerate(test_data):
            task_id = task_data.get('id') or task_data.get('number')
            if not task_id:
                continue
            
            try:
                # ã‚¿ã‚¹ã‚¯ã‚»ãƒƒãƒˆ
                test_env.current_task = Task(task_data)
                obs = test_env._get_observation()
                
                # å¼·åŒ–å­¦ç¿’ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã§äºˆæ¸¬
                action, _ = self.rl_agent.predict(obs, deterministic=True)
                dev_idx, confidence_level = action
                
                if dev_idx < len(test_env.developers):
                    predicted_dev = test_env.developers[dev_idx]
                    confidence = (confidence_level + 1) / 5.0
                    
                    predictions[task_id] = predicted_dev
                    prediction_scores[task_id] = {
                        'predicted_dev': predicted_dev,
                        'rl_confidence': confidence,
                        'dev_idx': dev_idx,
                        'confidence_level': confidence_level
                    }
                
                if (i + 1) % 100 == 0:
                    print(f"   é€²æ—: {i + 1}/{len(test_data)}")
            
            except Exception as e:
                print(f"âš ï¸ ã‚¿ã‚¹ã‚¯ {task_id} ã®RLäºˆæ¸¬ã‚¨ãƒ©ãƒ¼: {e}")
                continue
        
        print(f"   RLäºˆæ¸¬å®Œäº†: {len(predictions)} ã‚¿ã‚¹ã‚¯")
        return predictions, prediction_scores
    
    def hybrid_predict(self, test_data, alpha=0.7):
        """ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰äºˆæ¸¬: é¡ä¼¼åº¦ãƒ™ãƒ¼ã‚¹ + å¼·åŒ–å­¦ç¿’"""
        print("ğŸ”— ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰äºˆæ¸¬å®Ÿè¡Œä¸­...")
        
        # é¡ä¼¼åº¦ãƒ™ãƒ¼ã‚¹äºˆæ¸¬
        similarity_predictions = {}
        for task_data in test_data:
            task_id = task_data.get('id') or task_data.get('number')
            if not task_id:
                continue
            
            # ã‚·ãƒ³ãƒ—ãƒ«ãªé¡ä¼¼åº¦ãƒ™ãƒ¼ã‚¹äºˆæ¸¬ï¼ˆä¾‹ï¼‰
            best_dev = list(self.similarity_model.developer_embeddings.keys())[0]
            similarity_predictions[task_id] = {
                'dev': best_dev,
                'score': 0.5
            }
        
        # å¼·åŒ–å­¦ç¿’äºˆæ¸¬
        rl_predictions, rl_scores = self.predict_with_rl(test_data)
        
        # ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰çµ±åˆ
        final_predictions = {}
        final_scores = {}
        
        for task_id in set(similarity_predictions.keys()) & set(rl_predictions.keys()):
            sim_pred = similarity_predictions[task_id]
            rl_pred = rl_predictions[task_id]
            
            # é‡ã¿ä»˜ãçµ±åˆ
            sim_score = sim_pred['score']
            rl_confidence = rl_scores[task_id]['rl_confidence']
            
            combined_score = alpha * sim_score + (1 - alpha) * rl_confidence
            
            # ã‚ˆã‚Šé«˜ã„ã‚¹ã‚³ã‚¢ã®äºˆæ¸¬ã‚’é¸æŠ
            if sim_score > rl_confidence:
                final_predictions[task_id] = sim_pred['dev']
                final_scores[task_id] = {
                    'predicted_dev': sim_pred['dev'],
                    'method': 'similarity',
                    'combined_score': combined_score,
                    'sim_score': sim_score,
                    'rl_confidence': rl_confidence
                }
            else:
                final_predictions[task_id] = rl_pred
                final_scores[task_id] = {
                    'predicted_dev': rl_pred,
                    'method': 'reinforcement_learning',
                    'combined_score': combined_score,
                    'sim_score': sim_score,
                    'rl_confidence': rl_confidence
                }
        
        print(f"   ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰äºˆæ¸¬å®Œäº†: {len(final_predictions)} ã‚¿ã‚¹ã‚¯")
        return final_predictions, final_scores
    
    def save_rl_model(self, output_dir="models"):
        """å¼·åŒ–å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜"""
        output_dir = Path(output_dir)
        output_dir.mkdir(exist_ok=True)
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # RLã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆä¿å­˜
        rl_model_path = output_dir / f"rl_enhanced_recommender_{timestamp}.zip"
        self.rl_agent.save(rl_model_path)
        
        print(f"âœ… RLæ‹¡å¼µãƒ¢ãƒ‡ãƒ«ä¿å­˜: {rl_model_path}")
        return rl_model_path


def main():
    parser = argparse.ArgumentParser(description='å¼·åŒ–å­¦ç¿’æ‹¡å¼µã‚¿ã‚¹ã‚¯æ¨è–¦ã‚·ã‚¹ãƒ†ãƒ ')
    parser.add_argument('--config', default='configs/unified_rl.yaml',
                       help='è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹')
    parser.add_argument('--data', default='data/backlog.json',
                       help='çµ±åˆãƒ‡ãƒ¼ã‚¿ãƒ‘ã‚¹')
    parser.add_argument('--similarity_model', required=True,
                       help='å­¦ç¿’æ¸ˆã¿é¡ä¼¼åº¦ãƒ¢ãƒ‡ãƒ«ãƒ‘ã‚¹')
    parser.add_argument('--train_rl', action='store_true',
                       help='å¼·åŒ–å­¦ç¿’ãƒãƒªã‚·ãƒ¼ã‚’è¨“ç·´')
    parser.add_argument('--output', default='outputs',
                       help='å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª')
    
    args = parser.parse_args()
    
    # é¡ä¼¼åº¦ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿
    print("ğŸ“š é¡ä¼¼åº¦ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ä¸­...")
    with open(args.similarity_model, 'rb') as f:
        similarity_model_data = pickle.load(f)
    
    # ãƒ¢ãƒ‡ãƒ«å¾©å…ƒï¼ˆç°¡ç•¥åŒ–ï¼‰
    class SimilarityModel:
        def __init__(self, data):
            self.__dict__.update(data)
    
    similarity_model = SimilarityModel(similarity_model_data)
    
    # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    with open(args.data, 'r', encoding='utf-8') as f:
        all_data = json.load(f)
    
    training_data = [task for task in all_data if task.get('created_at', '').startswith('2022')]
    test_data = [task for task in all_data if task.get('created_at', '').startswith('2023')]
    
    # å¼·åŒ–å­¦ç¿’æ‹¡å¼µã‚·ã‚¹ãƒ†ãƒ 
    rl_recommender = RLEnhancedRecommender(similarity_model, args.config)
    
    if args.train_rl:
        # å¼·åŒ–å­¦ç¿’è¨“ç·´
        rl_recommender.train_rl_policy(training_data)
        
        # ãƒ¢ãƒ‡ãƒ«ä¿å­˜
        rl_recommender.save_rl_model()
    
    # ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰äºˆæ¸¬
    predictions, scores = rl_recommender.hybrid_predict(test_data)
    
    print("\nğŸ¯ å¼·åŒ–å­¦ç¿’æ‹¡å¼µã‚·ã‚¹ãƒ†ãƒ å®Œäº†")
    print(f"   äºˆæ¸¬æ•°: {len(predictions)}")
    
    return 0


if __name__ == "__main__":
    exit(main())
