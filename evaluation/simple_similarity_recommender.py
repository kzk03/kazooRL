#!/usr/bin/env python3
"""
ã‚·ãƒ³ãƒ—ãƒ«ãªé¡ä¼¼åº¦ãƒ™ãƒ¼ã‚¹ã®ã‚¿ã‚¹ã‚¯æ¨è–¦ã‚·ã‚¹ãƒ†ãƒ 

- åŸºæœ¬çš„ãªãƒ†ã‚­ã‚¹ãƒˆãƒ™ãƒ¼ã‚¹ã®ç‰¹å¾´é‡ã®ã¿ä½¿ç”¨
- TF-IDFã¨ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦ã§æ¨è–¦
- é–‹ç™ºè€…ã®å±¥æ­´ãƒ™ãƒ¼ã‚¹ã§é¡ä¼¼åº¦è¨ˆç®—
- æ™‚ç³»åˆ—ãƒ™ãƒ¼ã‚¹ã®å‹•çš„å€™è£œãƒ—ãƒ¼ãƒ«å¯¾å¿œ
"""

import argparse
import json
import pickle
import re
from collections import Counter, defaultdict
from datetime import datetime, timedelta
from pathlib import Path

import numpy as np
import pandas as pd
import yaml
from sklearn.ensemble import RandomForestClassifier
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.preprocessing import StandardScaler


class SimpleSimilarityRecommender:
    """ã‚·ãƒ³ãƒ—ãƒ«ãªé¡ä¼¼åº¦ãƒ™ãƒ¼ã‚¹ã®ã‚¿ã‚¹ã‚¯æ¨è–¦ã‚·ã‚¹ãƒ†ãƒ """

    def __init__(self, config_path):
        """
        Args:
            config_path: è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
        """
        self.config_path = config_path

        # è¨­å®šèª­ã¿è¾¼ã¿
        with open(config_path, "r", encoding="utf-8") as f:
            self.config = yaml.safe_load(f)

        # TF-IDFãƒ™ã‚¯ãƒˆãƒ©ã‚¤ã‚¶ãƒ¼
        self.vectorizer = TfidfVectorizer(
            max_features=1000, stop_words="english", ngram_range=(1, 2)
        )

        # å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ä¿å­˜ç”¨
        self.trained_models = {}
        self.developer_profiles = {}
        self.scaler = StandardScaler()

        # æ™‚ç³»åˆ—ãƒ™ãƒ¼ã‚¹ã®é–‹ç™ºè€…ã‚¢ã‚¯ãƒ†ã‚£ãƒ“ãƒ†ã‚£ãƒ‡ãƒ¼ã‚¿
        self.developer_activity_timeline = {}
        self.monthly_active_developers = defaultdict(set)

        print("ğŸ¯ ã‚·ãƒ³ãƒ—ãƒ«é¡ä¼¼åº¦ãƒ™ãƒ¼ã‚¹ã®æ¨è–¦ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å®Œäº†")

    def load_data(self, data_path):
        """ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã‚“ã§æ™‚ç³»åˆ—åˆ†å‰²"""
        print("ğŸ“Š ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ä¸­...")

        with open(data_path, "r", encoding="utf-8") as f:
            all_data = json.load(f)

        # æ™‚ç³»åˆ—åˆ†å‰²
        training_data = []  # 2022å¹´ä»¥å‰
        test_data = []  # 2023å¹´

        for task in all_data:
            created_at = task.get("created_at", "")
            if created_at.startswith("2023"):
                test_data.append(task)
            elif created_at:  # 2022å¹´ä»¥å‰ã®å…¨ãƒ‡ãƒ¼ã‚¿
                year = int(created_at[:4])
                if year <= 2022:
                    training_data.append(task)
            elif created_at:  # 2022å¹´ä»¥å‰ã®å…¨ãƒ‡ãƒ¼ã‚¿
                year = int(created_at[:4])
                if year <= 2022:
                    training_data.append(task)

        print(f"   å­¦ç¿’ãƒ‡ãƒ¼ã‚¿: {len(training_data):,} ã‚¿ã‚¹ã‚¯ (2014-2022å¹´)")
        print(f"   ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿: {len(test_data):,} ã‚¿ã‚¹ã‚¯ (2023å¹´)")

        return training_data, test_data

    def extract_developers_from_task(self, task_data, method="all"):
        """
        ã‚¿ã‚¹ã‚¯ã‹ã‚‰é–‹ç™ºè€…ã‚’æŠ½å‡ºï¼ˆè¤‡æ•°ã®æ–¹æ³•ã«å¯¾å¿œï¼‰

        Args:
            task_data: ã‚¿ã‚¹ã‚¯ãƒ‡ãƒ¼ã‚¿
            method: æŠ½å‡ºæ–¹æ³• ('assignees', 'creators', 'all')
                   - 'assignees': assigneesãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã®ã¿ï¼ˆå¾“æ¥ã®æ–¹æ³•ï¼‰
                   - 'creators': Issue/PRä½œæˆè€…ï¼ˆuserãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ï¼‰ã‚‚å«ã‚€
                   - 'all': ã™ã¹ã¦ã®æ–¹æ³•ã‚’çµ±åˆ

        Returns:
            list: æŠ½å‡ºã•ã‚ŒãŸé–‹ç™ºè€…ã®ãƒªã‚¹ãƒˆï¼ˆpriorityé †ï¼‰
        """
        developers = []

        if method in ["assignees", "all"]:
            # 1. Assigneesï¼ˆæœ€é«˜å„ªå…ˆåº¦ï¼‰
            if "assignees" in task_data and task_data["assignees"]:
                for assignee in task_data["assignees"]:
                    if "login" in assignee:
                        developers.append(
                            {
                                "login": assignee["login"],
                                "source": "assignees",
                                "priority": 1,
                            }
                        )

            # 2. Events (assigned) - å¾“æ¥ã®è£œå®Œæ–¹æ³•
            elif "events" in task_data:
                for event in task_data["events"]:
                    if event.get("event") == "assigned" and event.get("assignee"):
                        login = event["assignee"].get("login")
                        if login:
                            developers.append(
                                {
                                    "login": login,
                                    "source": "events_assigned",
                                    "priority": 2,
                                }
                            )
                            break

        if method in ["creators", "all"]:
            # 3. Issue/PRä½œæˆè€…ï¼ˆæ–°ã—ã„æ–¹æ³•ï¼‰
            if (
                "user" in task_data
                and task_data["user"]
                and "login" in task_data["user"]
            ):
                user_login = task_data["user"]["login"]
                # æ—¢ã«ä»–ã®æ–¹æ³•ã§æŠ½å‡ºã•ã‚Œã¦ã„ãªã„å ´åˆã®ã¿è¿½åŠ 
                existing_logins = {dev["login"] for dev in developers}
                if user_login not in existing_logins:
                    developers.append(
                        {"login": user_login, "source": "user_creator", "priority": 3}
                    )

        return developers

    def extract_training_pairs(self, training_data, extraction_method="all"):
        """
        å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰é–‹ç™ºè€…-ã‚¿ã‚¹ã‚¯ãƒšã‚¢ã‚’æŠ½å‡ºï¼ˆè¤‡æ•°ã®æŠ½å‡ºæ–¹æ³•ã«å¯¾å¿œï¼‰

        Args:
            extraction_method: 'assignees' | 'creators' | 'all'
        """
        print(f"ğŸ” å­¦ç¿’ç”¨ãƒšã‚¢æŠ½å‡ºä¸­... (æ–¹æ³•: {extraction_method})")

        training_pairs = []
        developer_stats = Counter()
        extraction_stats = Counter()  # æŠ½å‡ºæ–¹æ³•ã®çµ±è¨ˆ

        for task_data in training_data:
            task_id = task_data.get("id") or task_data.get("number")
            if not task_id:
                continue

            # è¤‡æ•°ã®æ–¹æ³•ã§é–‹ç™ºè€…ã‚’æŠ½å‡º
            developers = self.extract_developers_from_task(
                task_data, method=extraction_method
            )

            # æœ€é«˜å„ªå…ˆåº¦ã®é–‹ç™ºè€…ã‚’é¸æŠ
            if developers:
                # å„ªå…ˆåº¦ã§ã‚½ãƒ¼ãƒˆ
                developers.sort(key=lambda x: x["priority"])
                selected_dev = developers[0]

                training_pairs.append(
                    {
                        "task_data": task_data,
                        "developer": selected_dev["login"],
                        "task_id": task_id,
                        "extraction_source": selected_dev["source"],
                    }
                )
                developer_stats[selected_dev["login"]] += 1
                extraction_stats[selected_dev["source"]] += 1

        print(f"   å­¦ç¿’ãƒšã‚¢: {len(training_pairs):,} ãƒšã‚¢")
        print(f"   ãƒ¦ãƒ‹ãƒ¼ã‚¯é–‹ç™ºè€…: {len(developer_stats)} äºº")

        # æŠ½å‡ºæ–¹æ³•ã®çµ±è¨ˆ
        print("   æŠ½å‡ºæ–¹æ³•åˆ¥çµ±è¨ˆ:")
        for source, count in extraction_stats.most_common():
            print(f"     {source}: {count} ãƒšã‚¢ ({count/len(training_pairs)*100:.1f}%)")

        # ä¸Šä½é–‹ç™ºè€…è¡¨ç¤º
        top_devs = developer_stats.most_common(10)
        print("   ä¸Šä½é–‹ç™ºè€…:")
        for dev, count in top_devs:
            print(f"     {dev}: {count} ã‚¿ã‚¹ã‚¯")

        return training_pairs, developer_stats

    def build_developer_activity_timeline(self, training_data, extraction_method="all"):
        """
        å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰é–‹ç™ºè€…ã®æ™‚ç³»åˆ—ã‚¢ã‚¯ãƒ†ã‚£ãƒ“ãƒ†ã‚£ã‚’æ§‹ç¯‰

        Args:
            extraction_method: é–‹ç™ºè€…æŠ½å‡ºæ–¹æ³•ï¼ˆ'assignees', 'creators', 'all'ï¼‰

        ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ã®å®šç¾©:
        1. assignees: ã‚¿ã‚¹ã‚¯ã«æ­£å¼ã«å‰²ã‚Šå½“ã¦ã‚‰ã‚ŒãŸé–‹ç™ºè€…
        2. creators: Issue/PRä½œæˆè€…ã‚‚å«ã‚€
        3. all: ã™ã¹ã¦ã®é–¢ä¸å½¢æ…‹ã‚’çµ±åˆ
        """
        print(
            f"ğŸ“… é–‹ç™ºè€…ã‚¢ã‚¯ãƒ†ã‚£ãƒ“ãƒ†ã‚£ã‚¿ã‚¤ãƒ ãƒ©ã‚¤ãƒ³æ§‹ç¯‰ä¸­... (æ–¹æ³•: {extraction_method})"
        )

        # æœˆåˆ¥é–‹ç™ºè€…ã‚¢ã‚¯ãƒ†ã‚£ãƒ“ãƒ†ã‚£
        monthly_activity = defaultdict(lambda: defaultdict(int))
        total_assignments = 0
        extraction_stats = Counter()

        for task_data in training_data:
            created_at = task_data.get("created_at", "")
            if not created_at:
                continue

            # æœˆã‚’æŠ½å‡º (YYYY-MM)
            try:
                month = created_at[:7]
            except:
                continue

            # æ–°ã—ã„æŠ½å‡ºæ–¹æ³•ã‚’ä½¿ç”¨
            developers = self.extract_developers_from_task(
                task_data, method=extraction_method
            )

            if developers:
                # æœ€é«˜å„ªå…ˆåº¦ã®é–‹ç™ºè€…ã‚’é¸æŠ
                developers.sort(key=lambda x: x["priority"])
                selected_dev = developers[0]

                assignee = selected_dev["login"]
                monthly_activity[month][assignee] += 1
                self.monthly_active_developers[month].add(assignee)
                total_assignments += 1
                extraction_stats[selected_dev["source"]] += 1

        # ã‚¢ã‚¯ãƒ†ã‚£ãƒ“ãƒ†ã‚£ã‚¿ã‚¤ãƒ ãƒ©ã‚¤ãƒ³ã‚’ä¿å­˜
        self.developer_activity_timeline = dict(monthly_activity)

        print(
            f"   ã‚¢ã‚¯ãƒ†ã‚£ãƒ“ãƒ†ã‚£ã‚¿ã‚¤ãƒ ãƒ©ã‚¤ãƒ³: {len(self.developer_activity_timeline)} ãƒ¶æœˆ"
        )
        print(f"   ç·å‰²ã‚Šå½“ã¦æ•°: {total_assignments:,}")

        # æŠ½å‡ºæ–¹æ³•ã®çµ±è¨ˆè¡¨ç¤º
        print("   æŠ½å‡ºæ–¹æ³•åˆ¥çµ±è¨ˆ:")
        for source, count in extraction_stats.most_common():
            print(f"     {source}: {count} ã‚¿ã‚¹ã‚¯ ({count/total_assignments*100:.1f}%)")

        # æœˆåˆ¥çµ±è¨ˆè¡¨ç¤ºï¼ˆå…¨æœŸé–“ï¼‰
        print("   æœˆåˆ¥ã‚¢ã‚¯ãƒ†ã‚£ãƒ“ãƒ†ã‚£:")
        for month in sorted(self.developer_activity_timeline.keys()):
            active_devs = len(self.monthly_active_developers[month])
            total_tasks = sum(self.developer_activity_timeline[month].values())
            print(f"     {month}: {active_devs} é–‹ç™ºè€…, {total_tasks} ã‚¿ã‚¹ã‚¯")

        return self.developer_activity_timeline

    def get_active_developers_for_date(self, target_date, lookback_months=6):
        """
        æŒ‡å®šæ—¥æ™‚ã‹ã‚‰éå»N ãƒ¶æœˆé–“ã§ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ã ã£ãŸé–‹ç™ºè€…ãƒªã‚¹ãƒˆã‚’å–å¾—

        ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ã®å®šç¾©:
        - ã‚¿ã‚¹ã‚¯ã«å‰²ã‚Šå½“ã¦ã‚‰ã‚ŒãŸï¼ˆassigneeã¾ãŸã¯assigned eventï¼‰
        - æŒ‡å®šæœŸé–“å†…ã«1ã¤ä»¥ä¸Šã®ã‚¿ã‚¹ã‚¯ã‚’æ‹…å½“
        - å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ï¼ˆ2022å¹´ï¼‰ã§ã®æ´»å‹•å±¥æ­´ãŒã‚ã‚‹
        """
        try:
            # æ—¥æ™‚ãƒ‘ãƒ¼ã‚¹
            if isinstance(target_date, str):
                target_dt = datetime.fromisoformat(target_date.replace("Z", "+00:00"))
            else:
                target_dt = target_date

            # æ¤œç´¢ç¯„å›²ã‚’è¨ˆç®—ï¼ˆã‚ˆã‚ŠæŸ”è»Ÿãªæœˆè¨ˆç®—ï¼‰
            active_developers = set()

            for i in range(lookback_months + 1):  # å½“æœˆã‚’å«ã‚€
                # ã‚ˆã‚Šæ­£ç¢ºãªæœˆè¨ˆç®—
                if target_dt.month - i <= 0:
                    # å‰å¹´ã«é¡ã‚‹
                    year = target_dt.year - 1
                    month = 12 + (target_dt.month - i)
                else:
                    year = target_dt.year
                    month = target_dt.month - i

                search_month = f"{year:04d}-{month:02d}"

                if search_month in self.monthly_active_developers:
                    devs_in_month = self.monthly_active_developers[search_month]
                    active_developers.update(devs_in_month)
                    print(f"   {search_month}: {len(devs_in_month)} ã‚¢ã‚¯ãƒ†ã‚£ãƒ–é–‹ç™ºè€…")

            return list(active_developers)

        except Exception as e:
            print(f"âš ï¸ æ—¥æ™‚å‡¦ç†ã‚¨ãƒ©ãƒ¼ ({target_date}): {e}")
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: å…¨é–‹ç™ºè€…ã‚’è¿”ã™
            all_devs = set()
            for month_devs in self.monthly_active_developers.values():
                all_devs.update(month_devs)
            return list(all_devs)

    def create_temporal_candidate_pool(self, task_data, learned_profiles):
        """ãƒ†ã‚¹ãƒˆã‚¿ã‚¹ã‚¯ã®ä½œæˆæ—¥æ™‚ãƒ™ãƒ¼ã‚¹ã§å‹•çš„å€™è£œãƒ—ãƒ¼ãƒ«ã‚’ä½œæˆ"""
        created_at = task_data.get("created_at", "")
        if not created_at:
            # ä½œæˆæ—¥æ™‚ãŒä¸æ˜ãªå ´åˆã¯å…¨é–‹ç™ºè€…ã‚’å€™è£œã«ã™ã‚‹
            return learned_profiles

        # ãã®æ™‚ç‚¹ã§ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ã ã£ãŸé–‹ç™ºè€…ã‚’å–å¾—ï¼ˆç›´è¿‘6ãƒ¶æœˆï¼‰
        active_developers = self.get_active_developers_for_date(
            created_at, lookback_months=6
        )

        print(
            f"   ğŸ“… {created_at[:10]} æ™‚ç‚¹ã®ç›´è¿‘6ãƒ¶æœˆã‚¢ã‚¯ãƒ†ã‚£ãƒ–é–‹ç™ºè€…: {len(active_developers)} äºº"
        )

        # ã‚¢ã‚¯ãƒ†ã‚£ãƒ–é–‹ç™ºè€…ã®ä¸­ã§å­¦ç¿’æ¸ˆã¿ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚ã‚‹é–‹ç™ºè€…
        temporal_pool = {}
        for dev_name in active_developers:
            if dev_name in learned_profiles:
                temporal_pool[dev_name] = learned_profiles[dev_name]
                temporal_pool[dev_name]["temporal_active"] = True
            else:
                # å­¦ç¿’æ¸ˆã¿ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ãŒãªã„å ´åˆã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ
                temporal_pool[dev_name] = self._create_default_profile(dev_name)
                temporal_pool[dev_name]["temporal_active"] = True

        # å€™è£œãŒå°‘ãªã™ãã‚‹å ´åˆã¯æ‹¡å¼µï¼ˆã‚ˆã‚Šé•·æœŸé–“ã¾ã§é¡ã‚‹ï¼‰
        if len(temporal_pool) < 3:
            print(f"âš ï¸ å€™è£œãƒ—ãƒ¼ãƒ«ãŒå°ã•ã™ãã¾ã™ ({len(temporal_pool)} äºº), æ‹¡å¼µä¸­...")
            # ã‚ˆã‚Šé•·æœŸé–“ã‹ã‚‰è¿½åŠ ã§é–‹ç™ºè€…ã‚’æ¢ã™
            for i in range(7, 12):  # 7-11ãƒ¶æœˆå‰ã¾ã§æ‹¡å¼µï¼ˆæœ€å¤§1å¹´å‰ï¼‰
                extra_devs = self.get_active_developers_for_date(
                    created_at, lookback_months=i
                )
                for dev_name in extra_devs:
                    if dev_name not in temporal_pool:
                        if dev_name in learned_profiles:
                            temporal_pool[dev_name] = learned_profiles[dev_name]
                            temporal_pool[dev_name][
                                "temporal_active"
                            ] = False  # é–“æ¥çš„ã«ã‚¢ã‚¯ãƒ†ã‚£ãƒ–
                        else:
                            temporal_pool[dev_name] = self._create_default_profile(
                                dev_name
                            )
                            temporal_pool[dev_name]["temporal_active"] = False

                if len(temporal_pool) >= 5:  # æœ€ä½5äººã‚’ç¢ºä¿
                    print(f"   æ‹¡å¼µå®Œäº†: {len(temporal_pool)} äºº ({i}ãƒ¶æœˆå‰ã¾ã§æ¤œç´¢)")
                    break

        return temporal_pool

    def extract_task_text(self, task_data):
        """ã‚¿ã‚¹ã‚¯ã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡º"""
        # ãƒ‡ãƒ¼ã‚¿å‹ãƒã‚§ãƒƒã‚¯
        if isinstance(task_data, str):
            return task_data  # æ–‡å­—åˆ—ã®å ´åˆã¯ãã®ã¾ã¾è¿”ã™
        elif not isinstance(task_data, dict):
            return ""  # è¾æ›¸ã§ãªã„å ´åˆã¯ç©ºæ–‡å­—ã‚’è¿”ã™

        text_parts = []

        # ã‚¿ã‚¤ãƒˆãƒ«
        if "title" in task_data:
            text_parts.append(task_data["title"])

        # æœ¬æ–‡
        body = task_data.get("body", "") or ""  # Noneã®å ´åˆã‚‚ç©ºæ–‡å­—åˆ—ã«
        if body:
            # ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ã‚„HTMLã‚¿ã‚°ã‚’ç°¡å˜ã«é™¤å»
            body = re.sub(r"<[^>]*>", "", body)
            body = re.sub(r"```.*?```", "", body, flags=re.DOTALL)
            body = re.sub(r"`[^`]*`", "", body)
            text_parts.append(body)

        # ãƒ©ãƒ™ãƒ«
        if "labels" in task_data:
            labels = [
                label.get("name", "") if isinstance(label, dict) else str(label)
                for label in task_data["labels"]
            ]
            text_parts.extend(labels)

        return " ".join(text_parts)

    def extract_basic_features(self, task_data):
        """åŸºæœ¬çš„ãªç‰¹å¾´é‡ã‚’æŠ½å‡º"""
        # ãƒ‡ãƒ¼ã‚¿å‹ãƒã‚§ãƒƒã‚¯
        if not isinstance(task_data, dict):
            return {}  # è¾æ›¸ã§ãªã„å ´åˆã¯ç©ºã®ç‰¹å¾´é‡ã‚’è¿”ã™

        features = {}

        # ã‚¿ã‚¹ã‚¯ã®åŸºæœ¬æƒ…å ±
        features["title_length"] = len(task_data.get("title", ""))
        body = task_data.get("body", "") or ""  # Noneã®å ´åˆã‚‚ç©ºæ–‡å­—åˆ—ã«
        features["body_length"] = len(body)
        features["comments_count"] = task_data.get("comments", 0)

        # ãƒ©ãƒ™ãƒ«ãƒ™ãƒ¼ã‚¹ã®ç‰¹å¾´é‡
        labels = []
        if "labels" in task_data and task_data["labels"]:
            for label in task_data["labels"]:
                if isinstance(label, dict):
                    labels.append(label.get("name", "").lower())
                elif isinstance(label, str):
                    labels.append(label.lower())

        features["is_bug"] = 1 if any("bug" in label for label in labels) else 0
        features["is_enhancement"] = (
            1
            if any("enhancement" in label or "feature" in label for label in labels)
            else 0
        )
        features["is_documentation"] = (
            1 if any("doc" in label for label in labels) else 0
        )
        features["is_question"] = (
            1 if any("question" in label for label in labels) else 0
        )
        features["is_help_wanted"] = (
            1 if any("help" in label for label in labels) else 0
        )
        features["label_count"] = len(labels)

        # çŠ¶æ…‹
        features["is_open"] = 1 if task_data.get("state") == "open" else 0

        return features

    def build_developer_profiles(self, training_pairs):
        """é–‹ç™ºè€…ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ§‹ç¯‰"""
        print("ğŸ‘¥ é–‹ç™ºè€…ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«æ§‹ç¯‰ä¸­...")

        developer_profiles = defaultdict(
            lambda: {
                "task_count": 0,
                "task_texts": [],
                "feature_sums": defaultdict(float),
                "label_preferences": defaultdict(int),
            }
        )

        for pair in training_pairs:
            developer = pair["developer"]
            task_data = pair["task_data"]

            # ãƒ†ã‚­ã‚¹ãƒˆåé›†
            task_text = self.extract_task_text(task_data)
            developer_profiles[developer]["task_texts"].append(task_text)

            # åŸºæœ¬ç‰¹å¾´é‡é›†è¨ˆ
            features = self.extract_basic_features(task_data)
            for key, value in features.items():
                developer_profiles[developer]["feature_sums"][key] += value

            # ãƒ©ãƒ™ãƒ«å—œå¥½
            labels = []
            if "labels" in task_data and task_data["labels"]:
                for label in task_data["labels"]:
                    if isinstance(label, dict):
                        labels.append(label.get("name", "").lower())
                    elif isinstance(label, str):
                        labels.append(label.lower())

            for label in labels:
                developer_profiles[developer]["label_preferences"][label] += 1

            developer_profiles[developer]["task_count"] += 1

        # å¹³å‡ç‰¹å¾´é‡ã‚’è¨ˆç®—
        for dev_name, profile in developer_profiles.items():
            if profile["task_count"] > 0:
                for key, total in profile["feature_sums"].items():
                    profile[f"avg_{key}"] = total / profile["task_count"]

        print(f"   é–‹ç™ºè€…ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«å®Œäº†: {len(developer_profiles)} äºº")
        return dict(developer_profiles)

    def train_text_similarity_model(self, developer_profiles):
        """ãƒ†ã‚­ã‚¹ãƒˆãƒ™ãƒ¼ã‚¹ã®é¡ä¼¼åº¦ãƒ¢ãƒ‡ãƒ«ã‚’è¨“ç·´"""
        print("ğŸ“ ãƒ†ã‚­ã‚¹ãƒˆé¡ä¼¼åº¦ãƒ¢ãƒ‡ãƒ«è¨“ç·´ä¸­...")

        # å„é–‹ç™ºè€…ã®çµ±åˆãƒ†ã‚­ã‚¹ãƒˆã‚’ä½œæˆ
        dev_texts = []
        dev_names = []

        for dev_name, profile in developer_profiles.items():
            if profile["task_texts"]:
                combined_text = " ".join(profile["task_texts"])
                dev_texts.append(combined_text)
                dev_names.append(dev_name)

        # TF-IDFãƒ™ã‚¯ãƒˆãƒ«åŒ–
        if dev_texts:
            self.tfidf_matrix = self.vectorizer.fit_transform(dev_texts)
            self.dev_names = dev_names

            print(f"   TF-IDFè¡Œåˆ—: {self.tfidf_matrix.shape}")
            print(f"   èªå½™ã‚µã‚¤ã‚º: {len(self.vectorizer.vocabulary_)}")
        else:
            print("âš ï¸ ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ãŒä¸è¶³ã—ã¦ã„ã¾ã™")

    def train_feature_model(self, training_pairs):
        """ç‰¹å¾´é‡ãƒ™ãƒ¼ã‚¹ã®ãƒ¢ãƒ‡ãƒ«ã‚’è¨“ç·´"""
        print("ğŸ”§ ç‰¹å¾´é‡ãƒ¢ãƒ‡ãƒ«è¨“ç·´ä¸­...")

        features = []
        labels = []

        for pair in training_pairs:
            task_features = self.extract_basic_features(pair["task_data"])
            feature_vector = [
                task_features[key] for key in sorted(task_features.keys())
            ]

            features.append(feature_vector)
            labels.append(pair["developer"])

        if features:
            features = np.array(features)

            # æ­£è¦åŒ–
            features_scaled = self.scaler.fit_transform(features)

            # RandomForeståˆ†é¡å™¨
            self.rf_classifier = RandomForestClassifier(
                n_estimators=100, max_depth=10, random_state=42, n_jobs=-1
            )

            self.rf_classifier.fit(features_scaled, labels)

            print(f"   ç‰¹å¾´é‡ãƒ¢ãƒ‡ãƒ«è¨“ç·´å®Œäº†: {len(set(labels))} ã‚¯ãƒ©ã‚¹")

            # ç‰¹å¾´é‡è¦åº¦
            feature_names = sorted(
                self.extract_basic_features(training_pairs[0]["task_data"]).keys()
            )
            importances = self.rf_classifier.feature_importances_

            print("   ç‰¹å¾´é‡è¦åº¦ Top-5:")
            for i, (feat, imp) in enumerate(zip(feature_names, importances)):
                if i < 5:
                    print(f"     {feat}: {imp:.3f}")
        else:
            print("âš ï¸ ç‰¹å¾´é‡ãƒ‡ãƒ¼ã‚¿ãŒä¸è¶³ã—ã¦ã„ã¾ã™")

    def predict_assignments(self, test_data, developer_profiles):
        """ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®å‰²ã‚Šå½“ã¦ã‚’äºˆæ¸¬"""
        print("ğŸ¤– å‰²ã‚Šå½“ã¦äºˆæ¸¬ä¸­...")

        predictions = {}
        prediction_scores = {}

        # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®å®Ÿéš›ã®å‰²ã‚Šå½“ã¦ã‚’æŠ½å‡º
        test_assignments = {}
        for task_data in test_data:
            task_id = task_data.get("id") or task_data.get("number")
            if not task_id:
                continue

            assignee = None
            if "assignees" in task_data and task_data["assignees"]:
                assignee = task_data["assignees"][0].get("login")
            elif "events" in task_data:
                for event in task_data["events"]:
                    if event.get("event") == "assigned" and event.get("assignee"):
                        assignee = event["assignee"].get("login")
                        break

            if assignee:
                test_assignments[task_id] = assignee

        print(f"   äºˆæ¸¬å¯¾è±¡: {len(test_assignments)} ã‚¿ã‚¹ã‚¯")

        prediction_count = 0

        # å„ã‚¿ã‚¹ã‚¯ã®äºˆæ¸¬
        for task_data in test_data:
            task_id = task_data.get("id") or task_data.get("number")
            if task_id not in test_assignments:
                continue

            try:
                # ãƒ†ã‚­ã‚¹ãƒˆé¡ä¼¼åº¦ã«ã‚ˆã‚‹äºˆæ¸¬
                text_scores = self._predict_by_text_similarity(task_data)

                # ç‰¹å¾´é‡é¡ä¼¼åº¦ã«ã‚ˆã‚‹äºˆæ¸¬
                feature_scores = self._predict_by_features(task_data)

                # ä¸¡æ–¹ã®äºˆæ¸¬ã‚’çµ±åˆ
                combined_scores = self._combine_predictions(text_scores, feature_scores)

                if combined_scores:
                    # ã‚¹ã‚³ã‚¢é †ã«ã‚½ãƒ¼ãƒˆ
                    sorted_scores = sorted(
                        combined_scores.items(), key=lambda x: x[1], reverse=True
                    )

                    # æœ€é«˜ã‚¹ã‚³ã‚¢ã®é–‹ç™ºè€…ã‚’äºˆæ¸¬
                    best_dev = sorted_scores[0][0]

                    predictions[task_id] = best_dev
                    prediction_scores[task_id] = {
                        "predicted_dev": best_dev,
                        "combined_score": sorted_scores[0][1],
                        "text_score": text_scores.get(best_dev, 0.0),
                        "feature_score": feature_scores.get(best_dev, 0.0),
                        "all_scores": dict(
                            sorted_scores
                        ),  # å…¨é–‹ç™ºè€…ã®ã‚¹ã‚³ã‚¢ï¼ˆã‚½ãƒ¼ãƒˆæ¸ˆã¿ï¼‰
                    }

                    prediction_count += 1

                    if prediction_count % 100 == 0:
                        print(f"   é€²æ—: {prediction_count}/{len(test_assignments)}")

            except Exception as e:
                print(f"âš ï¸ ã‚¿ã‚¹ã‚¯ {task_id} ã®äºˆæ¸¬ã‚¨ãƒ©ãƒ¼: {e}")
                continue

        print(f"   äºˆæ¸¬å®Œäº†: {len(predictions)} ã‚¿ã‚¹ã‚¯")
        return predictions, prediction_scores, test_assignments

    def predict_assignments_with_pool(
        self, test_data, developer_profiles, test_assignments
    ):
        """ãƒ†ã‚¹ãƒˆãƒ™ãƒ¼ã‚¹å€™è£œãƒ—ãƒ¼ãƒ«ã§ã®å‰²ã‚Šå½“ã¦äºˆæ¸¬"""
        print("ğŸ¤– ãƒ†ã‚¹ãƒˆãƒ™ãƒ¼ã‚¹å‰²ã‚Šå½“ã¦äºˆæ¸¬ä¸­...")

        predictions = {}
        prediction_scores = {}

        print(f"   äºˆæ¸¬å¯¾è±¡: {len(test_assignments)} ã‚¿ã‚¹ã‚¯")
        print(f"   å€™è£œé–‹ç™ºè€…: {len(developer_profiles)} äºº")

        prediction_count = 0

        # å„ã‚¿ã‚¹ã‚¯ã®äºˆæ¸¬
        for task_data in test_data:
            # ãƒ‡ãƒ¼ã‚¿å‹ãƒã‚§ãƒƒã‚¯
            if isinstance(task_data, str):
                print(f"âš ï¸ ã‚¿ã‚¹ã‚¯ãƒ‡ãƒ¼ã‚¿ãŒæ–‡å­—åˆ—å½¢å¼: {task_data[:100]}...")
                continue
            elif not isinstance(task_data, dict):
                print(f"âš ï¸ ã‚¿ã‚¹ã‚¯ãƒ‡ãƒ¼ã‚¿ãŒè¾æ›¸å½¢å¼ã§ã¯ã‚ã‚Šã¾ã›ã‚“: {type(task_data)}")
                continue

            task_id = task_data.get("id") or task_data.get("number")
            if task_id not in test_assignments:
                continue

            try:
                # ãƒ†ã‚­ã‚¹ãƒˆé¡ä¼¼åº¦ã«ã‚ˆã‚‹äºˆæ¸¬
                text_scores = self._predict_by_text_similarity(task_data)

                # ç‰¹å¾´é‡é¡ä¼¼åº¦ã«ã‚ˆã‚‹äºˆæ¸¬
                feature_scores = self._predict_by_features(task_data)

                # ä¸¡æ–¹ã®äºˆæ¸¬ã‚’çµ±åˆ
                combined_scores = self._combine_predictions(text_scores, feature_scores)

                if combined_scores:
                    # ã‚¹ã‚³ã‚¢é †ã«ã‚½ãƒ¼ãƒˆ
                    sorted_scores = sorted(
                        combined_scores.items(), key=lambda x: x[1], reverse=True
                    )

                    # æœ€é«˜ã‚¹ã‚³ã‚¢ã®é–‹ç™ºè€…ã‚’äºˆæ¸¬
                    best_dev = sorted_scores[0][0]

                    predictions[task_id] = best_dev
                    prediction_scores[task_id] = {
                        "predicted_dev": best_dev,
                        "combined_score": sorted_scores[0][1],
                        "text_score": text_scores.get(best_dev, 0.0),
                        "feature_score": feature_scores.get(best_dev, 0.0),
                        "all_scores": dict(
                            sorted_scores
                        ),  # å…¨é–‹ç™ºè€…ã®ã‚¹ã‚³ã‚¢ï¼ˆã‚½ãƒ¼ãƒˆæ¸ˆã¿ï¼‰
                    }

                    prediction_count += 1

                    if prediction_count % 100 == 0:
                        print(f"   é€²æ—: {prediction_count}/{len(test_assignments)}")

            except Exception as e:
                print(f"âš ï¸ ã‚¿ã‚¹ã‚¯ {task_id} ã®äºˆæ¸¬ã‚¨ãƒ©ãƒ¼: {e}")
                continue

        print(f"   äºˆæ¸¬å®Œäº†: {len(predictions)} ã‚¿ã‚¹ã‚¯")
        return predictions, prediction_scores, test_assignments

    def predict_assignments_temporal(self, test_data, learned_profiles):
        """æ™‚ç³»åˆ—ãƒ™ãƒ¼ã‚¹å€™è£œãƒ—ãƒ¼ãƒ«ã§ã®å‰²ã‚Šå½“ã¦äºˆæ¸¬"""
        print("ğŸ• æ™‚ç³»åˆ—ãƒ™ãƒ¼ã‚¹å‰²ã‚Šå½“ã¦äºˆæ¸¬ä¸­...")

        predictions = {}
        prediction_scores = {}

        # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®å®Ÿéš›ã®å‰²ã‚Šå½“ã¦ã‚’æŠ½å‡º
        test_assignments = {}
        for task_data in test_data:
            task_id = task_data.get("id") or task_data.get("number")
            if not task_id:
                continue

            assignee = None
            if "assignees" in task_data and task_data["assignees"]:
                assignee = task_data["assignees"][0].get("login")
            elif "events" in task_data:
                for event in task_data["events"]:
                    if event.get("event") == "assigned" and event.get("assignee"):
                        assignee = event["assignee"].get("login")
                        break

            if assignee:
                test_assignments[task_id] = assignee

        print(f"   äºˆæ¸¬å¯¾è±¡: {len(test_assignments)} ã‚¿ã‚¹ã‚¯")

        prediction_count = 0
        temporal_stats = {
            "avg_pool_size": 0,
            "min_pool_size": float("inf"),
            "max_pool_size": 0,
            "pool_sizes": [],
        }

        # å„ã‚¿ã‚¹ã‚¯ã®äºˆæ¸¬
        for task_data in test_data:
            # ãƒ‡ãƒ¼ã‚¿å‹ãƒã‚§ãƒƒã‚¯
            if isinstance(task_data, str):
                print(f"âš ï¸ ã‚¿ã‚¹ã‚¯ãƒ‡ãƒ¼ã‚¿ãŒæ–‡å­—åˆ—å½¢å¼: {task_data[:100]}...")
                continue
            elif not isinstance(task_data, dict):
                print(f"âš ï¸ ã‚¿ã‚¹ã‚¯ãƒ‡ãƒ¼ã‚¿ãŒè¾æ›¸å½¢å¼ã§ã¯ã‚ã‚Šã¾ã›ã‚“: {type(task_data)}")
                continue

            task_id = task_data.get("id") or task_data.get("number")
            if task_id not in test_assignments:
                continue

            try:
                # ãã®æ™‚ç‚¹ã®å‹•çš„å€™è£œãƒ—ãƒ¼ãƒ«ã‚’ä½œæˆ
                temporal_pool = self.create_temporal_candidate_pool(
                    task_data, learned_profiles
                )

                # çµ±è¨ˆåé›†
                pool_size = len(temporal_pool)
                temporal_stats["pool_sizes"].append(pool_size)
                temporal_stats["min_pool_size"] = min(
                    temporal_stats["min_pool_size"], pool_size
                )
                temporal_stats["max_pool_size"] = max(
                    temporal_stats["max_pool_size"], pool_size
                )

                # ä¸€æ™‚çš„ã«é–‹ç™ºè€…ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ã‚’è¨­å®š
                original_profiles = self.developer_profiles
                self.developer_profiles = temporal_pool

                # ãƒ†ã‚­ã‚¹ãƒˆé¡ä¼¼åº¦ã«ã‚ˆã‚‹äºˆæ¸¬
                text_scores = self._predict_by_text_similarity(task_data)

                # ç‰¹å¾´é‡é¡ä¼¼åº¦ã«ã‚ˆã‚‹äºˆæ¸¬
                feature_scores = self._predict_by_features(task_data)

                # å…ƒã®ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å¾©å…ƒ
                self.developer_profiles = original_profiles

                # ä¸¡æ–¹ã®äºˆæ¸¬ã‚’çµ±åˆ
                combined_scores = self._combine_predictions(text_scores, feature_scores)

                # æ™‚ç³»åˆ—ã‚¢ã‚¯ãƒ†ã‚£ãƒ“ãƒ†ã‚£ã«ã‚ˆã‚‹é‡ã¿ä»˜ã‘
                combined_scores = self._apply_temporal_weights(
                    combined_scores, temporal_pool, task_data
                )

                if combined_scores:
                    # ã‚¹ã‚³ã‚¢é †ã«ã‚½ãƒ¼ãƒˆ
                    sorted_scores = sorted(
                        combined_scores.items(), key=lambda x: x[1], reverse=True
                    )

                    # æœ€é«˜ã‚¹ã‚³ã‚¢ã®é–‹ç™ºè€…ã‚’äºˆæ¸¬
                    best_dev = sorted_scores[0][0]

                    predictions[task_id] = best_dev
                    prediction_scores[task_id] = {
                        "predicted_dev": best_dev,
                        "combined_score": sorted_scores[0][1],
                        "text_score": text_scores.get(best_dev, 0.0),
                        "feature_score": feature_scores.get(best_dev, 0.0),
                        "all_scores": dict(sorted_scores),
                        "pool_size": pool_size,
                        "temporal_active": temporal_pool.get(best_dev, {}).get(
                            "temporal_active", False
                        ),
                        "created_at": task_data.get("created_at", ""),
                        "candidate_pool": list(temporal_pool.keys()),
                    }

                    prediction_count += 1

                    if prediction_count % 100 == 0:
                        print(f"   é€²æ—: {prediction_count}/{len(test_assignments)}")

            except Exception as e:
                print(f"âš ï¸ ã‚¿ã‚¹ã‚¯ {task_id} ã®äºˆæ¸¬ã‚¨ãƒ©ãƒ¼: {e}")
                continue

        # çµ±è¨ˆè¨ˆç®—
        if temporal_stats["pool_sizes"]:
            temporal_stats["avg_pool_size"] = np.mean(temporal_stats["pool_sizes"])

        print(f"   äºˆæ¸¬å®Œäº†: {len(predictions)} ã‚¿ã‚¹ã‚¯")
        print(f"   å¹³å‡å€™è£œãƒ—ãƒ¼ãƒ«ã‚µã‚¤ã‚º: {temporal_stats['avg_pool_size']:.1f}")
        print(
            f"   å€™è£œãƒ—ãƒ¼ãƒ«ã‚µã‚¤ã‚ºç¯„å›²: {temporal_stats['min_pool_size']}-{temporal_stats['max_pool_size']}"
        )

        return predictions, prediction_scores, test_assignments, temporal_stats

    def _apply_temporal_weights(self, combined_scores, temporal_pool, task_data):
        """æ™‚ç³»åˆ—ã‚¢ã‚¯ãƒ†ã‚£ãƒ“ãƒ†ã‚£ã«ã‚ˆã‚‹é‡ã¿ä»˜ã‘ã‚’é©ç”¨"""
        weighted_scores = {}

        for dev_name, score in combined_scores.items():
            base_score = score

            # æ™‚ç³»åˆ—ã‚¢ã‚¯ãƒ†ã‚£ãƒ“ãƒ†ã‚£ã«ã‚ˆã‚‹é‡ã¿
            if dev_name in temporal_pool:
                dev_profile = temporal_pool[dev_name]

                # ç›´è¿‘ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãªé–‹ç™ºè€…ã«ãƒœãƒ¼ãƒŠã‚¹
                if dev_profile.get("temporal_active", False):
                    base_score *= 1.2

                # ã‚¢ã‚¯ãƒ†ã‚£ãƒ“ãƒ†ã‚£ãƒ¬ãƒ™ãƒ«ã«ã‚ˆã‚‹èª¿æ•´
                activity_level = dev_profile.get("task_count", 1)
                activity_boost = min(1.3, 1.0 + (activity_level / 50))
                base_score *= activity_boost

            weighted_scores[dev_name] = base_score

        return weighted_scores

    def _predict_by_text_similarity(self, task_data):
        """ãƒ†ã‚­ã‚¹ãƒˆé¡ä¼¼åº¦ã«ã‚ˆã‚‹äºˆæ¸¬"""
        scores = {}

        if hasattr(self, "tfidf_matrix") and hasattr(self, "dev_names"):
            task_text = self.extract_task_text(task_data)

            if task_text.strip():
                # ã‚¿ã‚¹ã‚¯ã®TF-IDFãƒ™ã‚¯ãƒˆãƒ«
                task_vector = self.vectorizer.transform([task_text])

                # é¡ä¼¼åº¦è¨ˆç®—
                similarities = cosine_similarity(
                    task_vector, self.tfidf_matrix
                ).flatten()

                for i, dev_name in enumerate(self.dev_names):
                    scores[dev_name] = similarities[i]

        return scores

    def _predict_by_features(self, task_data):
        """ç‰¹å¾´é‡ã«ã‚ˆã‚‹äºˆæ¸¬ï¼ˆæ‹¡å¼µå€™è£œãƒ—ãƒ¼ãƒ«å¯¾å¿œï¼‰"""
        scores = {}

        # ã‚¿ã‚¹ã‚¯ã®ç‰¹å¾´é‡ã‚’æŠ½å‡º
        task_features = self.extract_basic_features(task_data)

        # å…¨å€™è£œé–‹ç™ºè€…ã«å¯¾ã—ã¦ã‚¹ã‚³ã‚¢è¨ˆç®—
        for dev_name, profile in self.developer_profiles.items():
            if profile.get("source") == "training":
                # å­¦ç¿’æ¸ˆã¿é–‹ç™ºè€…: RandomForeståˆ†é¡å™¨ã‚’ä½¿ç”¨
                if hasattr(self, "rf_classifier"):
                    feature_vector = [
                        task_features[key] for key in sorted(task_features.keys())
                    ]
                    feature_vector = np.array(feature_vector).reshape(1, -1)
                    feature_vector_scaled = self.scaler.transform(feature_vector)

                    # äºˆæ¸¬ç¢ºç‡
                    probas = self.rf_classifier.predict_proba(feature_vector_scaled)[0]
                    classes = self.rf_classifier.classes_

                    for i, class_name in enumerate(classes):
                        if class_name == dev_name:
                            scores[dev_name] = probas[i]
                            break
                    else:
                        scores[dev_name] = 0.0
            else:
                # æ—¢å­˜ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«é–‹ç™ºè€…: ç‰¹å¾´é‡é¡ä¼¼åº¦ãƒ™ãƒ¼ã‚¹ã®äºˆæ¸¬
                similarity_score = self._calculate_feature_similarity(
                    task_features, profile
                )
                scores[dev_name] = similarity_score

        return scores

    def _calculate_feature_similarity(self, task_features, dev_profile):
        """ã‚¿ã‚¹ã‚¯ç‰¹å¾´é‡ã¨é–‹ç™ºè€…ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ã®é¡ä¼¼åº¦ã‚’è¨ˆç®—"""
        # åŸºæœ¬çš„ãªç‰¹å¾´é‡é¡ä¼¼åº¦è¨ˆç®—
        score = 0.0

        # ã‚¿ã‚¹ã‚¯ã®é•·ã•ç‰¹å¾´é‡ã¨ã®é¡ä¼¼åº¦
        title_len_diff = abs(
            task_features["title_length"] - dev_profile.get("avg_title_length", 50)
        )
        body_len_diff = abs(
            task_features["body_length"] - dev_profile.get("avg_body_length", 200)
        )
        comments_diff = abs(
            task_features["comments_count"] - dev_profile.get("avg_comments_count", 2)
        )

        # æ­£è¦åŒ–ã—ã¦é¡ä¼¼åº¦ã«å¤‰æ›ï¼ˆå·®ãŒå°ã•ã„ã»ã©é«˜ã‚¹ã‚³ã‚¢ï¼‰
        title_sim = max(0, 1 - title_len_diff / 100)
        body_sim = max(0, 1 - body_len_diff / 1000)
        comments_sim = max(0, 1 - comments_diff / 10)

        # ãƒ©ãƒ™ãƒ«è¦ªå’Œæ€§
        label_affinity = 0.0
        if task_features["is_bug"]:
            label_affinity += dev_profile.get("avg_is_bug", 0.1)
        if task_features["is_enhancement"]:
            label_affinity += dev_profile.get("avg_is_enhancement", 0.1)
        if task_features["is_documentation"]:
            label_affinity += dev_profile.get("avg_is_documentation", 0.05)

        # é‡ã¿ä»˜ãçµ±åˆã‚¹ã‚³ã‚¢
        score = (
            0.2 * title_sim + 0.3 * body_sim + 0.2 * comments_sim + 0.3 * label_affinity
        )

        # ã‚¢ã‚¯ãƒ†ã‚£ãƒ“ãƒ†ã‚£ãƒ¬ãƒ™ãƒ«ã«ã‚ˆã‚‹èª¿æ•´
        activity_boost = min(1.0, dev_profile.get("task_count", 1) / 10)
        score *= 0.5 + 0.5 * activity_boost

        return score

    def _combine_predictions(self, text_scores, feature_scores):
        """äºˆæ¸¬ã®çµ±åˆ"""
        combined_scores = {}

        # é‡ã¿è¨­å®š
        text_weight = 0.6
        feature_weight = 0.4

        # å…¨é–‹ç™ºè€…ã®ãƒªã‚¹ãƒˆ
        all_devs = set(text_scores.keys()) | set(feature_scores.keys())

        for dev in all_devs:
            text_score = text_scores.get(dev, 0.0)
            feature_score = feature_scores.get(dev, 0.0)

            combined_scores[dev] = (
                text_weight * text_score + feature_weight * feature_score
            )

        return combined_scores

    def evaluate_predictions(self, predictions, test_assignments, prediction_scores):
        """äºˆæ¸¬çµæœã®è©•ä¾¡"""
        print("ğŸ“Š äºˆæ¸¬è©•ä¾¡ä¸­...")

        # å…±é€šã‚¿ã‚¹ã‚¯ã§è©•ä¾¡
        common_tasks = set(predictions.keys()) & set(test_assignments.keys())
        print(f"   è©•ä¾¡å¯¾è±¡: {len(common_tasks)} ã‚¿ã‚¹ã‚¯")

        if not common_tasks:
            print("âš ï¸ è©•ä¾¡å¯èƒ½ãªã‚¿ã‚¹ã‚¯ãŒã‚ã‚Šã¾ã›ã‚“")
            return {}

        # Top-1 (å¾“æ¥ã®)æ­£ç¢ºæ€§è©•ä¾¡
        correct_predictions = 0
        for task_id in common_tasks:
            if predictions[task_id] == test_assignments[task_id]:
                correct_predictions += 1

        accuracy = correct_predictions / len(common_tasks)

        # Top-Kè©•ä¾¡ (K=3, 5)
        topk_metrics = self._evaluate_topk_accuracy(
            common_tasks, test_assignments, prediction_scores
        )

        # ä¿¡é ¼åº¦çµ±è¨ˆ
        prediction_confidences = []
        text_scores = []
        feature_scores = []

        for task_id in common_tasks:
            if task_id in prediction_scores:
                scores = prediction_scores[task_id]
                prediction_confidences.append(scores["combined_score"])
                text_scores.append(scores["text_score"])
                feature_scores.append(scores["feature_score"])

        metrics = {
            "accuracy": accuracy,
            "top1_accuracy": accuracy,  # åŒã˜å€¤
            "top3_accuracy": topk_metrics["top3_accuracy"],
            "top5_accuracy": topk_metrics["top5_accuracy"],
            "correct_predictions": correct_predictions,
            "total_predictions": len(common_tasks),
            "avg_combined_score": (
                np.mean(prediction_confidences) if prediction_confidences else 0.0
            ),
            "avg_text_score": np.mean(text_scores) if text_scores else 0.0,
            "avg_feature_score": np.mean(feature_scores) if feature_scores else 0.0,
            "score_std": (
                np.std(prediction_confidences) if prediction_confidences else 0.0
            ),
        }

        print(
            f"   Top-1ç²¾åº¦: {accuracy:.3f} ({correct_predictions}/{len(common_tasks)})"
        )
        print(
            f"   Top-3ç²¾åº¦: {topk_metrics['top3_accuracy']:.3f} ({topk_metrics['top3_correct']}/{len(common_tasks)})"
        )
        print(
            f"   Top-5ç²¾åº¦: {topk_metrics['top5_accuracy']:.3f} ({topk_metrics['top5_correct']}/{len(common_tasks)})"
        )
        print(
            f"   å¹³å‡çµ±åˆã‚¹ã‚³ã‚¢: {metrics['avg_combined_score']:.3f} Â± {metrics['score_std']:.3f}"
        )
        print(f"   å¹³å‡ãƒ†ã‚­ã‚¹ãƒˆã‚¹ã‚³ã‚¢: {metrics['avg_text_score']:.3f}")
        print(f"   å¹³å‡ç‰¹å¾´é‡ã‚¹ã‚³ã‚¢: {metrics['avg_feature_score']:.3f}")

        # é–‹ç™ºè€…åˆ¥è©•ä¾¡
        dev_metrics = self._evaluate_by_developer(
            predictions, test_assignments, common_tasks
        )
        metrics["developer_metrics"] = dev_metrics

        return metrics

    def _evaluate_topk_accuracy(
        self, common_tasks, test_assignments, prediction_scores
    ):
        """Top-K accuracyè©•ä¾¡"""
        top3_correct = 0
        top5_correct = 0

        for task_id in common_tasks:
            actual_dev = test_assignments[task_id]

            if task_id in prediction_scores:
                # å…¨ã‚¹ã‚³ã‚¢ã‚’å–å¾—ã—ã¦ã‚½ãƒ¼ãƒˆ
                all_scores = prediction_scores[task_id].get("all_scores", {})
                if not all_scores:
                    # all_scoresãŒãªã„å ´åˆã€combined_scoresã‚’å†æ§‹ç¯‰
                    continue

                # ã‚¹ã‚³ã‚¢é †ã«é–‹ç™ºè€…ã‚’å–å¾—
                sorted_devs = list(all_scores.keys())

                # Top-3è©•ä¾¡
                if len(sorted_devs) >= 3:
                    top3_devs = sorted_devs[:3]
                    if actual_dev in top3_devs:
                        top3_correct += 1
                elif actual_dev in sorted_devs:
                    top3_correct += 1

                # Top-5è©•ä¾¡
                if len(sorted_devs) >= 5:
                    top5_devs = sorted_devs[:5]
                    if actual_dev in top5_devs:
                        top5_correct += 1
                elif actual_dev in sorted_devs:
                    top5_correct += 1

        total_tasks = len(common_tasks)
        return {
            "top3_accuracy": top3_correct / total_tasks if total_tasks > 0 else 0.0,
            "top5_accuracy": top5_correct / total_tasks if total_tasks > 0 else 0.0,
            "top3_correct": top3_correct,
            "top5_correct": top5_correct,
        }

    def _evaluate_by_developer(self, predictions, test_assignments, common_tasks):
        """é–‹ç™ºè€…åˆ¥ã®è©•ä¾¡"""
        dev_metrics = defaultdict(lambda: {"correct": 0, "total": 0})

        for task_id in common_tasks:
            actual_dev = test_assignments[task_id]
            predicted_dev = predictions[task_id]

            dev_metrics[actual_dev]["total"] += 1
            if actual_dev == predicted_dev:
                dev_metrics[actual_dev]["correct"] += 1

        # ç²¾åº¦è¨ˆç®—
        for dev, stats in dev_metrics.items():
            stats["accuracy"] = (
                stats["correct"] / stats["total"] if stats["total"] > 0 else 0.0
            )

        print("   é–‹ç™ºè€…åˆ¥ç²¾åº¦:")
        sorted_devs = sorted(
            dev_metrics.items(), key=lambda x: x[1]["total"], reverse=True
        )
        for dev, stats in sorted_devs[:10]:
            print(
                f"     {dev}: {stats['accuracy']:.3f} ({stats['correct']}/{stats['total']})"
            )

        return dict(dev_metrics)

    def save_model(self, output_dir="models"):
        """ãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜"""
        output_dir = Path(output_dir)
        output_dir.mkdir(exist_ok=True)

        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

        # ãƒ¢ãƒ‡ãƒ«ä¿å­˜
        model_path = output_dir / f"simple_similarity_recommender_{timestamp}.pkl"
        with open(model_path, "wb") as f:
            model_data = {
                "vectorizer": self.vectorizer,
                "rf_classifier": getattr(self, "rf_classifier", None),
                "scaler": self.scaler,
                "developer_profiles": self.developer_profiles,
                "tfidf_matrix": getattr(self, "tfidf_matrix", None),
                "dev_names": getattr(self, "dev_names", []),
            }
            pickle.dump(model_data, f)

        print(f"âœ… ãƒ¢ãƒ‡ãƒ«ä¿å­˜: {model_path}")
        return model_path

    def run_full_pipeline(
        self,
        data_path,
        output_dir="outputs",
        use_temporal=True,
        extraction_method="all",
    ):
        """
        å®Œå…¨ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®å®Ÿè¡Œ

        Args:
            extraction_method: é–‹ç™ºè€…æŠ½å‡ºæ–¹æ³• ('assignees', 'creators', 'all')
        """
        print("ğŸš€ ã‚·ãƒ³ãƒ—ãƒ«é¡ä¼¼åº¦ãƒ™ãƒ¼ã‚¹æ¨è–¦ã‚·ã‚¹ãƒ†ãƒ å®Ÿè¡Œé–‹å§‹")
        if use_temporal:
            print("â° æ™‚ç³»åˆ—ãƒ™ãƒ¼ã‚¹å‹•çš„å€™è£œãƒ—ãƒ¼ãƒ«æœ‰åŠ¹")
        print(f"ğŸ‘¥ é–‹ç™ºè€…æŠ½å‡ºæ–¹æ³•: {extraction_method}")
        print("=" * 70)

        # 1. ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
        training_data, test_data = self.load_data(data_path)

        # 2. å­¦ç¿’ãƒšã‚¢æŠ½å‡ºï¼ˆæ–°ã—ã„æŠ½å‡ºæ–¹æ³•ã‚’ä½¿ç”¨ï¼‰
        training_pairs, developer_stats = self.extract_training_pairs(
            training_data, extraction_method
        )

        if not training_pairs:
            print("âš ï¸ å­¦ç¿’ãƒšã‚¢ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
            return {}

        # 3. åŸºæœ¬é–‹ç™ºè€…ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«æ§‹ç¯‰
        learned_profiles = self.build_developer_profiles(training_pairs)

        # 4. æ™‚ç³»åˆ—ã‚¢ã‚¯ãƒ†ã‚£ãƒ“ãƒ†ã‚£ã‚¿ã‚¤ãƒ ãƒ©ã‚¤ãƒ³æ§‹ç¯‰ï¼ˆæ–°ã—ã„æŠ½å‡ºæ–¹æ³•ã‚’ä½¿ç”¨ï¼‰
        if use_temporal:
            self.build_developer_activity_timeline(training_data, extraction_method)

        # 5. ãƒ†ã‚­ã‚¹ãƒˆé¡ä¼¼åº¦ãƒ¢ãƒ‡ãƒ«è¨“ç·´ï¼ˆå­¦ç¿’æ¸ˆã¿ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ã®ã¿ã§ï¼‰
        self.train_text_similarity_model(learned_profiles)

        # 6. ç‰¹å¾´é‡ãƒ¢ãƒ‡ãƒ«è¨“ç·´ï¼ˆå­¦ç¿’æ¸ˆã¿ãƒšã‚¢ã®ã¿ã§ï¼‰
        self.train_feature_model(training_pairs)

        # 7. äºˆæ¸¬å®Ÿè¡Œ
        if use_temporal:
            # æ™‚ç³»åˆ—ãƒ™ãƒ¼ã‚¹å‹•çš„å€™è£œãƒ—ãƒ¼ãƒ«
            predictions, prediction_scores, test_assignments, temporal_stats = (
                self.predict_assignments_temporal(test_data, learned_profiles)
            )
        else:
            # å¾“æ¥ã®ãƒ†ã‚¹ãƒˆãƒ™ãƒ¼ã‚¹å€™è£œãƒ—ãƒ¼ãƒ«
            test_developers, test_assignments = self.extract_test_developers(test_data)
            self.developer_profiles = self.create_test_based_candidate_pool(
                learned_profiles, test_developers
            )
            predictions, prediction_scores, _ = self.predict_assignments_with_pool(
                test_data, self.developer_profiles, test_assignments
            )
            temporal_stats = {}

        # 8. è©•ä¾¡
        metrics = self.evaluate_predictions(
            predictions, test_assignments, prediction_scores
        )

        # æ™‚ç³»åˆ—çµ±è¨ˆã‚’è¿½åŠ 
        if use_temporal:
            metrics["temporal_stats"] = temporal_stats

        # 9. çµæœä¿å­˜
        self.save_results(
            metrics, predictions, prediction_scores, test_assignments, output_dir
        )

        # 10. ãƒ¢ãƒ‡ãƒ«ä¿å­˜
        model_path = self.save_model()

        print("âœ… ã‚·ãƒ³ãƒ—ãƒ«é¡ä¼¼åº¦ãƒ™ãƒ¼ã‚¹æ¨è–¦ã‚·ã‚¹ãƒ†ãƒ å®Œäº†")
        return metrics

    def save_results(
        self, metrics, predictions, prediction_scores, test_assignments, output_dir
    ):
        """çµæœã®ä¿å­˜"""
        output_dir = Path(output_dir)
        output_dir.mkdir(exist_ok=True)

        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

        # ãƒ¡ãƒˆãƒªã‚¯ã‚¹ä¿å­˜
        metrics_path = output_dir / f"simple_similarity_metrics_{timestamp}.json"
        with open(metrics_path, "w", encoding="utf-8") as f:
            # numpyé…åˆ—ã‚’å¯¾å¿œ
            def convert_numpy(obj):
                if isinstance(obj, np.ndarray):
                    return obj.tolist()
                elif isinstance(obj, (np.integer, np.floating)):
                    return float(obj)
                return obj

            json.dump(metrics, f, indent=2, ensure_ascii=False, default=convert_numpy)

        # äºˆæ¸¬çµæœä¿å­˜
        results = []
        for task_id in set(predictions.keys()) | set(test_assignments.keys()):
            result = {
                "task_id": task_id,
                "actual_developer": test_assignments.get(task_id),
                "predicted_developer": predictions.get(task_id),
                "correct": test_assignments.get(task_id) == predictions.get(task_id),
            }

            if task_id in prediction_scores:
                result.update(prediction_scores[task_id])

            results.append(result)

        results_df = pd.DataFrame(results)
        results_path = output_dir / f"simple_similarity_results_{timestamp}.csv"
        results_df.to_csv(results_path, index=False, encoding="utf-8")

        print(f"âœ… ãƒ¡ãƒˆãƒªã‚¯ã‚¹ä¿å­˜: {metrics_path}")
        print(f"âœ… çµæœä¿å­˜: {results_path}")

    def load_existing_profiles(self, profile_path="data/dev_profiles.json"):
        """æ—¢å­˜ã®é–‹ç™ºè€…ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚“ã§å€™è£œã‚’æ‹¡å¼µ"""
        print("ğŸ‘¥ æ—¢å­˜é–‹ç™ºè€…ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ä¸­...")

        try:
            with open(profile_path, "r", encoding="utf-8") as f:
                existing_profiles = json.load(f)

            # ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†ã—ã¦SimpleSimilarityã«é©ç”¨å¯èƒ½ãªå½¢å¼ã«å¤‰æ›
            expanded_profiles = {}

            for dev_name, profile in existing_profiles.items():
                # åŸºæœ¬çš„ãªçµ±è¨ˆæƒ…å ±ã‚’æŠ½å‡º
                if (
                    profile.get("total_contributions", 0) > 0
                ):  # æœ€ä½1ã¤ã¯è²¢çŒ®ã—ã¦ã„ã‚‹é–‹ç™ºè€…
                    expanded_profiles[dev_name] = {
                        "task_count": profile.get("total_contributions", 0),
                        "task_texts": [],  # å®Ÿéš›ã®ã‚¿ã‚¹ã‚¯ãƒ†ã‚­ã‚¹ãƒˆã¯å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰
                        "feature_sums": defaultdict(float),
                        "label_preferences": defaultdict(int),
                        "avg_title_length": profile.get("avg_issue_title_length", 0),
                        "avg_body_length": profile.get("avg_issue_body_length", 0),
                        "avg_comments_count": profile.get("avg_comments", 0),
                        "avg_is_bug": profile.get("label_affinities", {}).get(
                            "bug", 0.1
                        ),
                        "avg_is_enhancement": profile.get("label_affinities", {}).get(
                            "enhancement", 0.1
                        ),
                        "avg_is_documentation": profile.get("label_affinities", {}).get(
                            "documentation", 0.05
                        ),
                        "recent_activity": profile.get("recent_activity_score", 0),
                    }

            print(f"   æ—¢å­˜ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«: {len(expanded_profiles)} äºº")
            return expanded_profiles

        except FileNotFoundError:
            print(f"âš ï¸ ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {profile_path}")
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ã‚ˆã‚Šå¤šãã®å€™è£œã‚’ç”Ÿæˆ
            return self._generate_fallback_profiles()
        except Exception as e:
            print(f"âš ï¸ ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            return self._generate_fallback_profiles()

    def _generate_fallback_profiles(self):
        """ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ã‚ˆã‚Šå¤šãã®ä»®æƒ³é–‹ç™ºè€…ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç”Ÿæˆ"""
        print("ğŸ”„ ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å€™è£œãƒ—ãƒ¼ãƒ«ç”Ÿæˆä¸­...")

        # ã‚ˆãã‚ã‚‹é–‹ç™ºè€…åãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ç”Ÿæˆ
        common_names = [
            "alice",
            "bob",
            "charlie",
            "david",
            "eve",
            "frank",
            "grace",
            "henry",
            "ivan",
            "julia",
            "kevin",
            "lisa",
            "mike",
            "nancy",
            "oscar",
            "penny",
            "quinn",
            "robert",
            "sara",
            "tom",
            "ursula",
            "victor",
            "wendy",
            "xavier",
            "yuki",
            "zoe",
            "alex",
            "jordan",
            "taylor",
            "casey",
            "jamie",
            "riley",
        ]

        fallback_profiles = {}
        for i, name in enumerate(common_names):
            fallback_profiles[f"dev_{name}"] = {
                "task_count": max(1, 20 - i),  # ç•°ãªã‚‹ã‚¢ã‚¯ãƒ†ã‚£ãƒ“ãƒ†ã‚£ãƒ¬ãƒ™ãƒ«
                "task_texts": [],
                "feature_sums": defaultdict(float),
                "label_preferences": defaultdict(int),
                "avg_title_length": 40 + (i % 20),
                "avg_body_length": 150 + (i % 100),
                "avg_comments_count": 1 + (i % 5),
                "avg_is_bug": 0.05 + (i % 3) * 0.1,
                "avg_is_enhancement": 0.05 + ((i + 1) % 3) * 0.1,
                "avg_is_documentation": 0.02 + ((i + 2) % 3) * 0.05,
                "recent_activity": max(0.1, 1.0 - i * 0.03),
            }

        print(f"   ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å€™è£œ: {len(fallback_profiles)} äºº")
        return fallback_profiles

    def expand_candidate_pool(
        self, learned_profiles, existing_profiles, min_activity_threshold=1
    ):
        """å­¦ç¿’æ¸ˆã¿ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ã¨æ—¢å­˜ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ã‚’çµ„ã¿åˆã‚ã›ã¦å€™è£œãƒ—ãƒ¼ãƒ«ã‚’æ‹¡å¼µ"""
        print("ğŸ” å€™è£œãƒ—ãƒ¼ãƒ«æ‹¡å¼µä¸­...")

        expanded_pool = {}

        # 1. å­¦ç¿’æ¸ˆã¿ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆé«˜å“è³ªï¼‰ã‚’å„ªå…ˆ
        for dev_name, profile in learned_profiles.items():
            expanded_pool[dev_name] = profile
            expanded_pool[dev_name]["source"] = "training"
            expanded_pool[dev_name]["priority"] = "high"

        # 2. æ—¢å­˜ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰è¿½åŠ å€™è£œã‚’é¸å®š
        added_from_existing = 0
        for dev_name, profile in existing_profiles.items():
            if (
                dev_name not in expanded_pool
                and profile.get("task_count", 0) >= min_activity_threshold
            ):
                # æ—¢å­˜ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å­¦ç¿’æ¸ˆã¿å½¢å¼ã«åˆã‚ã›ã¦èª¿æ•´
                expanded_pool[dev_name] = {
                    "task_count": profile["task_count"],
                    "task_texts": [],  # ç©ºã®ãƒªã‚¹ãƒˆï¼ˆãƒ†ã‚­ã‚¹ãƒˆé¡ä¼¼åº¦ã§ã¯ä½¿ç”¨ã•ã‚Œãªã„ï¼‰
                    "feature_sums": profile.get("feature_sums", defaultdict(float)),
                    "label_preferences": profile.get(
                        "label_preferences", defaultdict(int)
                    ),
                    "avg_title_length": profile.get("avg_title_length", 50),
                    "avg_body_length": profile.get("avg_body_length", 200),
                    "avg_comments_count": profile.get("avg_comments_count", 2),
                    "avg_is_bug": profile.get("is_bug", 0.1),
                    "avg_is_enhancement": profile.get("is_enhancement", 0.1),
                    "avg_is_documentation": profile.get("is_documentation", 0.05),
                    "source": "existing",
                    "priority": "medium",
                }
                added_from_existing += 1

        print(f"   æ‹¡å¼µå€™è£œãƒ—ãƒ¼ãƒ«: {len(expanded_pool)} äºº")
        print(f"     - å­¦ç¿’æ¸ˆã¿: {len(learned_profiles)} äºº")
        print(f"     - æ—¢å­˜ã‹ã‚‰è¿½åŠ : {added_from_existing} äºº")

        return expanded_pool

    def extract_test_developers(self, test_data):
        """ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‹ã‚‰å®Ÿéš›ã®æ‹…å½“é–‹ç™ºè€…ã‚’æŠ½å‡º"""
        print("ğŸ‘¥ ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿é–‹ç™ºè€…æŠ½å‡ºä¸­...")

        test_developers = set()
        test_assignments = {}

        for task_data in test_data:
            task_id = task_data.get("id") or task_data.get("number")
            if not task_id:
                continue

            assignee = None
            if "assignees" in task_data and task_data["assignees"]:
                assignee = task_data["assignees"][0].get("login")
            elif "events" in task_data:
                for event in task_data["events"]:
                    if event.get("event") == "assigned" and event.get("assignee"):
                        assignee = event["assignee"].get("login")
                        break

            if assignee:
                test_developers.add(assignee)
                test_assignments[task_id] = assignee

        print(f"   ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿é–‹ç™ºè€…: {len(test_developers)} äºº")

        # ä¸Šä½é–‹ç™ºè€…è¡¨ç¤º
        dev_task_counts = Counter()
        for assignee in test_assignments.values():
            dev_task_counts[assignee] += 1

        top_test_devs = dev_task_counts.most_common(10)
        print("   ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ä¸Šä½é–‹ç™ºè€…:")
        for dev, count in top_test_devs:
            print(f"     {dev}: {count} ã‚¿ã‚¹ã‚¯")

        return list(test_developers), test_assignments

    def create_test_based_candidate_pool(self, learned_profiles, test_developers):
        """ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿é–‹ç™ºè€…ãƒ™ãƒ¼ã‚¹ã®å€™è£œãƒ—ãƒ¼ãƒ«ã‚’ä½œæˆ"""
        print("ğŸ¯ ãƒ†ã‚¹ãƒˆãƒ™ãƒ¼ã‚¹å€™è£œãƒ—ãƒ¼ãƒ«ä½œæˆä¸­...")

        candidate_pool = {}

        # å­¦ç¿’æ¸ˆã¿ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚ã‚‹é–‹ç™ºè€…
        learned_count = 0
        for dev_name in test_developers:
            if dev_name in learned_profiles:
                candidate_pool[dev_name] = learned_profiles[dev_name]
                candidate_pool[dev_name]["source"] = "training"
                candidate_pool[dev_name]["priority"] = "high"
                learned_count += 1

        # å­¦ç¿’æ¸ˆã¿ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ãŒãªã„é–‹ç™ºè€…ç”¨ã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«
        default_count = 0
        for dev_name in test_developers:
            if dev_name not in candidate_pool:
                candidate_pool[dev_name] = self._create_default_profile(dev_name)
                default_count += 1

        print(f"   å€™è£œãƒ—ãƒ¼ãƒ«: {len(candidate_pool)} äºº")
        print(f"     - å­¦ç¿’æ¸ˆã¿ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«: {learned_count} äºº")
        print(f"     - ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«: {default_count} äºº")

        return candidate_pool

    def _create_default_profile(self, dev_name):
        """å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã«ãªã„é–‹ç™ºè€…ç”¨ã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆ"""
        # é–‹ç™ºè€…åã®ç‰¹æ€§ã«åŸºã¥ã„ã¦ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª¿æ•´
        name_hash = hash(dev_name) % 100

        return {
            "task_count": 5 + (name_hash % 10),  # 5-14ã®ãƒ©ãƒ³ãƒ€ãƒ ãªã‚¿ã‚¹ã‚¯æ•°
            "task_texts": [],
            "feature_sums": defaultdict(float),
            "label_preferences": defaultdict(int),
            "avg_title_length": 45 + (name_hash % 15),  # 45-59ã®å¹³å‡ã‚¿ã‚¤ãƒˆãƒ«é•·
            "avg_body_length": 180 + (name_hash % 40),  # 180-219ã®å¹³å‡æœ¬æ–‡é•·
            "avg_comments_count": 2 + (name_hash % 3),  # 2-4ã®å¹³å‡ã‚³ãƒ¡ãƒ³ãƒˆæ•°
            "avg_is_bug": 0.1 + (name_hash % 3) * 0.05,  # 0.1-0.2ã®ãƒã‚°è¦ªå’Œæ€§
            "avg_is_enhancement": 0.1
            + ((name_hash + 1) % 3) * 0.05,  # 0.1-0.2ã®æ©Ÿèƒ½è¦ªå’Œæ€§
            "avg_is_documentation": 0.05
            + ((name_hash + 2) % 3) * 0.025,  # 0.05-0.1ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆè¦ªå’Œæ€§
            "source": "default",
            "priority": "low",
        }


def main():
    parser = argparse.ArgumentParser(
        description="ã‚·ãƒ³ãƒ—ãƒ«é¡ä¼¼åº¦ãƒ™ãƒ¼ã‚¹ã®ã‚¿ã‚¹ã‚¯æ¨è–¦ã‚·ã‚¹ãƒ†ãƒ "
    )
    parser.add_argument(
        "--config", default="configs/unified_rl.yaml", help="è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹"
    )
    parser.add_argument("--data", default="data/backlog.json", help="çµ±åˆãƒ‡ãƒ¼ã‚¿ãƒ‘ã‚¹")
    parser.add_argument("--output", default="outputs", help="å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª")
    parser.add_argument(
        "--temporal", action="store_true", help="æ™‚ç³»åˆ—ãƒ™ãƒ¼ã‚¹å‹•çš„å€™è£œãƒ—ãƒ¼ãƒ«ä½¿ç”¨"
    )
    parser.add_argument(
        "--no-temporal",
        dest="temporal",
        action="store_false",
        help="å¾“æ¥ã®ãƒ†ã‚¹ãƒˆãƒ™ãƒ¼ã‚¹å€™è£œãƒ—ãƒ¼ãƒ«ä½¿ç”¨",
    )
    parser.add_argument(
        "--extraction-method",
        default="all",
        choices=["assignees", "creators", "all"],
        help="é–‹ç™ºè€…æŠ½å‡ºæ–¹æ³• (assignees: å‰²ã‚Šå½“ã¦ã®ã¿, creators: ä½œæˆè€…ã‚‚å«ã‚€, all: ã™ã¹ã¦)",
    )
    parser.set_defaults(temporal=True)

    args = parser.parse_args()

    # æ¨è–¦ã‚·ã‚¹ãƒ†ãƒ å®Ÿè¡Œ
    recommender = SimpleSimilarityRecommender(args.config)
    metrics = recommender.run_full_pipeline(
        args.data,
        args.output,
        use_temporal=args.temporal,
        extraction_method=args.extraction_method,
    )

    print("\nğŸ¯ æœ€çµ‚çµæœ:")
    if metrics:
        for key, value in metrics.items():
            if isinstance(value, (int, float)) and key not in [
                "developer_metrics",
                "temporal_stats",
            ]:
                print(f"   {key}: {value:.3f}")

        # æ™‚ç³»åˆ—çµ±è¨ˆè¡¨ç¤º
        if "temporal_stats" in metrics:
            stats = metrics["temporal_stats"]
            print(f"\nâ° æ™‚ç³»åˆ—çµ±è¨ˆ:")
            print(f"   å¹³å‡å€™è£œãƒ—ãƒ¼ãƒ«ã‚µã‚¤ã‚º: {stats.get('avg_pool_size', 0):.1f}")
            print(
                f"   å€™è£œãƒ—ãƒ¼ãƒ«ã‚µã‚¤ã‚ºç¯„å›²: {stats.get('min_pool_size', 0)}-{stats.get('max_pool_size', 0)}"
            )

    return 0


if __name__ == "__main__":
    exit(main())
