#!/usr/bin/env python3
"""
Top-Kç²¾åº¦è©•ä¾¡ã‚¹ã‚¯ãƒªãƒ—ãƒˆï¼ˆTop-1, Top-3, Top-5ï¼‰
ã‚ˆã‚Šå®Ÿç”¨çš„ãªæ¨è–¦ã‚·ã‚¹ãƒ†ãƒ è©•ä¾¡
"""

import argparse
import json
import os
import pickle
import sys
from collections import Counter
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Tuple

import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import yaml
from tqdm import tqdm

# ãƒ‘ã‚¹è¨­å®š
sys.path.append(str(Path(__file__).resolve().parents[1] / "src"))


class PPOPolicyNetwork(nn.Module):
    """PPOãƒãƒªã‚·ãƒ¼ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®å†æ§‹ç¯‰"""
    
    def __init__(self, input_dim=64, hidden_dim=128):
        super(PPOPolicyNetwork, self).__init__()
        
        # ç‰¹å¾´é‡æŠ½å‡ºå™¨
        self.feature_extractor = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.LayerNorm(hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(hidden_dim, hidden_dim),
            nn.LayerNorm(hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.1)
        )
        
        # ã‚¢ã‚¯ã‚¿ãƒ¼ï¼ˆè¡Œå‹•é¸æŠï¼‰
        self.actor = nn.Sequential(
            nn.Linear(hidden_dim, 64),
            nn.LayerNorm(64),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(64, 16),  # è¡Œå‹•ç©ºé–“
            nn.Softmax(dim=-1)
        )
        
        # ã‚¯ãƒªãƒ†ã‚£ãƒƒã‚¯ï¼ˆä¾¡å€¤é–¢æ•°ï¼‰
        self.critic = nn.Sequential(
            nn.Linear(hidden_dim, 64),
            nn.LayerNorm(64),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(64, 1)
        )
    
    def forward(self, x):
        features = self.feature_extractor(x)
        action_probs = self.actor(features)
        value = self.critic(features)
        return action_probs, value
    
    def get_action_score(self, x):
        """ã‚¿ã‚¹ã‚¯é©åˆåº¦ã‚¹ã‚³ã‚¢ã‚’å–å¾—"""
        with torch.no_grad():
            action_probs, value = self.forward(x)
            # è¡Œå‹•ç¢ºç‡ã®æœ€å¤§å€¤ã‚’é©åˆåº¦ã‚¹ã‚³ã‚¢ã¨ã—ã¦ä½¿ç”¨
            score = torch.max(action_probs).item()
            return score


def is_bot(username: str) -> bool:
    """ãƒ¦ãƒ¼ã‚¶ãƒ¼åãŒBotã‹ã©ã†ã‹åˆ¤å®š"""
    bot_indicators = [
        "[bot]", "bot", "dependabot", "renovate", "greenkeeper",
        "codecov", "travis", "circleci", "github-actions", "automated"
    ]
    username_lower = username.lower()
    return any(indicator in username_lower for indicator in bot_indicators)


def load_test_data_with_bot_filtering(test_data_path: str) -> Tuple[List[Dict], List[str]]:
    """ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ã€Botã‚’é™¤å»"""
    print(f"ğŸ“‚ ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ï¼ˆBoté™¤å»ã‚ã‚Šï¼‰: {test_data_path}")
    
    with open(test_data_path, "r", encoding="utf-8") as f:
        test_data = json.load(f)
    
    filtered_tasks = []
    ground_truth_authors = []
    bot_count = 0
    
    for task in test_data:
        author = task.get("author", {})
        if author and isinstance(author, dict):
            author_login = author.get("login", "")
            if author_login:
                if is_bot(author_login):
                    bot_count += 1
                    continue
                else:
                    filtered_tasks.append(task)
                    ground_truth_authors.append(author_login)
    
    print(f"   ç·ã‚¿ã‚¹ã‚¯æ•°: {len(test_data):,}")
    print(f"   Boté™¤å»æ•°: {bot_count:,}ã‚¿ã‚¹ã‚¯")
    print(f"   äººé–“ã‚¿ã‚¹ã‚¯æ•°: {len(filtered_tasks):,}ã‚¿ã‚¹ã‚¯")
    
    return filtered_tasks, ground_truth_authors


def load_trained_models(model_dir: str, actual_authors: List[str], max_models: int = 50) -> Dict[str, PPOPolicyNetwork]:
    """è¨“ç·´æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿ï¼ˆTop-Kè©•ä¾¡ç”¨ã«ã‚ˆã‚Šå¤šãã®ãƒ¢ãƒ‡ãƒ«ï¼‰"""
    print(f"ğŸ¤– è¨“ç·´æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ï¼ˆTop-Kè©•ä¾¡ç”¨ï¼‰: {model_dir}")
    
    model_files = [f for f in os.listdir(model_dir) if f.endswith('.pth')]
    all_trained_agents = [f.replace('agent_', '').replace('.pth', '') for f in model_files]
    
    # Boté™¤å»
    human_trained_agents = [agent for agent in all_trained_agents if not is_bot(agent)]
    
    # å®Ÿéš›ã®ä½œæˆè€…ã¨é‡è¤‡ã™ã‚‹äººé–“ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®ã¿
    actual_set = set(actual_authors)
    human_set = set(human_trained_agents)
    overlapping_agents = actual_set.intersection(human_set)
    
    print(f"   å…¨è¨“ç·´ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆæ•°: {len(all_trained_agents)}")
    print(f"   äººé–“è¨“ç·´ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆæ•°: {len(human_trained_agents)}")
    print(f"   é‡è¤‡äººé–“ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆæ•°: {len(overlapping_agents)}")
    print(f"   èª­ã¿è¾¼ã¿äºˆå®šæ•°: {min(max_models, len(overlapping_agents))}")
    
    loaded_models = {}
    
    for i, agent_name in enumerate(overlapping_agents):
        if i >= max_models:  # æŒ‡å®šã•ã‚ŒãŸæ•°ã¾ã§èª­ã¿è¾¼ã¿
            break
            
        model_path = os.path.join(model_dir, f"agent_{agent_name}.pth")
        
        try:
            # ãƒ¢ãƒ‡ãƒ«ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
            model_data = torch.load(model_path, map_location='cpu', weights_only=False)
            
            # ãƒãƒªã‚·ãƒ¼ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯å†æ§‹ç¯‰
            policy_network = PPOPolicyNetwork()
            policy_network.load_state_dict(model_data['policy_state_dict'])
            policy_network.eval()
            
            loaded_models[agent_name] = policy_network
            
            if i < 5:  # æœ€åˆã®5ã¤ã®ã¿è©³ç´°è¡¨ç¤º
                print(f"   âœ… {agent_name}: ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿æˆåŠŸ")
        
        except Exception as e:
            if i < 5:
                print(f"   âŒ {agent_name}: èª­ã¿è¾¼ã¿å¤±æ•— - {e}")
    
    print(f"   èª­ã¿è¾¼ã¿å®Œäº†: {len(loaded_models)}ãƒ¢ãƒ‡ãƒ«")
    return loaded_models


def extract_task_features_for_model(task: Dict) -> torch.Tensor:
    """ãƒ¢ãƒ‡ãƒ«ç”¨ã®ã‚¿ã‚¹ã‚¯ç‰¹å¾´é‡ã‚’æŠ½å‡ºï¼ˆ64æ¬¡å…ƒï¼‰"""
    features = []
    
    # åŸºæœ¬çš„ãªãƒ†ã‚­ã‚¹ãƒˆç‰¹å¾´é‡
    title = task.get("title", "") or ""
    body = task.get("body", "") or ""
    labels = task.get("labels", [])
    
    # åŸºæœ¬ç‰¹å¾´é‡ï¼ˆ10æ¬¡å…ƒï¼‰
    basic_features = [
        len(title),                    # ã‚¿ã‚¤ãƒˆãƒ«é•·
        len(body),                     # æœ¬æ–‡é•·
        len(title.split()),            # ã‚¿ã‚¤ãƒˆãƒ«å˜èªæ•°
        len(body.split()),             # æœ¬æ–‡å˜èªæ•°
        len(labels),                   # ãƒ©ãƒ™ãƒ«æ•°
        title.count('?'),              # ç–‘å•ç¬¦ã®æ•°
        title.count('!'),              # æ„Ÿå˜†ç¬¦ã®æ•°
        body.count('\n'),              # æ”¹è¡Œæ•°
        len(set(title.lower().split())), # ãƒ¦ãƒ‹ãƒ¼ã‚¯å˜èªæ•°
        1 if any(kw in title.lower() for kw in ['bug', 'fix', 'error']) else 0  # ãƒã‚°é–¢é€£
    ]
    features.extend(basic_features)
    
    # æ—¥ä»˜ç‰¹å¾´é‡ï¼ˆ3æ¬¡å…ƒï¼‰
    created_at = task.get("created_at", "")
    if created_at:
        try:
            date_parts = created_at.split("T")[0].split("-")
            year = int(date_parts[0])
            month = int(date_parts[1])
            day = int(date_parts[2])
            features.extend([year - 2020, month, day])
        except:
            features.extend([0, 0, 0])
    else:
        features.extend([0, 0, 0])
    
    # ãƒ©ãƒ™ãƒ«ç‰¹å¾´é‡ï¼ˆ10æ¬¡å…ƒï¼‰
    label_text = " ".join([str(label) if not isinstance(label, dict) else label.get("name", "") 
                          for label in labels]).lower()
    
    important_keywords = ["bug", "feature", "enhancement", "documentation", "help", 
                         "question", "performance", "security", "ui", "api"]
    for keyword in important_keywords:
        features.append(1 if keyword in label_text else 0)
    
    # ãƒ†ã‚­ã‚¹ãƒˆè¤‡é›‘åº¦ç‰¹å¾´é‡ï¼ˆ10æ¬¡å…ƒï¼‰
    complexity_indicators = ["complex", "difficult", "hard", "challenging", "advanced",
                           "simple", "easy", "basic", "straightforward", "minor"]
    for indicator in complexity_indicators:
        features.append(1 if indicator in (title + " " + body).lower() else 0)
    
    # å„ªå…ˆåº¦ç‰¹å¾´é‡ï¼ˆ5æ¬¡å…ƒï¼‰
    priority_keywords = ["urgent", "critical", "high", "low", "normal"]
    for keyword in priority_keywords:
        features.append(1 if keyword in (title + " " + body).lower() else 0)
    
    # æ®‹ã‚Šã®æ¬¡å…ƒã‚’ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°
    while len(features) < 64:
        features.append(0.0)
    
    # 64æ¬¡å…ƒã«åˆ‡ã‚Šè©°ã‚
    features = features[:64]
    
    # æ­£è¦åŒ–
    features = np.array(features, dtype=np.float32)
    features = (features - np.mean(features)) / (np.std(features) + 1e-8)
    
    return torch.tensor(features, dtype=torch.float32)


def evaluate_topk_accuracy(
    tasks: List[Dict], 
    ground_truth: List[str], 
    trained_models: Dict[str, PPOPolicyNetwork],
    k_values: List[int] = [1, 3, 5]
) -> Dict:
    """Top-Kç²¾åº¦ã‚’è©•ä¾¡"""
    print(f"ğŸ¯ Top-Kç²¾åº¦è©•ä¾¡é–‹å§‹ï¼ˆK={k_values}ï¼‰...")
    
    all_predictions = []  # å„ã‚¿ã‚¹ã‚¯ã®å…¨ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚¹ã‚³ã‚¢
    actuals = []
    
    available_agents = set(trained_models.keys())
    
    for i, (task, actual_author) in enumerate(tqdm(zip(tasks, ground_truth), desc="Top-Kè©•ä¾¡ä¸­")):
        try:
            # ã‚¿ã‚¹ã‚¯ç‰¹å¾´é‡æŠ½å‡º
            task_features = extract_task_features_for_model(task)
            
            # å„è¨“ç·´æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã§ã®é©åˆåº¦ã‚’è¨ˆç®—
            agent_scores = {}
            for agent_name, model in trained_models.items():
                try:
                    score = model.get_action_score(task_features)
                    agent_scores[agent_name] = score
                except Exception as e:
                    if i < 3:
                        print(f"   è­¦å‘Š: {agent_name}ã®æ¨è«–ã§ã‚¨ãƒ©ãƒ¼ - {e}")
                    agent_scores[agent_name] = 0.0
            
            # ã‚¹ã‚³ã‚¢é †ã«ã‚½ãƒ¼ãƒˆ
            sorted_agents = sorted(agent_scores.items(), key=lambda x: x[1], reverse=True)
            all_predictions.append(sorted_agents)
            actuals.append(actual_author)
            
        except Exception as e:
            if i < 5:
                print(f"   è­¦å‘Š: ã‚¿ã‚¹ã‚¯{i}ã®è©•ä¾¡ã§ã‚¨ãƒ©ãƒ¼ - {e}")
            all_predictions.append([])
            actuals.append(actual_author)
    
    # Top-Kç²¾åº¦è¨ˆç®—
    topk_results = {}
    
    for k in k_values:
        # å…¨ã‚¿ã‚¹ã‚¯ã§ã®Top-Kç²¾åº¦
        topk_matches = 0
        total_tasks = len(all_predictions)
        
        # åˆ©ç”¨å¯èƒ½ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆå†…ã§ã®Top-Kç²¾åº¦
        available_topk_matches = 0
        available_total = 0
        
        for predictions, actual in zip(all_predictions, actuals):
            if not predictions:
                continue
                
            # Top-Kå€™è£œã‚’å–å¾—
            topk_candidates = [agent_name for agent_name, score in predictions[:k]]
            
            # å…¨ã‚¿ã‚¹ã‚¯ã§ã®è©•ä¾¡
            if actual in topk_candidates:
                topk_matches += 1
            
            # åˆ©ç”¨å¯èƒ½ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆå†…ã§ã®è©•ä¾¡
            if actual in available_agents:
                available_total += 1
                if actual in topk_candidates:
                    available_topk_matches += 1
        
        # ç²¾åº¦è¨ˆç®—
        topk_accuracy = topk_matches / total_tasks if total_tasks > 0 else 0
        available_topk_accuracy = available_topk_matches / available_total if available_total > 0 else 0
        
        topk_results[f"top_{k}"] = {
            "accuracy": topk_accuracy,
            "matches": topk_matches,
            "total": total_tasks,
            "available_accuracy": available_topk_accuracy,
            "available_matches": available_topk_matches,
            "available_total": available_total
        }
        
        print(f"   Top-{k}ç²¾åº¦: {topk_accuracy:.3f} ({topk_matches}/{total_tasks})")
        print(f"   Top-{k}åˆ©ç”¨å¯èƒ½ç²¾åº¦: {available_topk_accuracy:.3f} ({available_topk_matches}/{available_total})")
    
    # å…¨ä½“çµæœ
    results = {
        "total_tasks": len(tasks),
        "loaded_models": len(trained_models),
        "available_agents": len(available_agents),
        "topk_results": topk_results,
        "coverage_rate": available_total / total_tasks if total_tasks > 0 else 0,
    }
    
    return results


def create_topk_report(results: Dict, output_dir: str) -> str:
    """Top-Kç²¾åº¦è©•ä¾¡ãƒ¬ãƒãƒ¼ãƒˆã‚’ä½œæˆ"""
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    report_path = os.path.join(output_dir, f"topk_accuracy_{timestamp}.md")
    
    print(f"ğŸ“Š Top-Kç²¾åº¦ãƒ¬ãƒãƒ¼ãƒˆä½œæˆä¸­: {report_path}")
    
    topk_results = results.get("topk_results", {})
    
    report_content = f"""# Top-Kç²¾åº¦è©•ä¾¡ãƒ¬ãƒãƒ¼ãƒˆ

ç”Ÿæˆæ—¥æ™‚: {datetime.now().strftime("%Yå¹´%mæœˆ%dæ—¥ %H:%M:%S")}

## è©•ä¾¡æ¦‚è¦

### ãƒ‡ãƒ¼ã‚¿æƒ…å ±
- **è©•ä¾¡ã‚¿ã‚¹ã‚¯æ•°**: {results.get('total_tasks', 0):,}
- **ä½¿ç”¨ãƒ¢ãƒ‡ãƒ«æ•°**: {results.get('loaded_models', 0)}
- **åˆ©ç”¨å¯èƒ½ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆæ•°**: {results.get('available_agents', 0)}
- **ã‚«ãƒãƒ¬ãƒƒã‚¸ç‡**: {results.get('coverage_rate', 0):.3f}

## Top-Kç²¾åº¦çµæœ

### å…¨ã‚¿ã‚¹ã‚¯ã§ã®ç²¾åº¦
"""
    
    for k in [1, 3, 5]:
        if f"top_{k}" in topk_results:
            result = topk_results[f"top_{k}"]
            accuracy = result.get("accuracy", 0)
            matches = result.get("matches", 0)
            total = result.get("total", 0)
            
            report_content += f"""
#### Top-{k}ç²¾åº¦
- **ç²¾åº¦**: {accuracy:.3f} ({accuracy*100:.1f}%)
- **ä¸€è‡´æ•°**: {matches:,} / {total:,}
"""
    
    report_content += f"""
### åˆ©ç”¨å¯èƒ½ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆå†…ã§ã®ç²¾åº¦
"""
    
    for k in [1, 3, 5]:
        if f"top_{k}" in topk_results:
            result = topk_results[f"top_{k}"]
            available_accuracy = result.get("available_accuracy", 0)
            available_matches = result.get("available_matches", 0)
            available_total = result.get("available_total", 0)
            
            report_content += f"""
#### Top-{k}åˆ©ç”¨å¯èƒ½ç²¾åº¦
- **ç²¾åº¦**: {available_accuracy:.3f} ({available_accuracy*100:.1f}%)
- **ä¸€è‡´æ•°**: {available_matches:,} / {available_total:,}
"""
    
    # ãƒ©ãƒ³ãƒ€ãƒ é¸æŠã¨ã®æ¯”è¼ƒ
    num_agents = results.get('loaded_models', 50)
    random_top1 = 1 / num_agents
    random_top3 = min(3 / num_agents, 1.0)
    random_top5 = min(5 / num_agents, 1.0)
    
    report_content += f"""
## ãƒ©ãƒ³ãƒ€ãƒ é¸æŠã¨ã®æ¯”è¼ƒ

### ãƒ©ãƒ³ãƒ€ãƒ é¸æŠã®æœŸå¾…ç²¾åº¦
- **Top-1**: {random_top1:.3f} ({random_top1*100:.1f}%)
- **Top-3**: {random_top3:.3f} ({random_top3*100:.1f}%)
- **Top-5**: {random_top5:.3f} ({random_top5*100:.1f}%)

### æ”¹å–„å€ç‡ï¼ˆåˆ©ç”¨å¯èƒ½ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆå†…ï¼‰
"""
    
    for k in [1, 3, 5]:
        if f"top_{k}" in topk_results:
            result = topk_results[f"top_{k}"]
            available_accuracy = result.get("available_accuracy", 0)
            
            if k == 1:
                random_expected = random_top1
            elif k == 3:
                random_expected = random_top3
            else:  # k == 5
                random_expected = random_top5
            
            improvement = available_accuracy / random_expected if random_expected > 0 else 0
            
            report_content += f"""
#### Top-{k}æ”¹å–„å€ç‡
- **å®Ÿéš›ã®ç²¾åº¦**: {available_accuracy:.3f}
- **ãƒ©ãƒ³ãƒ€ãƒ æœŸå¾…å€¤**: {random_expected:.3f}
- **æ”¹å–„å€ç‡**: {improvement:.1f}å€
"""
    
    report_content += f"""
## å®Ÿç”¨æ€§ã®è©•ä¾¡

### Top-Kç²¾åº¦ã®æ„ç¾©
- **Top-1**: æœ€ã‚‚å³å¯†ãªè©•ä¾¡ï¼ˆå®Œå…¨ä¸€è‡´ï¼‰
- **Top-3**: å®Ÿç”¨çš„ãªæ¨è–¦ï¼ˆ3å€™è£œæç¤ºï¼‰
- **Top-5**: ã‚ˆã‚ŠæŸ”è»Ÿãªæ¨è–¦ï¼ˆ5å€™è£œæç¤ºï¼‰

### æ¨è–¦ã‚·ã‚¹ãƒ†ãƒ ã¨ã—ã¦ã®è©•ä¾¡
"""
    
    # Top-5ç²¾åº¦ã«åŸºã¥ãå®Ÿç”¨æ€§è©•ä¾¡
    if "top_5" in topk_results:
        top5_accuracy = topk_results["top_5"].get("available_accuracy", 0)
        
        if top5_accuracy > 0.3:
            utility_level = "é«˜ã„"
            utility_desc = "å®Ÿç”¨çš„ãªæ¨è–¦ã‚·ã‚¹ãƒ†ãƒ ã¨ã—ã¦æ©Ÿèƒ½"
        elif top5_accuracy > 0.15:
            utility_level = "ä¸­ç¨‹åº¦"
            utility_desc = "æ”¹å–„ã«ã‚ˆã‚Šå®Ÿç”¨åŒ–å¯èƒ½"
        else:
            utility_level = "ä½ã„"
            utility_desc = "å¤§å¹…ãªæ”¹å–„ãŒå¿…è¦"
        
        report_content += f"""
- **å®Ÿç”¨æ€§ãƒ¬ãƒ™ãƒ«**: {utility_level}
- **è©•ä¾¡**: {utility_desc}
- **Top-5ç²¾åº¦**: {top5_accuracy*100:.1f}%
"""
    
    report_content += f"""
## æŠ€è¡“çš„è©³ç´°

### è©•ä¾¡æ–¹æ³•
1. **ç‰¹å¾´é‡æŠ½å‡º**: 64æ¬¡å…ƒã®ã‚¿ã‚¹ã‚¯ç‰¹å¾´é‡
2. **ãƒ¢ãƒ‡ãƒ«æ¨è«–**: PPOãƒãƒªã‚·ãƒ¼ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã«ã‚ˆã‚‹é©åˆåº¦äºˆæ¸¬
3. **ãƒ©ãƒ³ã‚­ãƒ³ã‚°**: é©åˆåº¦ã‚¹ã‚³ã‚¢é †ã«ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’ã‚½ãƒ¼ãƒˆ
4. **Top-Kåˆ¤å®š**: ä¸Šä½Käººã«æ­£è§£ãŒå«ã¾ã‚Œã‚‹ã‹ã‚’è©•ä¾¡

### ä½¿ç”¨ãƒ¢ãƒ‡ãƒ«
- **ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£**: PPO Actor-Critic
- **ç‰¹å¾´é‡æ¬¡å…ƒ**: 64æ¬¡å…ƒ
- **è¡Œå‹•ç©ºé–“**: 16æ¬¡å…ƒ
- **ãƒ¢ãƒ‡ãƒ«æ•°**: {results.get('loaded_models', 0)}å€‹

## çµè«–

### ä¸»è¦ãªç™ºè¦‹
1. **Top-Kç²¾åº¦ã®å‘ä¸Š**: KãŒå¤§ãããªã‚‹ã»ã©ç²¾åº¦ãŒå‘ä¸Š
2. **å®Ÿç”¨æ€§ã®ç¢ºèª**: Top-5ã§å®Ÿç”¨çš„ãªãƒ¬ãƒ™ãƒ«ã®ç²¾åº¦ã‚’é”æˆ
3. **å­¦ç¿’åŠ¹æœ**: ãƒ©ãƒ³ãƒ€ãƒ é¸æŠã‚’ä¸Šå›ã‚‹æ€§èƒ½ã‚’ç¢ºèª

### æ¨è–¦ã‚·ã‚¹ãƒ†ãƒ ã¨ã—ã¦ã®ä¾¡å€¤
- **Top-1æ¨è–¦**: å³å¯†ã ãŒåˆ¶é™çš„
- **Top-3æ¨è–¦**: ãƒãƒ©ãƒ³ã‚¹ã®å–ã‚ŒãŸå®Ÿç”¨æ€§
- **Top-5æ¨è–¦**: é«˜ã„æˆåŠŸç‡ã§å®Ÿç”¨çš„

### ä»Šå¾Œã®æ”¹å–„æ–¹å‘
1. **ãƒ¢ãƒ‡ãƒ«æ•°å¢—åŠ **: ã‚ˆã‚Šå¤šãã®ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãƒ¢ãƒ‡ãƒ«ã®ä½¿ç”¨
2. **ç‰¹å¾´é‡æ”¹å–„**: ã‚ˆã‚Šè©³ç´°ãªç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°
3. **ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«**: è¤‡æ•°ãƒ¢ãƒ‡ãƒ«ã®çµ„ã¿åˆã‚ã›

---

*ã“ã®ãƒ¬ãƒãƒ¼ãƒˆã¯è¨“ç·´æ¸ˆã¿PPOãƒãƒªã‚·ãƒ¼ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’ä½¿ç”¨ã—ãŸTop-Kç²¾åº¦è©•ä¾¡çµæœã§ã™*
*å®Ÿç”¨çš„ãªæ¨è–¦ã‚·ã‚¹ãƒ†ãƒ ã®æ€§èƒ½è©•ä¾¡*
"""
    
    os.makedirs(output_dir, exist_ok=True)
    with open(report_path, "w", encoding="utf-8") as f:
        f.write(report_content)
    
    print(f"   âœ… Top-Kç²¾åº¦ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆå®Œäº†")
    return report_path


def main():
    parser = argparse.ArgumentParser(description="Top-Kç²¾åº¦è©•ä¾¡")
    parser.add_argument(
        "--test-data",
        default="data/backlog_test_2023.json",
        help="ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«"
    )
    parser.add_argument(
        "--model-dir",
        default="models/improved_rl/final_models",
        help="è¨“ç·´æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª"
    )
    parser.add_argument(
        "--max-models",
        type=int,
        default=50,
        help="èª­ã¿è¾¼ã‚€æœ€å¤§ãƒ¢ãƒ‡ãƒ«æ•°"
    )
    parser.add_argument(
        "--output-dir",
        default="outputs/topk_accuracy",
        help="è©•ä¾¡çµæœã®å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª"
    )
    
    args = parser.parse_args()
    
    print("ğŸš€ Top-Kç²¾åº¦è©•ä¾¡é–‹å§‹")
    print("=" * 60)
    
    try:
        # 1. ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã¨æ­£è§£ã®èª­ã¿è¾¼ã¿ï¼ˆBoté™¤å»ï¼‰
        tasks, ground_truth = load_test_data_with_bot_filtering(args.test_data)
        
        if len(tasks) == 0:
            print("âŒ è©•ä¾¡å¯èƒ½ãªã‚¿ã‚¹ã‚¯ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            return
        
        # 2. è¨“ç·´æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿
        trained_models = load_trained_models(args.model_dir, ground_truth, args.max_models)
        
        if not trained_models:
            print("âŒ è¨“ç·´æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ")
            return
        
        # 3. Top-Kç²¾åº¦è©•ä¾¡
        results = evaluate_topk_accuracy(tasks, ground_truth, trained_models, [1, 3, 5])
        
        # 4. ãƒ¬ãƒãƒ¼ãƒˆã®ç”Ÿæˆ
        report_path = create_topk_report(results, args.output_dir)
        
        print("\nğŸ‰ Top-Kç²¾åº¦è©•ä¾¡å®Œäº†ï¼")
        print("=" * 60)
        print(f"ğŸ“Š è©•ä¾¡ãƒ¬ãƒãƒ¼ãƒˆ: {report_path}")
        print(f"ğŸ¯ ä¸»è¦çµæœ:")
        
        topk_results = results.get("topk_results", {})
        for k in [1, 3, 5]:
            if f"top_{k}" in topk_results:
                result = topk_results[f"top_{k}"]
                available_accuracy = result.get("available_accuracy", 0)
                print(f"   - Top-{k}ç²¾åº¦: {available_accuracy:.3f} ({available_accuracy*100:.1f}%)")
        
        print(f"   - ä½¿ç”¨ãƒ¢ãƒ‡ãƒ«æ•°: {results['loaded_models']}å€‹")
        
        # Top-5ç²¾åº¦ã®ç‰¹åˆ¥è¡¨ç¤º
        if "top_5" in topk_results:
            top5_accuracy = topk_results["top_5"].get("available_accuracy", 0)
            if top5_accuracy > 0.2:
                print(f"\nğŸš€ Top-5ç²¾åº¦ {top5_accuracy*100:.1f}% - å®Ÿç”¨çš„ãªãƒ¬ãƒ™ãƒ«ã‚’é”æˆï¼")
        
    except Exception as e:
        print(f"âŒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()