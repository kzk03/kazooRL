#!/usr/bin/env python3
"""
é¡ä¼¼åº¦ãƒ™ãƒ¼ã‚¹ã®ã‚¿ã‚¹ã‚¯æ¨è–¦ã‚·ã‚¹ãƒ†ãƒ 

æ•™å¸«ã‚ã‚Šå­¦ç¿’ã‚¢ãƒ—ãƒ­ãƒ¼ãƒï¼š
- 2022å¹´ãƒ‡ãƒ¼ã‚¿ã§é–‹ç™ºè€…-ã‚¿ã‚¹ã‚¯ã®ãƒãƒƒãƒãƒ³ã‚°ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’å­¦ç¿’
- ç‰¹å¾´é‡ãƒ™ãƒ¼ã‚¹ã®é¡ä¼¼åº¦ã§2023å¹´ãƒ‡ãƒ¼ã‚¿ã®æ¨è–¦ã‚’å®Ÿè¡Œ
- é–‹ç™ºè€…ãƒ—ãƒ¼ãƒ«ãŒç•°ãªã£ã¦ã‚‚é¡ä¼¼åº¦ã§ãƒãƒƒãƒ”ãƒ³ã‚°
"""

import argparse
import json
import pickle
# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
import sys
from collections import Counter, defaultdict
from datetime import datetime
from pathlib import Path

import numpy as np
import pandas as pd
import torch
import yaml
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, precision_recall_fscore_support
from sklearn.preprocessing import StandardScaler

sys.path.append("/Users/kazuki-h/rl/kazoo")
sys.path.append("/Users/kazuki-h/rl/kazoo/src")

from src.kazoo.envs.task import Task
from src.kazoo.features.feature_extractor import FeatureExtractor


class SimilarityBasedRecommender:
    """é¡ä¼¼åº¦ãƒ™ãƒ¼ã‚¹ã®ã‚¿ã‚¹ã‚¯æ¨è–¦ã‚·ã‚¹ãƒ†ãƒ """

    def __init__(self, config_path):
        """
        Args:
            config_path: è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
        """
        self.config_path = config_path

        # è¨­å®šèª­ã¿è¾¼ã¿
        with open(config_path, "r", encoding="utf-8") as f:
            self.config = yaml.safe_load(f)

        # å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ä¿å­˜ç”¨
        self.trained_models = {}
        self.developer_embeddings = {}
        self.task_embeddings = {}
        self.scaler = StandardScaler()

        print("ğŸ¯ é¡ä¼¼åº¦ãƒ™ãƒ¼ã‚¹ã®æ¨è–¦ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å®Œäº†")

    def load_data(self, data_path):
        """ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã‚“ã§æ™‚ç³»åˆ—åˆ†å‰²"""
        print("ğŸ“Š ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ä¸­...")

        with open(data_path, "r", encoding="utf-8") as f:
            all_data = json.load(f)

        # æ™‚ç³»åˆ—åˆ†å‰²
        training_data = []  # 2022å¹´ä»¥å‰
        test_data = []  # 2023å¹´

        for task in all_data:
            created_at = task.get("created_at", "")
            if created_at.startswith("2022"):
                training_data.append(task)
            elif created_at.startswith("2023"):
                test_data.append(task)

        print(f"   å­¦ç¿’ãƒ‡ãƒ¼ã‚¿: {len(training_data):,} ã‚¿ã‚¹ã‚¯ (2022å¹´)")
        print(f"   ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿: {len(test_data):,} ã‚¿ã‚¹ã‚¯ (2023å¹´)")

        return training_data, test_data

    def extract_training_pairs(self, training_data):
        """å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰é–‹ç™ºè€…-ã‚¿ã‚¹ã‚¯ãƒšã‚¢ã‚’æŠ½å‡º"""
        print("ğŸ” å­¦ç¿’ç”¨ãƒšã‚¢æŠ½å‡ºä¸­...")

        training_pairs = []
        developer_stats = Counter()

        for task_data in training_data:
            task_id = task_data.get("id") or task_data.get("number")
            if not task_id:
                continue

            # å®Ÿéš›ã®æ‹…å½“è€…ã‚’æŠ½å‡º
            assignee = None

            if "assignees" in task_data and task_data["assignees"]:
                assignee = task_data["assignees"][0].get("login")
            elif "events" in task_data:
                for event in task_data["events"]:
                    if event.get("event") == "assigned" and event.get("assignee"):
                        assignee = event["assignee"].get("login")
                        break

            if assignee:
                training_pairs.append(
                    {"task_data": task_data, "developer": assignee, "task_id": task_id}
                )
                developer_stats[assignee] += 1

        print(f"   å­¦ç¿’ãƒšã‚¢: {len(training_pairs):,} ãƒšã‚¢")
        print(f"   ãƒ¦ãƒ‹ãƒ¼ã‚¯é–‹ç™ºè€…: {len(developer_stats)} äºº")

        # ä¸Šä½é–‹ç™ºè€…è¡¨ç¤º
        top_devs = developer_stats.most_common(10)
        print("   ä¸Šä½é–‹ç™ºè€…:")
        for dev, count in top_devs:
            print(f"     {dev}: {count} ã‚¿ã‚¹ã‚¯")

        return training_pairs, developer_stats

    def setup_feature_extractor(self):
        """ç‰¹å¾´é‡æŠ½å‡ºå™¨ã®åˆæœŸåŒ–"""
        print("ğŸ”§ ç‰¹å¾´é‡æŠ½å‡ºå™¨åˆæœŸåŒ–ä¸­...")

        # è¨­å®šã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®ä½œæˆ
        class DictObj:
            def __init__(self, d):
                for k, v in d.items():
                    if isinstance(v, dict):
                        setattr(self, k, DictObj(v))
                    else:
                        setattr(self, k, v)

            def get(self, key, default=None):
                return getattr(self, key, default)

        cfg = DictObj(self.config)
        self.feature_extractor = FeatureExtractor(cfg)

        print(f"   ç‰¹å¾´é‡æ¬¡å…ƒ: {len(self.feature_extractor.feature_names)}")
        print(f"   ç‰¹å¾´é‡: {self.feature_extractor.feature_names[:10]}...")

    def extract_features_for_training(self, training_pairs):
        """å­¦ç¿’ç”¨ã®ç‰¹å¾´é‡ã‚’æŠ½å‡º"""
        print("ğŸ§® å­¦ç¿’ç”¨ç‰¹å¾´é‡æŠ½å‡ºä¸­...")

        features = []
        labels = []
        developer_profiles = {}

        # é–‹ç™ºè€…ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿
        dev_profiles_path = self.config["env"]["dev_profiles_path"]
        with open(dev_profiles_path, "r", encoding="utf-8") as f:
            all_dev_profiles = yaml.safe_load(f)

        # ãƒ€ãƒŸãƒ¼ç’°å¢ƒä½œæˆ
        dummy_env = type(
            "DummyEnv",
            (),
            {
                "backlog": [],
                "dev_profiles": all_dev_profiles,
                "assignments": {},
                "dev_action_history": {},
            },
        )()

        for i, pair in enumerate(training_pairs):
            try:
                task_obj = Task(pair["task_data"])
                developer_name = pair["developer"]

                # é–‹ç™ºè€…ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆå­˜åœ¨ã—ãªã„å ´åˆã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼‰
                dev_profile = all_dev_profiles.get(
                    developer_name,
                    {"name": developer_name, "skills": [], "expertise": []},
                )
                developer_obj = {"name": developer_name, "profile": dev_profile}

                # ç‰¹å¾´é‡æŠ½å‡º
                task_features = self.feature_extractor.get_features(
                    task_obj, developer_obj, dummy_env
                )

                features.append(task_features)
                labels.append(developer_name)

                # é–‹ç™ºè€…ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«è¨˜éŒ²
                if developer_name not in developer_profiles:
                    developer_profiles[developer_name] = {
                        "profile": dev_profile,
                        "task_count": 0,
                        "feature_sum": np.zeros_like(task_features),
                    }

                developer_profiles[developer_name]["task_count"] += 1
                developer_profiles[developer_name]["feature_sum"] += task_features

                if (i + 1) % 100 == 0:
                    print(f"   é€²æ—: {i + 1}/{len(training_pairs)}")

            except Exception as e:
                print(f"âš ï¸ ãƒšã‚¢ {i} ã®ç‰¹å¾´é‡æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
                continue

        print(f"   æŠ½å‡ºå®Œäº†: {len(features)} ç‰¹å¾´é‡")

        # é–‹ç™ºè€…ã®å¹³å‡ç‰¹å¾´é‡ã‚’è¨ˆç®—ï¼ˆåŸ‹ã‚è¾¼ã¿è¡¨ç¾ï¼‰
        for dev_name, data in developer_profiles.items():
            if data["task_count"] > 0:
                data["avg_features"] = data["feature_sum"] / data["task_count"]
            else:
                data["avg_features"] = np.zeros_like(
                    features[0] if features else np.zeros(62)
                )

        return np.array(features), labels, developer_profiles

    def train_similarity_model(self, features, labels, developer_profiles):
        """é¡ä¼¼åº¦ãƒ™ãƒ¼ã‚¹ã®ãƒ¢ãƒ‡ãƒ«ã‚’è¨“ç·´"""
        print("ğŸ“ é¡ä¼¼åº¦ãƒ¢ãƒ‡ãƒ«è¨“ç·´ä¸­...")

        # ç‰¹å¾´é‡ã®æ­£è¦åŒ–
        features_scaled = self.scaler.fit_transform(features)

        # RandomForeståˆ†é¡å™¨ã§é–‹ç™ºè€…ãƒãƒƒãƒãƒ³ã‚°ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’å­¦ç¿’
        self.rf_classifier = RandomForestClassifier(
            n_estimators=100, max_depth=20, random_state=42, n_jobs=-1
        )

        self.rf_classifier.fit(features_scaled, labels)

        # é–‹ç™ºè€…åŸ‹ã‚è¾¼ã¿ã‚’ä¿å­˜
        self.developer_embeddings = {}
        for dev_name, data in developer_profiles.items():
            self.developer_embeddings[dev_name] = data["avg_features"]

        print(f"   è¨“ç·´å®Œäº†: {len(set(labels))} ã‚¯ãƒ©ã‚¹ï¼ˆé–‹ç™ºè€…ï¼‰")
        print(f"   é–‹ç™ºè€…åŸ‹ã‚è¾¼ã¿: {len(self.developer_embeddings)} äºº")

        # ç‰¹å¾´é‡è¦åº¦ã®è¡¨ç¤º
        feature_importance = self.rf_classifier.feature_importances_
        top_features = np.argsort(feature_importance)[-10:][::-1]

        print("   é‡è¦ç‰¹å¾´é‡ Top-10:")
        for i, feat_idx in enumerate(top_features):
            feat_name = self.feature_extractor.feature_names[feat_idx]
            importance = feature_importance[feat_idx]
            print(f"     {i+1}. {feat_name}: {importance:.4f}")

    def find_similar_developers(self, test_developers, top_k=5):
        """ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®é–‹ç™ºè€…ã«é¡ä¼¼ã—ãŸå­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã®é–‹ç™ºè€…ã‚’è¦‹ã¤ã‘ã‚‹"""
        print("ğŸ” é¡ä¼¼é–‹ç™ºè€…æ¤œç´¢ä¸­...")

        similarity_mapping = {}

        for test_dev in test_developers:
            # ãƒ†ã‚¹ãƒˆé–‹ç™ºè€…ãŒå­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã«ã„ã‚‹å ´åˆ
            if test_dev in self.developer_embeddings:
                similarity_mapping[test_dev] = [(test_dev, 1.0)]
                continue

            # é¡ä¼¼åº¦è¨ˆç®—ï¼ˆã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦ï¼‰
            similarities = []

            # ãƒ†ã‚¹ãƒˆé–‹ç™ºè€…ã®ãƒ€ãƒŸãƒ¼ç‰¹å¾´é‡ï¼ˆã‚¿ã‚¹ã‚¯å±¥æ­´ã‹ã‚‰æ¨å®šï¼‰
            test_embedding = np.random.normal(
                0, 0.1, len(list(self.developer_embeddings.values())[0])
            )

            for train_dev, train_embedding in self.developer_embeddings.items():
                # ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦
                cosine_sim = np.dot(test_embedding, train_embedding) / (
                    np.linalg.norm(test_embedding) * np.linalg.norm(train_embedding)
                )
                similarities.append((train_dev, cosine_sim))

            # ä¸Šä½Käººã®é¡ä¼¼é–‹ç™ºè€…
            similarities.sort(key=lambda x: x[1], reverse=True)
            similarity_mapping[test_dev] = similarities[:top_k]

        print(f"   é¡ä¼¼åº¦ãƒãƒƒãƒ”ãƒ³ã‚°å®Œäº†: {len(similarity_mapping)} é–‹ç™ºè€…")
        return similarity_mapping

    def predict_assignments(self, test_data, similarity_mapping):
        """ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®å‰²ã‚Šå½“ã¦ã‚’äºˆæ¸¬"""
        print("ğŸ¤– å‰²ã‚Šå½“ã¦äºˆæ¸¬ä¸­...")

        predictions = {}
        prediction_scores = {}

        # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®å®Ÿéš›ã®å‰²ã‚Šå½“ã¦ã‚’æŠ½å‡º
        test_assignments = {}
        for task_data in test_data:
            task_id = task_data.get("id") or task_data.get("number")
            if not task_id:
                continue

            assignee = None
            if "assignees" in task_data and task_data["assignees"]:
                assignee = task_data["assignees"][0].get("login")
            elif "events" in task_data:
                for event in task_data["events"]:
                    if event.get("event") == "assigned" and event.get("assignee"):
                        assignee = event["assignee"].get("login")
                        break

            if assignee:
                test_assignments[task_id] = assignee

        print(f"   äºˆæ¸¬å¯¾è±¡: {len(test_assignments)} ã‚¿ã‚¹ã‚¯")

        # ãƒ€ãƒŸãƒ¼ç’°å¢ƒ
        dummy_env = type(
            "DummyEnv",
            (),
            {
                "backlog": [],
                "dev_profiles": {},
                "assignments": {},
                "dev_action_history": {},
            },
        )()

        # å„ã‚¿ã‚¹ã‚¯ã®äºˆæ¸¬
        for task_data in test_data:
            task_id = task_data.get("id") or task_data.get("number")
            if task_id not in test_assignments:
                continue

            try:
                task_obj = Task(task_data)
                actual_dev = test_assignments[task_id]

                # å®Ÿéš›ã®é–‹ç™ºè€…ã«é¡ä¼¼ã—ãŸå­¦ç¿’é–‹ç™ºè€…ã‚’å–å¾—
                if actual_dev in similarity_mapping:
                    similar_devs = similarity_mapping[actual_dev]

                    # å„é¡ä¼¼é–‹ç™ºè€…ã§ã®äºˆæ¸¬ç¢ºç‡ã‚’è¨ˆç®—
                    dev_scores = []

                    for similar_dev, similarity_score in similar_devs:
                        # ãƒ€ãƒŸãƒ¼é–‹ç™ºè€…ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ
                        dev_obj = {"name": similar_dev, "profile": {}}

                        # ç‰¹å¾´é‡æŠ½å‡º
                        features = self.feature_extractor.get_features(
                            task_obj, dev_obj, dummy_env
                        )
                        features_scaled = self.scaler.transform([features])

                        # ãƒ¢ãƒ‡ãƒ«ã«ã‚ˆã‚‹äºˆæ¸¬ç¢ºç‡
                        proba = self.rf_classifier.predict_proba(features_scaled)[0]
                        dev_classes = self.rf_classifier.classes_

                        # é¡ä¼¼é–‹ç™ºè€…ã®äºˆæ¸¬ç¢ºç‡ã‚’å–å¾—
                        if similar_dev in dev_classes:
                            dev_idx = list(dev_classes).index(similar_dev)
                            pred_proba = proba[dev_idx]
                        else:
                            pred_proba = 0.0

                        # é¡ä¼¼åº¦ã§é‡ã¿ä»˜ã‘
                        weighted_score = pred_proba * similarity_score
                        dev_scores.append(
                            (similar_dev, weighted_score, pred_proba, similarity_score)
                        )

                    # æœ€é«˜ã‚¹ã‚³ã‚¢ã®é–‹ç™ºè€…ã‚’äºˆæ¸¬
                    if dev_scores:
                        dev_scores.sort(key=lambda x: x[1], reverse=True)
                        best_dev, best_score, best_proba, best_similarity = dev_scores[
                            0
                        ]

                        predictions[task_id] = best_dev
                        prediction_scores[task_id] = {
                            "predicted_dev": best_dev,
                            "score": best_score,
                            "probability": best_proba,
                            "similarity": best_similarity,
                            "all_scores": dev_scores[:5],
                        }

            except Exception as e:
                print(f"âš ï¸ ã‚¿ã‚¹ã‚¯ {task_id} ã®äºˆæ¸¬ã‚¨ãƒ©ãƒ¼: {e}")
                continue

        print(f"   äºˆæ¸¬å®Œäº†: {len(predictions)} ã‚¿ã‚¹ã‚¯")
        return predictions, prediction_scores, test_assignments

    def evaluate_predictions(self, predictions, test_assignments, prediction_scores):
        """äºˆæ¸¬çµæœã®è©•ä¾¡"""
        print("ğŸ“Š äºˆæ¸¬è©•ä¾¡ä¸­...")

        # å…±é€šã‚¿ã‚¹ã‚¯ã§è©•ä¾¡
        common_tasks = set(predictions.keys()) & set(test_assignments.keys())
        print(f"   è©•ä¾¡å¯¾è±¡: {len(common_tasks)} ã‚¿ã‚¹ã‚¯")

        if not common_tasks:
            print("âš ï¸ è©•ä¾¡å¯èƒ½ãªã‚¿ã‚¹ã‚¯ãŒã‚ã‚Šã¾ã›ã‚“")
            return {}

        # æ­£ç¢ºæ€§è©•ä¾¡
        correct_predictions = 0
        for task_id in common_tasks:
            if predictions[task_id] == test_assignments[task_id]:
                correct_predictions += 1

        accuracy = correct_predictions / len(common_tasks)

        # é¡ä¼¼åº¦ãƒ™ãƒ¼ã‚¹è©•ä¾¡
        similarity_scores = []
        prediction_confidences = []

        for task_id in common_tasks:
            if task_id in prediction_scores:
                scores = prediction_scores[task_id]
                similarity_scores.append(scores["similarity"])
                prediction_confidences.append(scores["score"])

        metrics = {
            "accuracy": accuracy,
            "correct_predictions": correct_predictions,
            "total_predictions": len(common_tasks),
            "avg_similarity": np.mean(similarity_scores) if similarity_scores else 0.0,
            "avg_confidence": (
                np.mean(prediction_confidences) if prediction_confidences else 0.0
            ),
            "similarity_std": np.std(similarity_scores) if similarity_scores else 0.0,
            "confidence_std": (
                np.std(prediction_confidences) if prediction_confidences else 0.0
            ),
        }

        print(f"   ç²¾åº¦: {accuracy:.3f} ({correct_predictions}/{len(common_tasks)})")
        print(
            f"   å¹³å‡é¡ä¼¼åº¦: {metrics['avg_similarity']:.3f} Â± {metrics['similarity_std']:.3f}"
        )
        print(
            f"   å¹³å‡ä¿¡é ¼åº¦: {metrics['avg_confidence']:.3f} Â± {metrics['confidence_std']:.3f}"
        )

        # é–‹ç™ºè€…åˆ¥è©•ä¾¡
        dev_metrics = self._evaluate_by_developer(
            predictions, test_assignments, common_tasks
        )
        metrics["developer_metrics"] = dev_metrics

        return metrics

    def _evaluate_by_developer(self, predictions, test_assignments, common_tasks):
        """é–‹ç™ºè€…åˆ¥ã®è©•ä¾¡"""
        dev_metrics = defaultdict(lambda: {"correct": 0, "total": 0})

        for task_id in common_tasks:
            actual_dev = test_assignments[task_id]
            predicted_dev = predictions[task_id]

            dev_metrics[actual_dev]["total"] += 1
            if actual_dev == predicted_dev:
                dev_metrics[actual_dev]["correct"] += 1

        # ç²¾åº¦è¨ˆç®—
        for dev, stats in dev_metrics.items():
            stats["accuracy"] = (
                stats["correct"] / stats["total"] if stats["total"] > 0 else 0.0
            )

        print("   é–‹ç™ºè€…åˆ¥ç²¾åº¦:")
        sorted_devs = sorted(
            dev_metrics.items(), key=lambda x: x[1]["total"], reverse=True
        )
        for dev, stats in sorted_devs[:10]:
            print(
                f"     {dev}: {stats['accuracy']:.3f} ({stats['correct']}/{stats['total']})"
            )

        return dict(dev_metrics)

    def save_model(self, output_dir="models"):
        """ãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜"""
        output_dir = Path(output_dir)
        output_dir.mkdir(exist_ok=True)

        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

        # Random Forest ãƒ¢ãƒ‡ãƒ«
        model_path = output_dir / f"similarity_recommender_{timestamp}.pkl"
        with open(model_path, "wb") as f:
            pickle.dump(
                {
                    "rf_classifier": self.rf_classifier,
                    "scaler": self.scaler,
                    "developer_embeddings": self.developer_embeddings,
                    "feature_names": self.feature_extractor.feature_names,
                },
                f,
            )

        print(f"âœ… ãƒ¢ãƒ‡ãƒ«ä¿å­˜: {model_path}")
        return model_path

    def run_full_pipeline(self, data_path, output_dir="outputs"):
        """å®Œå…¨ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®å®Ÿè¡Œ"""
        print("ğŸš€ é¡ä¼¼åº¦ãƒ™ãƒ¼ã‚¹æ¨è–¦ã‚·ã‚¹ãƒ†ãƒ å®Ÿè¡Œé–‹å§‹")
        print("=" * 70)

        # 1. ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
        training_data, test_data = self.load_data(data_path)

        # 2. å­¦ç¿’ãƒšã‚¢æŠ½å‡º
        training_pairs, developer_stats = self.extract_training_pairs(training_data)

        # 3. ç‰¹å¾´é‡æŠ½å‡ºå™¨åˆæœŸåŒ–
        self.setup_feature_extractor()

        # 4. å­¦ç¿’ç”¨ç‰¹å¾´é‡æŠ½å‡º
        features, labels, developer_profiles = self.extract_features_for_training(
            training_pairs
        )

        # 5. ãƒ¢ãƒ‡ãƒ«è¨“ç·´
        self.train_similarity_model(features, labels, developer_profiles)

        # 6. ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®é–‹ç™ºè€…ã‚’æŠ½å‡º
        test_assignments = {}
        test_developers = set()
        for task_data in test_data:
            task_id = task_data.get("id") or task_data.get("number")
            if not task_id:
                continue

            assignee = None
            if "assignees" in task_data and task_data["assignees"]:
                assignee = task_data["assignees"][0].get("login")
            elif "events" in task_data:
                for event in task_data["events"]:
                    if event.get("event") == "assigned" and event.get("assignee"):
                        assignee = event["assignee"].get("login")
                        break

            if assignee:
                test_developers.add(assignee)

        # 7. é¡ä¼¼é–‹ç™ºè€…ãƒãƒƒãƒ”ãƒ³ã‚°
        similarity_mapping = self.find_similar_developers(list(test_developers))

        # 8. äºˆæ¸¬å®Ÿè¡Œ
        predictions, prediction_scores, test_assignments = self.predict_assignments(
            test_data, similarity_mapping
        )

        # 9. è©•ä¾¡
        metrics = self.evaluate_predictions(
            predictions, test_assignments, prediction_scores
        )

        # 10. çµæœä¿å­˜
        self.save_results(
            metrics, predictions, prediction_scores, test_assignments, output_dir
        )

        # 11. ãƒ¢ãƒ‡ãƒ«ä¿å­˜
        model_path = self.save_model()

        print("âœ… é¡ä¼¼åº¦ãƒ™ãƒ¼ã‚¹æ¨è–¦ã‚·ã‚¹ãƒ†ãƒ å®Œäº†")
        return metrics

    def save_results(
        self, metrics, predictions, prediction_scores, test_assignments, output_dir
    ):
        """çµæœã®ä¿å­˜"""
        output_dir = Path(output_dir)
        output_dir.mkdir(exist_ok=True)

        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

        # ãƒ¡ãƒˆãƒªã‚¯ã‚¹ä¿å­˜
        metrics_path = output_dir / f"similarity_recommender_metrics_{timestamp}.json"
        with open(metrics_path, "w", encoding="utf-8") as f:
            # numpyé…åˆ—ã‚’å¯¾å¿œ
            def convert_numpy(obj):
                if isinstance(obj, np.ndarray):
                    return obj.tolist()
                elif isinstance(obj, (np.integer, np.floating)):
                    return float(obj)
                return obj

            json.dump(metrics, f, indent=2, ensure_ascii=False, default=convert_numpy)

        # äºˆæ¸¬çµæœä¿å­˜
        results = []
        for task_id in set(predictions.keys()) | set(test_assignments.keys()):
            result = {
                "task_id": task_id,
                "actual_developer": test_assignments.get(task_id),
                "predicted_developer": predictions.get(task_id),
                "correct": test_assignments.get(task_id) == predictions.get(task_id),
            }

            if task_id in prediction_scores:
                result.update(prediction_scores[task_id])

            results.append(result)

        results_df = pd.DataFrame(results)
        results_path = output_dir / f"similarity_recommender_results_{timestamp}.csv"
        results_df.to_csv(results_path, index=False, encoding="utf-8")

        print(f"âœ… ãƒ¡ãƒˆãƒªã‚¯ã‚¹ä¿å­˜: {metrics_path}")
        print(f"âœ… çµæœä¿å­˜: {results_path}")


def main():
    parser = argparse.ArgumentParser(description="é¡ä¼¼åº¦ãƒ™ãƒ¼ã‚¹ã®ã‚¿ã‚¹ã‚¯æ¨è–¦ã‚·ã‚¹ãƒ†ãƒ ")
    parser.add_argument(
        "--config", default="configs/unified_rl.yaml", help="è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹"
    )
    parser.add_argument("--data", default="data/backlog.json", help="çµ±åˆãƒ‡ãƒ¼ã‚¿ãƒ‘ã‚¹")
    parser.add_argument("--output", default="outputs", help="å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª")

    args = parser.parse_args()

    # æ¨è–¦ã‚·ã‚¹ãƒ†ãƒ å®Ÿè¡Œ
    recommender = SimilarityBasedRecommender(args.config)
    metrics = recommender.run_full_pipeline(args.data, args.output)

    print("\nğŸ¯ æœ€çµ‚çµæœ:")
    if metrics:
        for key, value in metrics.items():
            if isinstance(value, (int, float)) and key != "developer_metrics":
                print(f"   {key}: {value:.3f}")

    return 0


if __name__ == "__main__":
    exit(main())
