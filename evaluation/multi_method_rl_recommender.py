#!/usr/bin/env python3
"""
ãƒãƒ«ãƒãƒ¡ã‚½ãƒƒãƒ‰å¼·åŒ–å­¦ç¿’ã‚¿ã‚¹ã‚¯æ¨è–¦ã‚·ã‚¹ãƒ†ãƒ 

3ã¤ã®ç•°ãªã‚‹é–‹ç™ºè€…æŠ½å‡ºæ–¹æ³•ã§ç‹¬ç«‹ã—ãŸå¼·åŒ–å­¦ç¿’ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’è¨“ç·´ã—ã€
ãã‚Œãã‚Œã®æ€§èƒ½ã‚’æ¯”è¼ƒã™ã‚‹ï¼š

1. assignees_agent: assigneesãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã®ã¿ä½¿ç”¨
2. creators_agent: Issue/PRä½œæˆè€…ï¼ˆuserï¼‰ã®ã¿ä½¿ç”¨  
3. all_agent: ã™ã¹ã¦ã®æ–¹æ³•ã‚’çµ±åˆï¼ˆassignees + creatorsï¼‰

ã“ã‚Œã«ã‚ˆã‚Šã€ãƒ‡ãƒ¼ã‚¿ãƒªãƒ¼ã‚¯ã‚„åã‚Šã®å½±éŸ¿ã‚’æœ€å°åŒ–ã—ã€
ã‚ˆã‚Šç¾å®Ÿçš„ãªæ¨è–¦ã‚·ã‚¹ãƒ†ãƒ ã®æ€§èƒ½è©•ä¾¡ã‚’è¡Œã†ã€‚
"""

import argparse
import json
from collections import Counter, defaultdict
from datetime import datetime
from pathlib import Path

import gymnasium as gym
import numpy as np
import torch
import torch.nn as nn
from gymnasium import spaces
from simple_similarity_recommender import SimpleSimilarityRecommender
from stable_baselines3 import PPO
from stable_baselines3.common.callbacks import EvalCallback
from stable_baselines3.common.env_util import make_vec_env


class MultiMethodRLEnvironment(gym.Env):
    """
    ãƒãƒ«ãƒãƒ¡ã‚½ãƒƒãƒ‰å¼·åŒ–å­¦ç¿’ç’°å¢ƒ
    
    å„æŠ½å‡ºæ–¹æ³•ã«ç‰¹åŒ–ã—ãŸç‹¬ç«‹ç’°å¢ƒã§å­¦ç¿’ã‚’è¡Œã†
    """
    
    def __init__(self, extraction_method='assignees'):
        super().__init__()
        
        self.extraction_method = extraction_method
        self.recommender = SimpleSimilarityRecommender('configs/unified_rl.yaml')
        
        # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
        self.training_data, self.test_data = self.recommender.load_data('data/backlog.json')
        
        # æŠ½å‡ºæ–¹æ³•ã«å¿œã˜ã¦å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‚’æº–å‚™
        self.training_pairs, self.developer_stats = self.recommender.extract_training_pairs(
            self.training_data, extraction_method=self.extraction_method
        )
        
        print(f"ğŸ¤– {extraction_method}ãƒ¡ã‚½ãƒƒãƒ‰RLç’°å¢ƒåˆæœŸåŒ–:")
        print(f"   æŠ½å‡ºæ–¹æ³•: {extraction_method}")
        print(f"   å­¦ç¿’ãƒšã‚¢: {len(self.training_pairs)} ãƒšã‚¢")
        print(f"   é–‹ç™ºè€…æ•°: {len(self.developer_stats)} äºº")
        
        # é–‹ç™ºè€…ãƒªã‚¹ãƒˆï¼ˆã“ã®æŠ½å‡ºæ–¹æ³•ã§è¦‹ã¤ã‹ã£ãŸé–‹ç™ºè€…ã®ã¿ï¼‰
        self.developers = list(self.developer_stats.keys())
        self.dev_to_idx = {dev: idx for idx, dev in enumerate(self.developers)}
        
        # å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ç®¡ç†
        self.current_task_idx = 0
        self.time_step = 0
        
        # å±¥æ­´ã¨ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹çµ±è¨ˆ
        self.prediction_history = []
        self.success_rate = 0.0
        self.recent_successes = []
        
        # ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ãƒ»è¦³æ¸¬ç©ºé–“å®šç¾©ï¼ˆé–‹ç™ºè€…æ•°ãŒ0ã®å ´åˆã®å¯¾ç­–ï¼‰
        num_developers = max(1, len(self.developers))  # æœ€ä½1ã¤ã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³
        self.action_space = spaces.Discrete(num_developers)
        self.observation_space = spaces.Box(
            low=0.0,
            high=1.0,
            shape=(20,),  # ã‚¿ã‚¹ã‚¯ç‰¹å¾´é‡(10) + é–‹ç™ºè€…çµ±è¨ˆ(5) + å±¥æ­´çµ±è¨ˆ(5)
            dtype=np.float32
        )
        
        # åŸºæœ¬ãƒ¢ãƒ‡ãƒ«è¨“ç·´
        self._train_base_models()
    
    def _train_base_models(self):
        """ãƒ™ãƒ¼ã‚¹ã¨ãªã‚‹é¡ä¼¼åº¦ãƒ¢ãƒ‡ãƒ«ã‚’è¨“ç·´"""
        print(f"ğŸ“š {self.extraction_method}ç”¨ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«è¨“ç·´ä¸­...")
        
        # é–‹ç™ºè€…ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«æ§‹ç¯‰
        self.developer_profiles = self.recommender.build_developer_profiles(self.training_pairs)
        
        # ãƒ†ã‚­ã‚¹ãƒˆé¡ä¼¼åº¦ãƒ¢ãƒ‡ãƒ«
        self.recommender.train_text_similarity_model(self.developer_profiles)
        
        # ç‰¹å¾´é‡ãƒ¢ãƒ‡ãƒ«  
        self.recommender.train_feature_model(self.training_pairs)
        
        print(f"   ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«è¨“ç·´å®Œäº†: TF-IDFè¡Œåˆ— {getattr(self.recommender, 'tfidf_matrix', 'N/A')}")
    
    def reset(self, seed=None, options=None):
        """ç’°å¢ƒã‚’ãƒªã‚»ãƒƒãƒˆ"""
        super().reset(seed=seed)
        
        self.current_task_idx = 0
        self.time_step = 0
        self.prediction_history = []
        self.recent_successes = []
        self.success_rate = 0.0
        
        obs = self._get_observation()
        info = {
            'extraction_method': self.extraction_method,
            'total_developers': len(self.developers),
            'total_tasks': len(self.training_pairs)
        }
        
        return obs, info
    
    def step(self, action):
        """ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚’å®Ÿè¡Œã—ã¦æ¬¡ã®çŠ¶æ…‹ã¸"""
        if self.current_task_idx >= len(self.training_pairs):
            # ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰çµ‚äº†
            obs = np.zeros(20, dtype=np.float32)
            return obs, 0.0, True, False, {'episode_complete': True}
        
        # ç¾åœ¨ã®ã‚¿ã‚¹ã‚¯ãƒ‡ãƒ¼ã‚¿
        current_pair = self.training_pairs[self.current_task_idx]
        current_task = current_pair['task_data']
        actual_developer = current_pair['developer']
        
        # ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‹ã‚‰äºˆæ¸¬é–‹ç™ºè€…ã‚’å–å¾—
        if action < len(self.developers):
            predicted_developer = self.developers[action]
        else:
            # ç„¡åŠ¹ãªã‚¢ã‚¯ã‚·ãƒ§ãƒ³
            predicted_developer = self.developers[0]
        
        # å ±é…¬è¨ˆç®—
        reward = self._calculate_reward(predicted_developer, actual_developer, current_task)
        
        # å±¥æ­´æ›´æ–°
        success = (predicted_developer == actual_developer)
        self.prediction_history.append({
            'task_idx': self.current_task_idx,
            'predicted': predicted_developer,
            'actual': actual_developer,
            'success': success,
            'reward': reward
        })
        
        self.recent_successes.append(success)
        if len(self.recent_successes) > 50:  # ç›´è¿‘50ä»¶ã®æˆåŠŸç‡
            self.recent_successes.pop(0)
        
        self.success_rate = np.mean(self.recent_successes) if self.recent_successes else 0.0
        
        # æ¬¡ã®ã‚¿ã‚¹ã‚¯ã¸
        self.current_task_idx += 1
        self.time_step += 1
        
        # æ¬¡ã®è¦³æ¸¬
        obs = self._get_observation()
        
        # çµ‚äº†åˆ¤å®š
        terminated = (self.current_task_idx >= len(self.training_pairs))
        truncated = False
        
        info = {
            'success': success,
            'predicted_dev': predicted_developer,
            'actual_dev': actual_developer,
            'success_rate': self.success_rate,
            'extraction_method': self.extraction_method
        }
        
        return obs, reward, terminated, truncated, info
    
    def _calculate_reward(self, predicted_dev, actual_dev, task_data):
        """å ±é…¬ã‚’è¨ˆç®—"""
        base_reward = 1.0 if predicted_dev == actual_dev else 0.0
        
        # æŠ½å‡ºæ–¹æ³•ç‰¹æœ‰ã®ãƒœãƒ¼ãƒŠã‚¹
        method_bonus = 0.0
        if self.extraction_method == 'assignees' and base_reward > 0:
            # assigneesã¯é«˜å“è³ªã ãŒã‚«ãƒãƒ¬ãƒƒã‚¸ãŒç‹­ã„
            method_bonus = 0.5
        elif self.extraction_method == 'creators' and base_reward > 0:
            # creatorsã¯å¹…åºƒã„ã‚«ãƒãƒ¬ãƒƒã‚¸
            method_bonus = 0.3
        elif self.extraction_method == 'all' and base_reward > 0:
            # allã¯ãƒãƒ©ãƒ³ã‚¹å‹
            method_bonus = 0.4
        
        # é–‹ç™ºè€…ã®æ´»å‹•ãƒ¬ãƒ™ãƒ«ã«ã‚ˆã‚‹èª¿æ•´
        dev_activity = self.developer_stats.get(actual_dev, 1)
        activity_factor = min(1.0, dev_activity / 10)  # æ­£è¦åŒ–
        
        # æœ€çµ‚å ±é…¬
        final_reward = base_reward + (method_bonus * activity_factor)
        
        return final_reward
    
    def _get_observation(self):
        """ç¾åœ¨ã®è¦³æ¸¬ã‚’å–å¾—"""
        if self.current_task_idx >= len(self.training_pairs):
            return np.zeros(20, dtype=np.float32)
        
        current_pair = self.training_pairs[self.current_task_idx]
        current_task = current_pair['task_data']
        
        # 1. ã‚¿ã‚¹ã‚¯ç‰¹å¾´é‡ (10æ¬¡å…ƒ)
        task_features = self._extract_task_features(current_task)
        
        # 2. é–‹ç™ºè€…ãƒ—ãƒ¼ãƒ«çµ±è¨ˆ (5æ¬¡å…ƒ)
        pool_stats = self._extract_pool_statistics()
        
        # 3. å±¥æ­´çµ±è¨ˆ (5æ¬¡å…ƒ)
        history_stats = self._extract_history_statistics()
        
        # å…¨ç‰¹å¾´é‡ã‚’çµåˆ
        obs = np.concatenate([
            task_features,
            pool_stats,
            history_stats
        ]).astype(np.float32)
        
        return obs
    
    def _extract_task_features(self, task_data):
        """ã‚¿ã‚¹ã‚¯ã‹ã‚‰ç‰¹å¾´é‡ã‚’æŠ½å‡º"""
        features = self.recommender.extract_basic_features(task_data)
        
        # æ­£è¦åŒ–ã•ã‚ŒãŸç‰¹å¾´é‡ãƒ™ã‚¯ãƒˆãƒ« (10æ¬¡å…ƒ)
        normalized_features = np.array([
            min(1.0, features.get('title_length', 0) / 100),
            min(1.0, features.get('body_length', 0) / 1000),
            min(1.0, features.get('comments_count', 0) / 20),
            features.get('is_bug', 0),
            features.get('is_enhancement', 0),
            features.get('is_documentation', 0),
            features.get('is_question', 0),
            features.get('is_help_wanted', 0),
            min(1.0, features.get('label_count', 0) / 10),
            features.get('is_open', 0)
        ], dtype=np.float32)
        
        return normalized_features
    
    def _extract_pool_statistics(self):
        """é–‹ç™ºè€…ãƒ—ãƒ¼ãƒ«ã®çµ±è¨ˆã‚’æŠ½å‡º (5æ¬¡å…ƒ)"""
        total_devs = len(self.developers)
        avg_activity = np.mean(list(self.developer_stats.values())) if self.developer_stats else 0
        max_activity = max(self.developer_stats.values()) if self.developer_stats else 0
        
        return np.array([
            min(1.0, total_devs / 100),        # æ­£è¦åŒ–ã•ã‚ŒãŸé–‹ç™ºè€…æ•°
            min(1.0, avg_activity / 50),       # æ­£è¦åŒ–ã•ã‚ŒãŸå¹³å‡æ´»å‹•åº¦
            min(1.0, max_activity / 200),      # æ­£è¦åŒ–ã•ã‚ŒãŸæœ€å¤§æ´»å‹•åº¦
            self.time_step / max(1, len(self.training_pairs)),  # é€²æ—ç‡
            min(1.0, self.current_task_idx / 100)  # æ­£è¦åŒ–ã•ã‚ŒãŸã‚¿ã‚¹ã‚¯ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
        ], dtype=np.float32)
    
    def _extract_history_statistics(self):
        """å±¥æ­´çµ±è¨ˆã‚’æŠ½å‡º (5æ¬¡å…ƒ)"""
        if not self.prediction_history:
            return np.zeros(5, dtype=np.float32)
        
        recent_history = self.prediction_history[-20:]  # ç›´è¿‘20ä»¶
        
        success_rate = np.mean([h['success'] for h in recent_history])
        avg_reward = np.mean([h['reward'] for h in recent_history])
        
        # é–‹ç™ºè€…ã®å¤šæ§˜æ€§ï¼ˆç›´è¿‘ã§ä½•äººã®ç•°ãªã‚‹é–‹ç™ºè€…ã‚’äºˆæ¸¬ã—ãŸã‹ï¼‰
        predicted_devs = set([h['predicted'] for h in recent_history])
        diversity = len(predicted_devs) / len(recent_history) if recent_history else 0
        
        return np.array([
            success_rate,
            avg_reward,
            diversity,
            self.success_rate,  # å…¨ä½“ã®æˆåŠŸç‡
            min(1.0, len(self.prediction_history) / 100)  # å±¥æ­´ã®é•·ã•
        ], dtype=np.float32)


class MultiMethodRLRecommender:
    """ãƒãƒ«ãƒãƒ¡ã‚½ãƒƒãƒ‰å¼·åŒ–å­¦ç¿’æ¨è–¦ã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self):
        self.extraction_methods = ['assignees', 'creators', 'all']
        self.agents = {}
        self.environments = {}
        self.results = {}
        
        print("ğŸ¯ ãƒãƒ«ãƒãƒ¡ã‚½ãƒƒãƒ‰å¼·åŒ–å­¦ç¿’æ¨è–¦ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–")
    
    def train_all_methods(self, timesteps=10000):
        """å…¨ã¦ã®æŠ½å‡ºæ–¹æ³•ã§ç‹¬ç«‹ã—ã¦ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’è¨“ç·´"""
        print("ğŸš€ ãƒãƒ«ãƒãƒ¡ã‚½ãƒƒãƒ‰è¨“ç·´é–‹å§‹")
        print("=" * 60)
        
        for method in self.extraction_methods:
            print(f"\nğŸ“š {method.upper()}ãƒ¡ã‚½ãƒƒãƒ‰è¨“ç·´é–‹å§‹...")
            
            # ç’°å¢ƒä½œæˆ
            env = MultiMethodRLEnvironment(extraction_method=method)
            self.environments[method] = env
            
            # PPOã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆä½œæˆ
            agent = PPO(
                "MlpPolicy",
                env,
                learning_rate=0.0003,
                n_steps=2048,
                batch_size=64,
                n_epochs=10,
                gamma=0.99,
                gae_lambda=0.95,
                clip_range=0.2,
                verbose=1,
                tensorboard_log=None  # ãƒ­ãƒ¼ã‚«ãƒ«ãƒ­ã‚°ã®ã¿
            )
            
            # è¨“ç·´å®Ÿè¡Œ
            print(f"ğŸ‹ï¸ {method}ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆè¨“ç·´ä¸­... ({timesteps:,} ã‚¹ãƒ†ãƒƒãƒ—)")
            agent.learn(total_timesteps=timesteps)
            
            self.agents[method] = agent
            
            print(f"âœ… {method}ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆè¨“ç·´å®Œäº†")
        
        print("\nğŸ‰ å…¨ãƒ¡ã‚½ãƒƒãƒ‰è¨“ç·´å®Œäº†")
    
    def evaluate_all_methods(self):
        """å…¨ã¦ã®æ–¹æ³•ã§ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’è©•ä¾¡"""
        print("ğŸ“Š ãƒãƒ«ãƒãƒ¡ã‚½ãƒƒãƒ‰è©•ä¾¡é–‹å§‹")
        print("=" * 60)
        
        # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿æº–å‚™
        recommender = SimpleSimilarityRecommender('configs/unified_rl.yaml')
        training_data, test_data = recommender.load_data('data/backlog.json')
        
        for method in self.extraction_methods:
            print(f"\nğŸ” {method.upper()}ãƒ¡ã‚½ãƒƒãƒ‰è©•ä¾¡ä¸­...")
            
            # ãƒ†ã‚¹ãƒˆç”¨ã®ç’°å¢ƒã¨ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ
            env = self.environments[method]
            agent = self.agents[method]
            
            # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã§ã®è©•ä¾¡
            test_results = self._evaluate_method(method, agent, env, test_data)
            self.results[method] = test_results
            
            print(f"   ç²¾åº¦: {test_results['accuracy']:.3f}")
            print(f"   äºˆæ¸¬æ•°: {test_results['total_predictions']}")
            print(f"   å¯¾è±¡é–‹ç™ºè€…æ•°: {test_results['unique_developers']}")
    
    def _evaluate_method(self, method, agent, env, test_data):
        """ç‰¹å®šã®æ–¹æ³•ã§ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’è©•ä¾¡"""
        # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‹ã‚‰è©²å½“ã™ã‚‹æŠ½å‡ºæ–¹æ³•ã§ã®ãƒšã‚¢ã‚’ä½œæˆ
        recommender = SimpleSimilarityRecommender('configs/unified_rl.yaml')
        test_assignments = {}
        
        for task_data in test_data:
            task_id = task_data.get('id') or task_data.get('number')
            if not task_id:
                continue
            
            # ã“ã®æŠ½å‡ºæ–¹æ³•ã§é–‹ç™ºè€…ã‚’æŠ½å‡º
            developers = recommender.extract_developers_from_task(task_data, method=method)
            if developers:
                developers.sort(key=lambda x: x['priority'])
                selected_dev = developers[0]
                test_assignments[task_id] = selected_dev['login']
        
        print(f"   {method}ã§ã®ãƒ†ã‚¹ãƒˆå‰²ã‚Šå½“ã¦: {len(test_assignments)} ã‚¿ã‚¹ã‚¯")
        
        # å®Ÿéš›ã®äºˆæ¸¬å®Ÿè¡Œ
        predictions = {}
        correct_predictions = 0
        
        for task_data in test_data:
            task_id = task_data.get('id') or task_data.get('number')
            if task_id not in test_assignments:
                continue
            
            # ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã«ã‚ˆã‚‹äºˆæ¸¬
            obs = self._create_test_observation(task_data, env)
            action, _ = agent.predict(obs, deterministic=True)
            
            if action < len(env.developers):
                predicted_dev = env.developers[action]
                predictions[task_id] = predicted_dev
                
                actual_dev = test_assignments[task_id]
                if predicted_dev == actual_dev:
                    correct_predictions += 1
        
        # çµæœé›†è¨ˆ
        accuracy = correct_predictions / len(test_assignments) if test_assignments else 0.0
        unique_developers = len(set(test_assignments.values()))
        
        return {
            'accuracy': accuracy,
            'correct_predictions': correct_predictions,
            'total_predictions': len(test_assignments),
            'unique_developers': unique_developers,
            'predictions': predictions,
            'test_assignments': test_assignments
        }
    
    def _create_test_observation(self, task_data, env):
        """ãƒ†ã‚¹ãƒˆã‚¿ã‚¹ã‚¯ç”¨ã®è¦³æ¸¬ã‚’ä½œæˆ"""
        # ã‚¿ã‚¹ã‚¯ç‰¹å¾´é‡
        task_features = env._extract_task_features(task_data)
        
        # ç’°å¢ƒçµ±è¨ˆï¼ˆè¨“ç·´æ™‚ã®çµ±è¨ˆã‚’ä½¿ç”¨ï¼‰
        pool_stats = env._extract_pool_statistics()
        
        # å±¥æ­´çµ±è¨ˆï¼ˆãƒ†ã‚¹ãƒˆæ™‚ã¯åˆæœŸå€¤ï¼‰
        history_stats = np.zeros(5, dtype=np.float32)
        
        obs = np.concatenate([task_features, pool_stats, history_stats])
        return obs
    
    def compare_results(self):
        """å…¨ãƒ¡ã‚½ãƒƒãƒ‰ã®çµæœã‚’æ¯”è¼ƒ"""
        print("\nğŸ“ˆ ãƒãƒ«ãƒãƒ¡ã‚½ãƒƒãƒ‰æ¯”è¼ƒçµæœ")
        print("=" * 60)
        
        comparison_data = []
        
        for method in self.extraction_methods:
            if method in self.results:
                result = self.results[method]
                comparison_data.append({
                    'method': method,
                    'accuracy': result['accuracy'],
                    'predictions': result['total_predictions'],
                    'developers': result['unique_developers'],
                    'coverage': result['total_predictions'] / max(1, sum(r['total_predictions'] for r in self.results.values())) * 100
                })
        
        # ã‚½ãƒ¼ãƒˆï¼ˆç²¾åº¦é †ï¼‰
        comparison_data.sort(key=lambda x: x['accuracy'], reverse=True)
        
        print("æ–¹æ³•         | ç²¾åº¦   | äºˆæ¸¬æ•° | é–‹ç™ºè€…æ•° | ã‚«ãƒãƒ¬ãƒƒã‚¸")
        print("-" * 55)
        for data in comparison_data:
            print(f"{data['method']:12} | {data['accuracy']:.3f} | {data['predictions']:6} | {data['developers']:8} | {data['coverage']:6.1f}%")
        
        # è©³ç´°åˆ†æ
        print(f"\nğŸ” è©³ç´°åˆ†æ:")
        for method in self.extraction_methods:
            if method in self.results:
                result = self.results[method]
                dev_counts = Counter(result['test_assignments'].values())
                print(f"\n{method.upper()}:")
                print(f"  ä¸Šä½é–‹ç™ºè€…:")
                for dev, count in dev_counts.most_common(5):
                    print(f"    {dev}: {count} ã‚¿ã‚¹ã‚¯")
        
        return comparison_data
    
    def save_results(self, output_dir="outputs"):
        """çµæœã‚’ä¿å­˜"""
        output_dir = Path(output_dir)
        output_dir.mkdir(exist_ok=True)
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # å„æ–¹æ³•ã®çµæœä¿å­˜
        for method in self.extraction_methods:
            if method in self.results:
                # ãƒ¡ãƒˆãƒªã‚¯ã‚¹ä¿å­˜
                metrics_path = output_dir / f"multi_method_{method}_metrics_{timestamp}.json"
                with open(metrics_path, 'w', encoding='utf-8') as f:
                    # numpyå‹ã‚’å¤‰æ›
                    result = self.results[method].copy()
                    result.pop('predictions', None)  # äºˆæ¸¬çµæœã¯åˆ¥ãƒ•ã‚¡ã‚¤ãƒ«
                    result.pop('test_assignments', None)
                    json.dump(result, f, indent=2, ensure_ascii=False)
                
                # ãƒ¢ãƒ‡ãƒ«ä¿å­˜
                agent_path = output_dir / f"multi_method_{method}_agent_{timestamp}.zip"
                self.agents[method].save(agent_path)
                
                print(f"âœ… {method}çµæœä¿å­˜: {metrics_path}")
                print(f"âœ… {method}ãƒ¢ãƒ‡ãƒ«ä¿å­˜: {agent_path}")


def main():
    parser = argparse.ArgumentParser(description='ãƒãƒ«ãƒãƒ¡ã‚½ãƒƒãƒ‰å¼·åŒ–å­¦ç¿’ã‚¿ã‚¹ã‚¯æ¨è–¦ã‚·ã‚¹ãƒ†ãƒ ')
    parser.add_argument('--timesteps', type=int, default=10000,
                       help='å„ãƒ¡ã‚½ãƒƒãƒ‰ã®è¨“ç·´ã‚¹ãƒ†ãƒƒãƒ—æ•°')
    parser.add_argument('--output', default='outputs',
                       help='å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª')
    
    args = parser.parse_args()
    
    # ãƒãƒ«ãƒãƒ¡ã‚½ãƒƒãƒ‰å¼·åŒ–å­¦ç¿’ã‚·ã‚¹ãƒ†ãƒ å®Ÿè¡Œ
    recommender = MultiMethodRLRecommender()
    
    # å…¨ãƒ¡ã‚½ãƒƒãƒ‰ã§è¨“ç·´
    recommender.train_all_methods(timesteps=args.timesteps)
    
    # å…¨ãƒ¡ã‚½ãƒƒãƒ‰ã§è©•ä¾¡
    recommender.evaluate_all_methods()
    
    # çµæœæ¯”è¼ƒ
    comparison_results = recommender.compare_results()
    
    # çµæœä¿å­˜
    recommender.save_results(args.output)
    
    print(f"\nğŸ¯ æœ€çµ‚çµæœ:")
    best_method = max(comparison_results, key=lambda x: x['accuracy'])
    print(f"   æœ€é«˜ç²¾åº¦: {best_method['method']} ({best_method['accuracy']:.3f})")
    print(f"   æœ€å¤§ã‚«ãƒãƒ¬ãƒƒã‚¸: {max(comparison_results, key=lambda x: x['predictions'])['method']}")
    
    return 0


if __name__ == "__main__":
    exit(main())
