#!/usr/bin/env python3
"""
è»½é‡ç‰ˆé‡ã¿åˆ†æãƒ‡ãƒãƒƒã‚°
"""

import json
import os
from collections import Counter, defaultdict
from typing import Dict, List, Tuple

import numpy as np
import torch
import torch.nn as nn
from tqdm import tqdm


class PPOPolicyNetwork(nn.Module):
    """PPOãƒãƒªã‚·ãƒ¼ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®å†æ§‹ç¯‰"""

    def __init__(self, input_dim=64, hidden_dim=128):
        super(PPOPolicyNetwork, self).__init__()

        self.feature_extractor = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.LayerNorm(hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(hidden_dim, hidden_dim),
            nn.LayerNorm(hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.1),
        )

        self.actor = nn.Sequential(
            nn.Linear(hidden_dim, 64),
            nn.LayerNorm(64),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(64, 16),
            nn.Softmax(dim=-1),
        )

        self.critic = nn.Sequential(
            nn.Linear(hidden_dim, 64),
            nn.LayerNorm(64),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(64, 1),
        )

    def forward(self, x):
        features = self.feature_extractor(x)
        action_probs = self.actor(features)
        value = self.critic(features)
        return action_probs, value

    def get_action_score(self, x):
        with torch.no_grad():
            action_probs, value = self.forward(x)
            score = torch.max(action_probs).item()
            return score


def is_bot(username: str) -> bool:
    """ãƒ¦ãƒ¼ã‚¶ãƒ¼åãŒBotã‹ã©ã†ã‹åˆ¤å®š"""
    bot_indicators = [
        "[bot]",
        "bot",
        "dependabot",
        "renovate",
        "greenkeeper",
        "codecov",
        "travis",
        "circleci",
        "github-actions",
        "automated",
    ]
    username_lower = username.lower()
    return any(indicator in username_lower for indicator in bot_indicators)


class LightweightWeightAnalyzer:
    """è»½é‡ç‰ˆé‡ã¿åˆ†æã‚·ã‚¹ãƒ†ãƒ """

    def __init__(self, model_dir: str, test_data_path: str):
        print("ğŸ”§ è»½é‡ç‰ˆã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–ä¸­...")
        self.model_dir = model_dir
        self.test_data_path = test_data_path
        self.models = {}
        self.author_contributions = {}

        self._load_data()
        self._load_models()
        print(f"   åˆæœŸåŒ–å®Œäº†: {len(self.models)}ãƒ¢ãƒ‡ãƒ«, {len(self.tasks)}ã‚¿ã‚¹ã‚¯")

    def _load_data(self):
        """ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿"""
        with open(self.test_data_path, "r", encoding="utf-8") as f:
            test_data = json.load(f)

        self.tasks = []
        self.ground_truth = []

        for task in test_data:
            author = task.get("author", {})
            if author and isinstance(author, dict):
                author_login = author.get("login", "")
                if author_login and not is_bot(author_login):
                    self.tasks.append(task)
                    self.ground_truth.append(author_login)

        self.author_contributions = Counter(self.ground_truth)

    def _load_models(self):
        """ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿"""
        model_files = [f for f in os.listdir(self.model_dir) if f.endswith(".pth")]
        all_trained_agents = [
            f.replace("agent_", "").replace(".pth", "") for f in model_files
        ]

        human_agents = [agent for agent in all_trained_agents if not is_bot(agent)]
        actual_set = set(self.ground_truth)
        overlapping_agents = actual_set.intersection(set(human_agents))

        for agent_name in list(overlapping_agents)[:20]:  # æœ€åˆã®20äººã ã‘
            model_path = os.path.join(self.model_dir, f"agent_{agent_name}.pth")
            try:
                model_data = torch.load(
                    model_path, map_location="cpu", weights_only=False
                )
                policy_network = PPOPolicyNetwork()
                policy_network.load_state_dict(model_data["policy_state_dict"])
                policy_network.eval()
                self.models[agent_name] = policy_network
            except Exception:
                continue

    def _extract_task_features(self, task: Dict) -> torch.Tensor:
        """ç°¡æ˜“ã‚¿ã‚¹ã‚¯ç‰¹å¾´é‡æŠ½å‡º"""
        features = []

        title = task.get("title", "") or ""
        body = task.get("body", "") or ""

        # åŸºæœ¬ç‰¹å¾´é‡
        features.extend(
            [
                len(title),
                len(body),
                len(title.split()),
                len(body.split()),
                1 if "bug" in title.lower() else 0,
                1 if "feature" in title.lower() else 0,
                1 if "doc" in title.lower() else 0,
            ]
        )

        # ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°
        while len(features) < 64:
            features.append(0.0)
        features = features[:64]

        return torch.tensor(features, dtype=torch.float32)

    def simple_weight_test(self, sample_size: int = 50):
        """ã‚·ãƒ³ãƒ—ãƒ«ãªé‡ã¿åˆ†æãƒ†ã‚¹ãƒˆ"""
        print(f"\nğŸ” ã‚·ãƒ³ãƒ—ãƒ«é‡ã¿åˆ†æé–‹å§‹ (ã‚µãƒ³ãƒ—ãƒ«æ•°: {sample_size})")

        available_agents = set(self.models.keys())

        # è©•ä¾¡ç”¨ã‚¿ã‚¹ã‚¯é¸æŠ
        eval_tasks = []
        eval_ground_truth = []

        for task, author in zip(
            self.tasks[: sample_size * 3], self.ground_truth[: sample_size * 3]
        ):
            if author in available_agents and len(eval_tasks) < sample_size:
                eval_tasks.append(task)
                eval_ground_truth.append(author)

        print(f"   å®Ÿéš›ã®è©•ä¾¡ã‚¿ã‚¹ã‚¯æ•°: {len(eval_tasks)}")

        # é‡ã¿ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆã‚·ãƒ³ãƒ—ãƒ«ç‰ˆï¼‰
        weight_patterns = {
            "balanced": {"ppo": 0.5, "contribution": 0.5},
            "ppo_heavy": {"ppo": 0.8, "contribution": 0.2},
            "contribution_heavy": {"ppo": 0.2, "contribution": 0.8},
            "ppo_only": {"ppo": 1.0, "contribution": 0.0},
            "contribution_only": {"ppo": 0.0, "contribution": 1.0},
        }

        results = {}

        for pattern_name, weights in weight_patterns.items():
            print(f"\n   {pattern_name}ãƒ‘ã‚¿ãƒ¼ãƒ³è©•ä¾¡ä¸­...")
            correct_count = 0

            # ä¿®æ­£: zip()ã‚’ä½¿ã£ã¦æ­£ã—ããƒšã‚¢ã«ã™ã‚‹
            for task, actual_author in zip(eval_tasks, eval_ground_truth):
                try:
                    task_features = self._extract_task_features(task)
                    agent_scores = {}

                    for agent_name, model in self.models.items():
                        # PPOã‚¹ã‚³ã‚¢
                        ppo_score = model.get_action_score(task_features)

                        # è²¢çŒ®é‡ã‚¹ã‚³ã‚¢
                        contribution = self.author_contributions.get(agent_name, 0)
                        contribution_score = min(contribution / 100.0, 1.0)

                        # é‡ã¿ä»˜ãæœ€çµ‚ã‚¹ã‚³ã‚¢
                        final_score = (
                            weights["ppo"] * ppo_score
                            + weights["contribution"] * contribution_score
                        )

                        agent_scores[agent_name] = final_score

                    # Top-1æ¨è–¦
                    if agent_scores:
                        top_agent = max(agent_scores.items(), key=lambda x: x[1])[0]
                        if top_agent == actual_author:
                            correct_count += 1

                except Exception as e:
                    print(f"     ã‚¨ãƒ©ãƒ¼: {e}")
                    continue

            accuracy = correct_count / len(eval_tasks) if eval_tasks else 0
            results[pattern_name] = {
                "accuracy": accuracy,
                "correct_count": correct_count,
                "total_count": len(eval_tasks),
                "weights": weights,
            }

            print(f"     ç²¾åº¦: {accuracy:.3f} ({accuracy*100:.1f}%)")
            print(f"     æ­£è§£æ•°: {correct_count}/{len(eval_tasks)}")

        # çµæœè¡¨ç¤º
        print(f"\n## ğŸ“Š çµæœã‚µãƒãƒªãƒ¼")
        print("=" * 40)

        sorted_results = sorted(
            results.items(), key=lambda x: x[1]["accuracy"], reverse=True
        )

        print("| ãƒ‘ã‚¿ãƒ¼ãƒ³å | ç²¾åº¦ | PPOé‡ã¿ | è²¢çŒ®é‡é‡ã¿ |")
        print("|------------|------|---------|------------|")

        for pattern_name, result in sorted_results:
            accuracy = result["accuracy"]
            ppo_weight = result["weights"]["ppo"]
            contrib_weight = result["weights"]["contribution"]
            print(
                f"| {pattern_name} | {accuracy:.3f} ({accuracy*100:.1f}%) | {ppo_weight:.1f} | {contrib_weight:.1f} |"
            )

        # æœ€é«˜ç²¾åº¦ãƒ‘ã‚¿ãƒ¼ãƒ³
        best_pattern_name, best_result = sorted_results[0]
        print(f"\nğŸ† æœ€é«˜ç²¾åº¦: {best_pattern_name}")
        print(f"   ç²¾åº¦: {best_result['accuracy']*100:.1f}%")
        print(f"   PPOé‡ã¿: {best_result['weights']['ppo']}")
        print(f"   è²¢çŒ®é‡é‡ã¿: {best_result['weights']['contribution']}")

        return results


def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ"""
    print("ğŸš€ è»½é‡ç‰ˆé‡ã¿åˆ†æã‚·ã‚¹ãƒ†ãƒ ")
    print("=" * 40)

    try:
        # ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–
        analyzer = LightweightWeightAnalyzer(
            model_dir="models/improved_rl/final_models",
            test_data_path="data/backlog_test_2023.json",
        )

        # é‡ã¿åˆ†æå®Ÿè¡Œ
        results = analyzer.simple_weight_test(sample_size=30)

        print(f"\nâœ… åˆ†æå®Œäº†ï¼")

    except Exception as e:
        print(f"âŒ ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿ: {e}")
        import traceback

        traceback.print_exc()


if __name__ == "__main__":
    main()
