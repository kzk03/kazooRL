#!/usr/bin/env python3
"""
ãƒãƒ«ãƒæŠ½å‡ºæ–¹æ³•å¯¾å¿œã®å¼·åŒ–å­¦ç¿’æ¨è–¦ã‚·ã‚¹ãƒ†ãƒ 

- è¤‡æ•°ã®é–‹ç™ºè€…æŠ½å‡ºæ–¹æ³•ï¼ˆassignees, creators, allï¼‰ã‚’RLçŠ¶æ…‹ã«çµ„ã¿è¾¼ã¿
- å‹•çš„å€™è£œãƒ—ãƒ¼ãƒ«é¸æŠã‚’ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ç©ºé–“ã«å«ã‚ã‚‹
- æ™‚ç³»åˆ—ãƒ™ãƒ¼ã‚¹ã®å ±é…¬è¨­è¨ˆ
- ãƒ¡ã‚¿å­¦ç¿’ã«ã‚ˆã‚‹é©å¿œçš„æŠ½å‡ºæ–¹æ³•é¸æŠ
"""

import argparse
import json
import pickle
from collections import Counter, defaultdict
from datetime import datetime, timedelta
from pathlib import Path

import gymnasium as gym
import numpy as np
import pandas as pd
import yaml
from gymnasium import spaces
from simple_similarity_recommender import SimpleSimilarityRecommender
from stable_baselines3 import PPO
from stable_baselines3.common.callbacks import EvalCallback
from stable_baselines3.common.vec_env import DummyVecEnv


class MultiExtractionRLEnvironment(gym.Env):
    """
    ãƒãƒ«ãƒæŠ½å‡ºæ–¹æ³•å¯¾å¿œã®å¼·åŒ–å­¦ç¿’ç’°å¢ƒ
    
    çŠ¶æ…‹ç©ºé–“: [ã‚¿ã‚¹ã‚¯ç‰¹å¾´, æ™‚ç³»åˆ—æƒ…å ±, å€™è£œãƒ—ãƒ¼ãƒ«çµ±è¨ˆ, æŠ½å‡ºæ–¹æ³•å±¥æ­´]
    ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ç©ºé–“: [æŠ½å‡ºæ–¹æ³•é¸æŠ, é–‹ç™ºè€…é¸æŠ]
    å ±é…¬: æˆåŠŸå ±é…¬ + æŠ½å‡ºæ–¹æ³•é©åˆ‡æ€§ãƒœãƒ¼ãƒŠã‚¹ + æ™‚ç³»åˆ—ä¸€è²«æ€§ãƒœãƒ¼ãƒŠã‚¹
    """
    
    def __init__(self, similarity_recommender, training_data, config):
        super().__init__()
        
        self.recommender = similarity_recommender
        self.training_data = training_data
        self.config = config
        
        # æŠ½å‡ºæ–¹æ³•ã®å®šç¾©
        self.extraction_methods = ['assignees', 'creators', 'all']
        self.method_to_id = {method: i for i, method in enumerate(self.extraction_methods)}
        
        # çŠ¶æ…‹ç©ºé–“ã®å®šç¾©
        # [ã‚¿ã‚¹ã‚¯ç‰¹å¾´(10æ¬¡å…ƒ) + æ™‚ç³»åˆ—æƒ…å ±(5æ¬¡å…ƒ) + å€™è£œãƒ—ãƒ¼ãƒ«çµ±è¨ˆ(6æ¬¡å…ƒ) + æŠ½å‡ºå±¥æ­´(3æ¬¡å…ƒ)]
        self.observation_space = spaces.Box(
            low=-np.inf, high=np.inf, shape=(24,), dtype=np.float32
        )
        
        # ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ç©ºé–“: [æŠ½å‡ºæ–¹æ³•(3), é–‹ç™ºè€…ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹(æœ€å¤§100)]
        # ãƒãƒ«ãƒãƒ‡ã‚£ã‚¹ã‚¯ãƒªãƒ¼ãƒˆ: æœ€åˆã®æ¬¡å…ƒã§æŠ½å‡ºæ–¹æ³•ã€2ç•ªç›®ã§é–‹ç™ºè€…ã‚’é¸æŠ
        self.action_space = spaces.MultiDiscrete([3, 100])
        
        # ç’°å¢ƒçŠ¶æ…‹
        self.current_task_idx = 0
        self.extraction_history = np.zeros(3)  # å„æ–¹æ³•ã®ä½¿ç”¨å›æ•°
        self.recent_rewards = []
        self.time_step = 0
        
        print(f"ğŸ¤– ãƒãƒ«ãƒæŠ½å‡ºRLç’°å¢ƒåˆæœŸåŒ–:")
        print(f"   çŠ¶æ…‹æ¬¡å…ƒ: {self.observation_space.shape[0]}")
        print(f"   ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ç©ºé–“: æŠ½å‡ºæ–¹æ³•Ã—{len(self.extraction_methods)}, é–‹ç™ºè€…Ã—100")
        print(f"   å­¦ç¿’ã‚¿ã‚¹ã‚¯æ•°: {len(training_data)}")
    
    def reset(self, seed=None, options=None):
        """ç’°å¢ƒã‚’ãƒªã‚»ãƒƒãƒˆ"""
        super().reset(seed=seed)
        
        self.current_task_idx = 0
        self.extraction_history = np.zeros(3)
        self.recent_rewards = []
        self.time_step = 0
        
        obs = self._get_observation()
        info = {}
        
        return obs, info
    
    def step(self, action):
        """1ã‚¹ãƒ†ãƒƒãƒ—å®Ÿè¡Œ"""
        extraction_method_id, developer_idx = action
        extraction_method = self.extraction_methods[extraction_method_id]
        
        # ç¾åœ¨ã®ã‚¿ã‚¹ã‚¯ã‚’å–å¾—
        if self.current_task_idx >= len(self.training_data):
            # ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰çµ‚äº†
            obs = self._get_observation()
            return obs, 0.0, True, True, {}
        
        current_task = self.training_data[self.current_task_idx]
        
        # å ±é…¬è¨ˆç®—ã¨ã‚¿ã‚¹ã‚¯å®Ÿè¡Œ
        reward, info = self._calculate_reward(current_task, extraction_method, developer_idx)
        
        # å±¥æ­´æ›´æ–°
        self.extraction_history[extraction_method_id] += 1
        self.recent_rewards.append(reward)
        if len(self.recent_rewards) > 10:
            self.recent_rewards.pop(0)
        
        # æ¬¡ã®ã‚¿ã‚¹ã‚¯ã¸
        self.current_task_idx += 1
        self.time_step += 1
        
        # ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰çµ‚äº†åˆ¤å®š
        terminated = self.current_task_idx >= len(self.training_data)
        truncated = False  # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆãªã—
        
        obs = self._get_observation()
        return obs, reward, terminated, truncated, info
    
    def _get_observation(self):
        """ç¾åœ¨ã®è¦³æ¸¬çŠ¶æ…‹ã‚’å–å¾—"""
        if self.current_task_idx >= len(self.training_data):
            # çµ‚äº†çŠ¶æ…‹
            return np.zeros(24, dtype=np.float32)
        
        current_task = self.training_data[self.current_task_idx]
        
        # 1. ã‚¿ã‚¹ã‚¯ç‰¹å¾´é‡ (10æ¬¡å…ƒ)
        task_features = self._extract_task_features(current_task)
        
        # 2. æ™‚ç³»åˆ—æƒ…å ± (5æ¬¡å…ƒ)
        temporal_features = self._extract_temporal_features(current_task)
        
        # 3. å€™è£œãƒ—ãƒ¼ãƒ«çµ±è¨ˆ (6æ¬¡å…ƒ)
        pool_stats = self._extract_pool_statistics(current_task)
        
        # 4. æŠ½å‡ºå±¥æ­´ (3æ¬¡å…ƒ)
        history_normalized = self.extraction_history / max(1, self.time_step)
        
        # å…¨ç‰¹å¾´é‡ã‚’çµåˆ
        obs = np.concatenate([
            task_features,
            temporal_features, 
            pool_stats,
            history_normalized
        ]).astype(np.float32)
        
        return obs
    
    def _extract_task_features(self, task_data):
        """ã‚¿ã‚¹ã‚¯ã‹ã‚‰ç‰¹å¾´é‡ã‚’æŠ½å‡º"""
        features = self.recommender.extract_basic_features(task_data)
        
        # 10æ¬¡å…ƒã®ç‰¹å¾´ãƒ™ã‚¯ãƒˆãƒ«
        return np.array([
            features.get('title_length', 0) / 100.0,  # æ­£è¦åŒ–
            features.get('body_length', 0) / 1000.0,
            features.get('comments_count', 0) / 10.0,
            features.get('is_bug', 0),
            features.get('is_enhancement', 0),
            features.get('is_documentation', 0),
            features.get('is_question', 0),
            features.get('is_help_wanted', 0),
            features.get('label_count', 0) / 5.0,
            features.get('is_open', 0)
        ], dtype=np.float32)
    
    def _extract_temporal_features(self, task_data):
        """æ™‚ç³»åˆ—ç‰¹å¾´é‡ã‚’æŠ½å‡º"""
        created_at = task_data.get('created_at', '')
        
        try:
            if created_at:
                dt = datetime.fromisoformat(created_at.replace('Z', '+00:00'))
                
                # æ™‚ç³»åˆ—ç‰¹å¾´
                return np.array([
                    dt.month / 12.0,  # æœˆ
                    dt.weekday() / 6.0,  # æ›œæ—¥
                    dt.hour / 23.0,  # æ™‚é–“
                    len(self.recent_rewards) / 10.0,  # æœ€è¿‘ã®ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰æ•°
                    np.mean(self.recent_rewards) if self.recent_rewards else 0.0  # æœ€è¿‘ã®å¹³å‡å ±é…¬
                ], dtype=np.float32)
        except:
            pass
        
        return np.zeros(5, dtype=np.float32)
    
    def _extract_pool_statistics(self, task_data):
        """å„æŠ½å‡ºæ–¹æ³•ã®å€™è£œãƒ—ãƒ¼ãƒ«çµ±è¨ˆ"""
        stats = []
        
        for method in self.extraction_methods:
            # å„æ–¹æ³•ã§ã®é–‹ç™ºè€…æ•°ã‚’å–å¾—
            developers = self.recommender.extract_developers_from_task(task_data, method=method)
            
            # çµ±è¨ˆè¨ˆç®—
            dev_count = len(developers)
            avg_priority = np.mean([dev['priority'] for dev in developers]) if developers else 0
            
            stats.extend([dev_count / 100.0, avg_priority / 3.0])  # æ­£è¦åŒ–
        
        return np.array(stats, dtype=np.float32)
    
    def _calculate_reward(self, task_data, extraction_method, developer_idx):
        """å ±é…¬è¨ˆç®—"""
        # æŒ‡å®šã•ã‚ŒãŸæŠ½å‡ºæ–¹æ³•ã§é–‹ç™ºè€…ã‚’æŠ½å‡º
        developers = self.recommender.extract_developers_from_task(task_data, method=extraction_method)
        
        if not developers:
            return -0.1, {'reason': 'no_developers', 'method': extraction_method}
        
        # é–‹ç™ºè€…ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®èª¿æ•´
        actual_dev_idx = min(developer_idx, len(developers) - 1)
        selected_dev = developers[actual_dev_idx]
        
        # å®Ÿéš›ã®æ­£è§£é–‹ç™ºè€…ã‚’å–å¾—
        actual_developers = self.recommender.extract_developers_from_task(task_data, method='all')
        actual_dev = actual_developers[0]['login'] if actual_developers else None
        
        # åŸºæœ¬å ±é…¬
        base_reward = 0.0
        success = False
        
        if actual_dev and selected_dev['login'] == actual_dev:
            # æˆåŠŸæ™‚ã®å ±é…¬
            method_rewards = {
                'assignees': 1.0,    # æœ€é«˜å“è³ª
                'creators': 0.7,     # é«˜ãƒ¢ãƒãƒ™ãƒ¼ã‚·ãƒ§ãƒ³
                'all': 0.8           # ãƒãƒ©ãƒ³ã‚¹
            }
            base_reward = method_rewards.get(extraction_method, 0.5)
            success = True
        else:
            # å¤±æ•—æ™‚ã®è»½å¾®ãªãƒšãƒŠãƒ«ãƒ†ã‚£
            base_reward = -0.05
        
        # ãƒœãƒ¼ãƒŠã‚¹å ±é…¬
        bonus_reward = 0.0
        
        # 1. æŠ½å‡ºæ–¹æ³•é©åˆ‡æ€§ãƒœãƒ¼ãƒŠã‚¹
        if extraction_method == 'assignees' and selected_dev['priority'] == 1:
            bonus_reward += 0.1  # é«˜å„ªå…ˆåº¦é–‹ç™ºè€…ã‚’é¸æŠ
        elif extraction_method == 'creators' and selected_dev['source'] == 'user_creator':
            bonus_reward += 0.1  # ä½œæˆè€…ã‚’é©åˆ‡ã«é¸æŠ
        
        # 2. æ™‚ç³»åˆ—ä¸€è²«æ€§ãƒœãƒ¼ãƒŠã‚¹
        if len(self.recent_rewards) >= 3:
            recent_success_rate = np.mean([r > 0 for r in self.recent_rewards[-3:]])
            if recent_success_rate > 0.5:
                bonus_reward += 0.05  # æœ€è¿‘ã®æˆåŠŸç‡ãŒé«˜ã„
        
        # 3. æ¢ç´¢ãƒœãƒ¼ãƒŠã‚¹
        method_usage = self.extraction_history / max(1, self.time_step)
        if np.std(method_usage) > 0.1:  # å‡ç­‰ã«æ¢ç´¢ã—ã¦ã„ã‚‹
            bonus_reward += 0.02
        
        total_reward = base_reward + bonus_reward
        
        info = {
            'success': success,
            'method': extraction_method,
            'selected_dev': selected_dev['login'],
            'actual_dev': actual_dev,
            'base_reward': base_reward,
            'bonus_reward': bonus_reward,
            'priority': selected_dev['priority'],
            'source': selected_dev['source']
        }
        
        return total_reward, info


class MultiExtractionRLRecommender:
    """ãƒãƒ«ãƒæŠ½å‡ºæ–¹æ³•å¯¾å¿œã®å¼·åŒ–å­¦ç¿’æ¨è–¦ã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self, config_path):
        self.config_path = config_path
        
        # è¨­å®šèª­ã¿è¾¼ã¿
        with open(config_path, 'r', encoding='utf-8') as f:
            self.config = yaml.safe_load(f)
        
        # ãƒ™ãƒ¼ã‚¹ã¨ãªã‚‹é¡ä¼¼åº¦æ¨è–¦ã‚·ã‚¹ãƒ†ãƒ 
        self.similarity_recommender = SimpleSimilarityRecommender(config_path)
        
        print("ğŸš€ ãƒãƒ«ãƒæŠ½å‡ºRLæ¨è–¦ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å®Œäº†")
    
    def train_base_models(self, data_path):
        """ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã‚’è¨“ç·´"""
        print("ğŸ“š ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«è¨“ç·´ä¸­...")
        
        # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
        training_data, test_data = self.similarity_recommender.load_data(data_path)
        
        # 'all'æ–¹æ³•ã§å­¦ç¿’ãƒšã‚¢æŠ½å‡ºï¼ˆæœ€ã‚‚åŒ…æ‹¬çš„ï¼‰
        training_pairs, developer_stats = self.similarity_recommender.extract_training_pairs(
            training_data, extraction_method='all'
        )
        
        if not training_pairs:
            raise ValueError("å­¦ç¿’ãƒšã‚¢ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
        
        # æ™‚ç³»åˆ—ã‚¢ã‚¯ãƒ†ã‚£ãƒ“ãƒ†ã‚£æ§‹ç¯‰
        self.similarity_recommender.build_developer_activity_timeline(
            training_data, extraction_method='all'
        )
        
        # é–‹ç™ºè€…ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«æ§‹ç¯‰
        learned_profiles = self.similarity_recommender.build_developer_profiles(training_pairs)
        
        # ãƒ¢ãƒ‡ãƒ«è¨“ç·´
        self.similarity_recommender.train_text_similarity_model(learned_profiles)
        self.similarity_recommender.train_feature_model(training_pairs)
        
        print("âœ… ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«è¨“ç·´å®Œäº†")
        return training_pairs, test_data
    
    def train_rl_agent(self, training_pairs, total_timesteps=50000):
        """å¼·åŒ–å­¦ç¿’ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’è¨“ç·´"""
        print("ğŸ¯ ãƒãƒ«ãƒæŠ½å‡ºRLè¨“ç·´é–‹å§‹...")
        
        # ç’°å¢ƒä½œæˆ
        def make_env():
            return MultiExtractionRLEnvironment(
                self.similarity_recommender, 
                [pair['task_data'] for pair in training_pairs],
                self.config
            )
        
        env = DummyVecEnv([make_env])
        
        # PPOã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆä½œæˆ
        self.rl_agent = PPO(
            "MlpPolicy",
            env,
            verbose=1,
            learning_rate=3e-4,
            n_steps=2048,
            batch_size=64,
            n_epochs=10,
            gamma=0.99,
            gae_lambda=0.95,
            clip_range=0.2,
            ent_coef=0.01,
            device="auto",
            seed=42,
            policy_kwargs={
                'net_arch': [256, 256, 128]  # ã‚ˆã‚Šæ·±ã„ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯
            }
        )
        
        # è¨“ç·´å®Ÿè¡Œ
        print(f"   è¨“ç·´ã‚¹ãƒ†ãƒƒãƒ—æ•°: {total_timesteps:,}")
        self.rl_agent.learn(total_timesteps=total_timesteps)
        
        print("âœ… ãƒãƒ«ãƒæŠ½å‡ºRLè¨“ç·´å®Œäº†")
    
    def predict_with_rl(self, test_data):
        """å¼·åŒ–å­¦ç¿’ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã§äºˆæ¸¬"""
        print("ğŸ”® ãƒãƒ«ãƒæŠ½å‡ºRLäºˆæ¸¬å®Ÿè¡Œ...")
        
        predictions = {}
        prediction_scores = {}
        method_usage_stats = Counter()
        
        for task_data in test_data:
            task_id = task_data.get('id') or task_data.get('number')
            if not task_id:
                continue
            
            try:
                # ç’°å¢ƒä½œæˆï¼ˆå˜ä¸€ã‚¿ã‚¹ã‚¯ç”¨ï¼‰
                temp_env = MultiExtractionRLEnvironment(
                    self.similarity_recommender,
                    [task_data],
                    self.config
                )
                
                # è¦³æ¸¬å–å¾—
                obs, _ = temp_env.reset()
                
                # RLã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã§äºˆæ¸¬
                action, _ = self.rl_agent.predict(obs, deterministic=True)
                extraction_method_id, developer_idx = action
                
                # æŠ½å‡ºæ–¹æ³•ã¨ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’è§£é‡ˆ
                extraction_method = temp_env.extraction_methods[extraction_method_id]
                method_usage_stats[extraction_method] += 1
                
                # æŒ‡å®šã•ã‚ŒãŸæ–¹æ³•ã§é–‹ç™ºè€…ã‚’æŠ½å‡º
                developers = self.similarity_recommender.extract_developers_from_task(
                    task_data, method=extraction_method
                )
                
                if developers:
                    actual_dev_idx = min(developer_idx, len(developers) - 1)
                    selected_dev = developers[actual_dev_idx]
                    
                    predictions[task_id] = selected_dev['login']
                    prediction_scores[task_id] = {
                        'predicted_dev': selected_dev['login'],
                        'extraction_method': extraction_method,
                        'developer_idx': actual_dev_idx,
                        'priority': selected_dev['priority'],
                        'source': selected_dev['source'],
                        'pool_size': len(developers)
                    }
                
            except Exception as e:
                print(f"âš ï¸ ã‚¿ã‚¹ã‚¯ {task_id} ã®äºˆæ¸¬ã‚¨ãƒ©ãƒ¼: {e}")
                continue
        
        print(f"   äºˆæ¸¬å®Œäº†: {len(predictions)} ã‚¿ã‚¹ã‚¯")
        print("   æŠ½å‡ºæ–¹æ³•ä½¿ç”¨çµ±è¨ˆ:")
        for method, count in method_usage_stats.items():
            print(f"     {method}: {count} ã‚¿ã‚¹ã‚¯ ({count/len(predictions)*100:.1f}%)")
        
        return predictions, prediction_scores
    
    def evaluate_predictions(self, predictions, test_data):
        """äºˆæ¸¬çµæœã‚’è©•ä¾¡"""
        print("ğŸ“Š ãƒãƒ«ãƒæŠ½å‡ºRLè©•ä¾¡ä¸­...")
        
        # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®æ­£è§£ã‚’æŠ½å‡º
        test_assignments = {}
        for task_data in test_data:
            task_id = task_data.get('id') or task_data.get('number')
            if not task_id:
                continue
                
            # 'all'æ–¹æ³•ã§æ­£è§£ã‚’å–å¾—
            developers = self.similarity_recommender.extract_developers_from_task(
                task_data, method='all'
            )
            if developers:
                test_assignments[task_id] = developers[0]['login']
        
        # è©•ä¾¡è¨ˆç®—
        common_tasks = set(predictions.keys()) & set(test_assignments.keys())
        correct_predictions = 0
        
        for task_id in common_tasks:
            if predictions[task_id] == test_assignments[task_id]:
                correct_predictions += 1
        
        accuracy = correct_predictions / len(common_tasks) if common_tasks else 0.0
        
        metrics = {
            'accuracy': accuracy,
            'correct_predictions': correct_predictions,
            'total_predictions': len(common_tasks)
        }
        
        print(f"   ãƒãƒ«ãƒæŠ½å‡ºRLç²¾åº¦: {accuracy:.3f} ({correct_predictions}/{len(common_tasks)})")
        
        return metrics
    
    def run_full_pipeline(self, data_path, total_timesteps=50000, output_dir="outputs"):
        """å®Œå…¨ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè¡Œ"""
        print("ğŸš€ ãƒãƒ«ãƒæŠ½å‡ºRLæ¨è–¦ã‚·ã‚¹ãƒ†ãƒ å®Ÿè¡Œé–‹å§‹")
        print("=" * 70)
        
        # 1. ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«è¨“ç·´
        training_pairs, test_data = self.train_base_models(data_path)
        
        # 2. å¼·åŒ–å­¦ç¿’ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆè¨“ç·´
        self.train_rl_agent(training_pairs, total_timesteps)
        
        # 3. äºˆæ¸¬å®Ÿè¡Œ
        predictions, prediction_scores = self.predict_with_rl(test_data)
        
        # 4. è©•ä¾¡
        metrics = self.evaluate_predictions(predictions, test_data)
        
        # 5. çµæœä¿å­˜
        self.save_results(metrics, predictions, prediction_scores, output_dir)
        
        print("âœ… ãƒãƒ«ãƒæŠ½å‡ºRLæ¨è–¦ã‚·ã‚¹ãƒ†ãƒ å®Œäº†")
        return metrics
    
    def save_results(self, metrics, predictions, prediction_scores, output_dir):
        """çµæœä¿å­˜"""
        output_dir = Path(output_dir)
        output_dir.mkdir(exist_ok=True)
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # ãƒ¡ãƒˆãƒªã‚¯ã‚¹ä¿å­˜
        metrics_path = output_dir / f"multi_extraction_rl_metrics_{timestamp}.json"
        with open(metrics_path, 'w', encoding='utf-8') as f:
            json.dump(metrics, f, indent=2, ensure_ascii=False)
        
        # äºˆæ¸¬çµæœä¿å­˜
        results = []
        for task_id, predicted_dev in predictions.items():
            result = {
                'task_id': task_id,
                'predicted_developer': predicted_dev
            }
            
            if task_id in prediction_scores:
                result.update(prediction_scores[task_id])
            
            results.append(result)
        
        results_df = pd.DataFrame(results)
        results_path = output_dir / f"multi_extraction_rl_results_{timestamp}.csv"
        results_df.to_csv(results_path, index=False, encoding='utf-8')
        
        # ãƒ¢ãƒ‡ãƒ«ä¿å­˜
        model_path = output_dir / f"multi_extraction_rl_agent_{timestamp}.zip"
        self.rl_agent.save(model_path)
        
        print(f"âœ… ãƒ¡ãƒˆãƒªã‚¯ã‚¹ä¿å­˜: {metrics_path}")
        print(f"âœ… çµæœä¿å­˜: {results_path}")
        print(f"âœ… ãƒ¢ãƒ‡ãƒ«ä¿å­˜: {model_path}")


def main():
    parser = argparse.ArgumentParser(description='ãƒãƒ«ãƒæŠ½å‡ºæ–¹æ³•å¯¾å¿œã®å¼·åŒ–å­¦ç¿’æ¨è–¦ã‚·ã‚¹ãƒ†ãƒ ')
    parser.add_argument('--config', default='configs/unified_rl.yaml',
                       help='è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹')
    parser.add_argument('--data', default='data/backlog.json',
                       help='çµ±åˆãƒ‡ãƒ¼ã‚¿ãƒ‘ã‚¹')
    parser.add_argument('--timesteps', type=int, default=50000,
                       help='å¼·åŒ–å­¦ç¿’è¨“ç·´ã‚¹ãƒ†ãƒƒãƒ—æ•°')
    parser.add_argument('--output', default='outputs',
                       help='å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª')
    
    args = parser.parse_args()
    
    # ãƒãƒ«ãƒæŠ½å‡ºRLæ¨è–¦ã‚·ã‚¹ãƒ†ãƒ å®Ÿè¡Œ
    recommender = MultiExtractionRLRecommender(args.config)
    metrics = recommender.run_full_pipeline(
        args.data,
        total_timesteps=args.timesteps,
        output_dir=args.output
    )
    
    print("\nğŸ¯ æœ€çµ‚çµæœ:")
    for key, value in metrics.items():
        if isinstance(value, (int, float)):
            print(f"   {key}: {value:.3f}")
    
    return 0


if __name__ == "__main__":
    exit(main())
