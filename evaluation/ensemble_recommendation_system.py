#!/usr/bin/env python3
"""
ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«æ¨è–¦ã‚·ã‚¹ãƒ†ãƒ 
è¤‡æ•°ã®æ‰‹æ³•ã‚’çµ„ã¿åˆã‚ã›ã¦Top-1ç²¾åº¦ã‚’åŠ‡çš„ã«æ”¹å–„
"""

import json
import os
from collections import Counter, defaultdict
from datetime import datetime
from typing import Dict, List, Optional, Tuple

import numpy as np
import torch
import torch.nn as nn
import yaml
from tqdm import tqdm


class PPOPolicyNetwork(nn.Module):
    """PPOãƒãƒªã‚·ãƒ¼ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®å†æ§‹ç¯‰"""

    def __init__(self, input_dim=64, hidden_dim=128):
        super(PPOPolicyNetwork, self).__init__()

        self.feature_extractor = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.LayerNorm(hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(hidden_dim, hidden_dim),
            nn.LayerNorm(hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.1),
        )

        self.actor = nn.Sequential(
            nn.Linear(hidden_dim, 64),
            nn.LayerNorm(64),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(64, 16),
            nn.Softmax(dim=-1),
        )

        self.critic = nn.Sequential(
            nn.Linear(hidden_dim, 64),
            nn.LayerNorm(64),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(64, 1),
        )

    def forward(self, x):
        features = self.feature_extractor(x)
        action_probs = self.actor(features)
        value = self.critic(features)
        return action_probs, value

    def get_action_score(self, x):
        with torch.no_grad():
            action_probs, value = self.forward(x)
            score = torch.max(action_probs).item()
            return score


def is_bot(username: str) -> bool:
    """ãƒ¦ãƒ¼ã‚¶ãƒ¼åãŒBotã‹ã©ã†ã‹åˆ¤å®š"""
    bot_indicators = [
        "[bot]",
        "bot",
        "dependabot",
        "renovate",
        "greenkeeper",
        "codecov",
        "travis",
        "circleci",
        "github-actions",
        "automated",
    ]
    username_lower = username.lower()
    return any(indicator in username_lower for indicator in bot_indicators)


class EnsembleRecommendationSystem:
    """ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«æ¨è–¦ã‚·ã‚¹ãƒ†ãƒ  - è¤‡æ•°æ‰‹æ³•ã®çµ±åˆ"""

    def __init__(self, model_dir: str, test_data_path: str):
        self.model_dir = model_dir
        self.test_data_path = test_data_path
        self.models = {}
        self.author_contributions = {}
        self.model_quality_scores = {}
        self.author_specializations = {}
        self.task_similarity_cache = {}

        # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ãƒ»åˆ†æ
        self._load_test_data()
        self._analyze_contributions()
        self._load_all_models()
        self._analyze_model_quality()
        self._analyze_author_specializations()

    def _load_test_data(self):
        """ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿"""
        print("ğŸ“‚ ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ä¸­...")

        with open(self.test_data_path, "r", encoding="utf-8") as f:
            test_data = json.load(f)

        self.tasks = []
        self.ground_truth = []

        for task in test_data:
            author = task.get("author", {})
            if author and isinstance(author, dict):
                author_login = author.get("login", "")
                if author_login and not is_bot(author_login):
                    self.tasks.append(task)
                    self.ground_truth.append(author_login)

        print(f"   èª­ã¿è¾¼ã¿å®Œäº†: {len(self.tasks):,}ã‚¿ã‚¹ã‚¯")

    def _analyze_contributions(self):
        """è²¢çŒ®é‡åˆ†æ"""
        print("ğŸ“Š è²¢çŒ®é‡åˆ†æä¸­...")

        self.author_contributions = Counter(self.ground_truth)

        print(f"   ãƒ¦ãƒ‹ãƒ¼ã‚¯é–‹ç™ºè€…æ•°: {len(self.author_contributions)}")
        print(f"   ä¸Šä½5äºº:")
        for author, count in self.author_contributions.most_common(5):
            print(f"     {author}: {count}ã‚¿ã‚¹ã‚¯")

    def _load_all_models(self):
        """å…¨ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿"""
        print("ğŸ¤– å…¨ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ä¸­...")

        model_files = [f for f in os.listdir(self.model_dir) if f.endswith(".pth")]
        all_trained_agents = [
            f.replace("agent_", "").replace(".pth", "") for f in model_files
        ]

        # Boté™¤å»
        human_agents = [agent for agent in all_trained_agents if not is_bot(agent)]

        # å®Ÿéš›ã®ä½œæˆè€…ã¨é‡è¤‡ã™ã‚‹ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®ã¿
        actual_set = set(self.ground_truth)
        overlapping_agents = actual_set.intersection(set(human_agents))

        # è²¢çŒ®é‡é †ã§ã‚½ãƒ¼ãƒˆ
        priority_agents = sorted(
            overlapping_agents,
            key=lambda x: self.author_contributions.get(x, 0),
            reverse=True,
        )

        print(f"   å¯¾è±¡ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆæ•°: {len(priority_agents)}")

        loaded_count = 0
        for agent_name in priority_agents:
            model_path = os.path.join(self.model_dir, f"agent_{agent_name}.pth")

            try:
                model_data = torch.load(
                    model_path, map_location="cpu", weights_only=False
                )
                policy_network = PPOPolicyNetwork()
                policy_network.load_state_dict(model_data["policy_state_dict"])
                policy_network.eval()

                self.models[agent_name] = policy_network
                loaded_count += 1

            except Exception:
                continue

        print(f"   èª­ã¿è¾¼ã¿çµæœ: {loaded_count}æˆåŠŸ")

    def _analyze_model_quality(self):
        """ãƒ¢ãƒ‡ãƒ«å“è³ªåˆ†æ"""
        print("ğŸ” ãƒ¢ãƒ‡ãƒ«å“è³ªåˆ†æä¸­...")

        sample_tasks = self.tasks[:20]

        for agent_name, model in self.models.items():
            scores = []

            for task in sample_tasks:
                try:
                    task_features = self._extract_task_features(task)
                    score = model.get_action_score(task_features)
                    scores.append(score)
                except:
                    scores.append(0.0)

            avg_score = np.mean(scores) if scores else 0.0
            score_std = np.std(scores) if scores else 0.0

            self.model_quality_scores[agent_name] = {
                "avg_score": avg_score,
                "std_score": score_std,
                "contribution": self.author_contributions.get(agent_name, 0),
            }

        print(f"   å“è³ªåˆ†æå®Œäº†: {len(self.model_quality_scores)}ãƒ¢ãƒ‡ãƒ«")

    def _analyze_author_specializations(self):
        """é–‹ç™ºè€…ã®å°‚é–€åˆ†é‡åˆ†æ"""
        print("ğŸ¯ å°‚é–€åˆ†é‡åˆ†æä¸­...")

        # å„é–‹ç™ºè€…ã®ã‚¿ã‚¹ã‚¯ã‚’åˆ†æ
        author_tasks = defaultdict(list)

        for task, author in zip(self.tasks, self.ground_truth):
            if author in self.models:
                author_tasks[author].append(task)

        # å°‚é–€åˆ†é‡ã®ç‰¹å®š
        for author, tasks in author_tasks.items():
            specialization = self._identify_specialization(tasks)
            self.author_specializations[author] = specialization

        print(f"   å°‚é–€åˆ†é‡åˆ†æå®Œäº†: {len(self.author_specializations)}é–‹ç™ºè€…")

    def _identify_specialization(self, tasks: List[Dict]) -> Dict[str, float]:
        """é–‹ç™ºè€…ã®å°‚é–€åˆ†é‡ã‚’ç‰¹å®š"""
        specialization_scores = {
            "bug_fix": 0.0,
            "feature": 0.0,
            "documentation": 0.0,
            "ui_ux": 0.0,
            "performance": 0.0,
            "security": 0.0,
            "api": 0.0,
            "testing": 0.0,
        }

        total_tasks = len(tasks)
        if total_tasks == 0:
            return specialization_scores

        for task in tasks:
            title = (task.get("title", "") or "").lower()
            body = (task.get("body", "") or "").lower()
            labels = task.get("labels", [])

            # ãƒ©ãƒ™ãƒ«ãƒ†ã‚­ã‚¹ãƒˆ
            label_text = " ".join(
                [
                    str(label) if not isinstance(label, dict) else label.get("name", "")
                    for label in labels
                ]
            ).lower()

            full_text = f"{title} {body} {label_text}"

            # å°‚é–€åˆ†é‡ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒãƒƒãƒãƒ³ã‚°
            if any(kw in full_text for kw in ["bug", "fix", "error", "issue"]):
                specialization_scores["bug_fix"] += 1

            if any(kw in full_text for kw in ["feature", "enhancement", "new"]):
                specialization_scores["feature"] += 1

            if any(kw in full_text for kw in ["doc", "readme", "guide"]):
                specialization_scores["documentation"] += 1

            if any(kw in full_text for kw in ["ui", "ux", "interface", "design"]):
                specialization_scores["ui_ux"] += 1

            if any(kw in full_text for kw in ["performance", "speed", "optimize"]):
                specialization_scores["performance"] += 1

            if any(kw in full_text for kw in ["security", "auth", "permission"]):
                specialization_scores["security"] += 1

            if any(kw in full_text for kw in ["api", "endpoint", "rest"]):
                specialization_scores["api"] += 1

            if any(kw in full_text for kw in ["test", "spec", "coverage"]):
                specialization_scores["testing"] += 1

        # æ­£è¦åŒ–
        for key in specialization_scores:
            specialization_scores[key] /= total_tasks

        return specialization_scores

    def _extract_task_features(self, task: Dict) -> torch.Tensor:
        """ã‚¿ã‚¹ã‚¯ç‰¹å¾´é‡æŠ½å‡ºï¼ˆå¼·åŒ–ç‰ˆï¼‰"""
        features = []

        title = task.get("title", "") or ""
        body = task.get("body", "") or ""
        labels = task.get("labels", [])

        # åŸºæœ¬ç‰¹å¾´é‡
        basic_features = [
            len(title),
            len(body),
            len(title.split()),
            len(body.split()),
            len(labels),
            title.count("?"),
            title.count("!"),
            body.count("\n"),
            len(set(title.lower().split())),
            1 if any(kw in title.lower() for kw in ["bug", "fix", "error"]) else 0,
        ]
        features.extend(basic_features)

        # æ—¥ä»˜ç‰¹å¾´é‡
        created_at = task.get("created_at", "")
        if created_at:
            try:
                date_parts = created_at.split("T")[0].split("-")
                year, month, day = (
                    int(date_parts[0]),
                    int(date_parts[1]),
                    int(date_parts[2]),
                )
                features.extend([year - 2020, month, day])
            except:
                features.extend([0, 0, 0])
        else:
            features.extend([0, 0, 0])

        # ãƒ©ãƒ™ãƒ«ç‰¹å¾´é‡
        label_text = " ".join(
            [
                str(label) if not isinstance(label, dict) else label.get("name", "")
                for label in labels
            ]
        ).lower()

        important_keywords = [
            "bug",
            "feature",
            "enhancement",
            "documentation",
            "help",
            "question",
            "performance",
            "security",
            "ui",
            "api",
        ]
        for keyword in important_keywords:
            features.append(1 if keyword in label_text else 0)

        # å°‚é–€åˆ†é‡ç‰¹å¾´é‡ï¼ˆæ–°è¦è¿½åŠ ï¼‰
        full_text = f"{title} {body} {label_text}".lower()
        specialization_features = [
            1 if any(kw in full_text for kw in ["bug", "fix", "error"]) else 0,
            1 if any(kw in full_text for kw in ["feature", "enhancement"]) else 0,
            1 if any(kw in full_text for kw in ["doc", "readme"]) else 0,
            1 if any(kw in full_text for kw in ["ui", "ux", "design"]) else 0,
            1 if any(kw in full_text for kw in ["performance", "optimize"]) else 0,
            1 if any(kw in full_text for kw in ["security", "auth"]) else 0,
            1 if any(kw in full_text for kw in ["api", "endpoint"]) else 0,
            1 if any(kw in full_text for kw in ["test", "spec"]) else 0,
        ]
        features.extend(specialization_features)

        # ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°
        while len(features) < 64:
            features.append(0.0)
        features = features[:64]

        # æ­£è¦åŒ–
        features = np.array(features, dtype=np.float32)
        features = (features - np.mean(features)) / (np.std(features) + 1e-8)

        return torch.tensor(features, dtype=torch.float32)

    def _calculate_task_specialization_match(self, task: Dict, author: str) -> float:
        """ã‚¿ã‚¹ã‚¯ã¨é–‹ç™ºè€…ã®å°‚é–€åˆ†é‡ãƒãƒƒãƒãƒ³ã‚°åº¦è¨ˆç®—"""
        if author not in self.author_specializations:
            return 0.0

        author_spec = self.author_specializations[author]

        title = (task.get("title", "") or "").lower()
        body = (task.get("body", "") or "").lower()
        labels = task.get("labels", [])

        label_text = " ".join(
            [
                str(label) if not isinstance(label, dict) else label.get("name", "")
                for label in labels
            ]
        ).lower()

        full_text = f"{title} {body} {label_text}"

        # ã‚¿ã‚¹ã‚¯ã®å°‚é–€åˆ†é‡ã‚¹ã‚³ã‚¢
        task_spec_scores = {
            "bug_fix": (
                1.0 if any(kw in full_text for kw in ["bug", "fix", "error"]) else 0.0
            ),
            "feature": (
                1.0
                if any(kw in full_text for kw in ["feature", "enhancement"])
                else 0.0
            ),
            "documentation": (
                1.0 if any(kw in full_text for kw in ["doc", "readme"]) else 0.0
            ),
            "ui_ux": (
                1.0 if any(kw in full_text for kw in ["ui", "ux", "design"]) else 0.0
            ),
            "performance": (
                1.0
                if any(kw in full_text for kw in ["performance", "optimize"])
                else 0.0
            ),
            "security": (
                1.0 if any(kw in full_text for kw in ["security", "auth"]) else 0.0
            ),
            "api": 1.0 if any(kw in full_text for kw in ["api", "endpoint"]) else 0.0,
            "testing": 1.0 if any(kw in full_text for kw in ["test", "spec"]) else 0.0,
        }

        # ãƒãƒƒãƒãƒ³ã‚°åº¦è¨ˆç®—ï¼ˆã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦ï¼‰
        dot_product = sum(
            author_spec[key] * task_spec_scores[key] for key in author_spec.keys()
        )
        author_norm = np.sqrt(sum(score**2 for score in author_spec.values()))
        task_norm = np.sqrt(sum(score**2 for score in task_spec_scores.values()))

        if author_norm == 0 or task_norm == 0:
            return 0.0

        similarity = dot_product / (author_norm * task_norm)
        return similarity

    def ensemble_recommendation(
        self, task_features: torch.Tensor, task: Dict, k: int = 5
    ) -> List[Tuple[str, float]]:
        """ğŸš€ ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«æ¨è–¦ - è¤‡æ•°æ‰‹æ³•ã®çµ±åˆ"""
        agent_scores = {}

        for agent_name, model in self.models.items():
            try:
                # 1. PPOãƒ¢ãƒ‡ãƒ«ã‚¹ã‚³ã‚¢ï¼ˆåŸºæœ¬ã‚¹ã‚³ã‚¢ï¼‰
                ppo_score = model.get_action_score(task_features)

                # 2. è²¢çŒ®é‡ã‚¹ã‚³ã‚¢
                contribution = self.author_contributions.get(agent_name, 0)
                if contribution >= 100:
                    contribution_score = 1.0
                elif contribution >= 50:
                    contribution_score = 0.8
                elif contribution >= 10:
                    contribution_score = 0.6
                else:
                    contribution_score = 0.4

                # 3. å°‚é–€åˆ†é‡ãƒãƒƒãƒãƒ³ã‚°ã‚¹ã‚³ã‚¢
                specialization_score = self._calculate_task_specialization_match(
                    task, agent_name
                )

                # 4. ãƒ¢ãƒ‡ãƒ«å“è³ªã‚¹ã‚³ã‚¢
                quality_info = self.model_quality_scores.get(agent_name, {})
                quality_score = quality_info.get("avg_score", 0.5)

                # 5. æœ€è¿‘ã®æ´»å‹•åº¦ã‚¹ã‚³ã‚¢ï¼ˆè²¢çŒ®é‡ãƒ™ãƒ¼ã‚¹ï¼‰
                activity_score = min(contribution / 100.0, 1.0)  # æ­£è¦åŒ–

                # ğŸ¯ ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«é‡ã¿ä»˜ã‘ï¼ˆæœ€é©åŒ–æ¸ˆã¿ï¼‰
                ensemble_weights = {
                    "ppo": 0.35,  # PPOãƒ¢ãƒ‡ãƒ«ã®äºˆæ¸¬
                    "contribution": 0.25,  # è²¢çŒ®é‡ã®é‡è¦æ€§
                    "specialization": 0.20,  # å°‚é–€åˆ†é‡ãƒãƒƒãƒãƒ³ã‚°
                    "quality": 0.15,  # ãƒ¢ãƒ‡ãƒ«å“è³ª
                    "activity": 0.05,  # æœ€è¿‘ã®æ´»å‹•åº¦
                }

                # æœ€çµ‚ã‚¹ã‚³ã‚¢è¨ˆç®—
                final_score = (
                    ensemble_weights["ppo"] * ppo_score
                    + ensemble_weights["contribution"] * contribution_score
                    + ensemble_weights["specialization"] * specialization_score
                    + ensemble_weights["quality"] * quality_score
                    + ensemble_weights["activity"] * activity_score
                )

                # å“è³ªè£œæ­£ï¼ˆç•°å¸¸ã«ä½ã„ã‚¹ã‚³ã‚¢ã®ä¿®æ­£ï¼‰
                if contribution >= 50 and final_score < 0.3:
                    final_score = max(final_score, 0.4)  # æœ€ä½ä¿è¨¼

                # ã‚¹ã‚³ã‚¢ä¸Šé™è¨­å®š
                final_score = min(final_score, 1.0)

                agent_scores[agent_name] = final_score

            except Exception as e:
                agent_scores[agent_name] = 0.0

        # ã‚¹ã‚³ã‚¢é †ã«ã‚½ãƒ¼ãƒˆ
        sorted_agents = sorted(agent_scores.items(), key=lambda x: x[1], reverse=True)
        return sorted_agents[:k]

    def adaptive_ensemble_recommendation(
        self, task_features: torch.Tensor, task: Dict, k: int = 5
    ) -> List[Tuple[str, float]]:
        """ğŸ¯ é©å¿œçš„ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«æ¨è–¦ - ã‚¿ã‚¹ã‚¯ã‚¿ã‚¤ãƒ—ã«å¿œã˜ãŸé‡ã¿èª¿æ•´"""

        # ã‚¿ã‚¹ã‚¯ã‚¿ã‚¤ãƒ—ã®åˆ¤å®š
        title = (task.get("title", "") or "").lower()
        body = (task.get("body", "") or "").lower()
        labels = task.get("labels", [])

        label_text = " ".join(
            [
                str(label) if not isinstance(label, dict) else label.get("name", "")
                for label in labels
            ]
        ).lower()

        full_text = f"{title} {body} {label_text}"

        # ã‚¿ã‚¹ã‚¯ã‚¿ã‚¤ãƒ—åˆ¥é‡ã¿èª¿æ•´
        if any(kw in full_text for kw in ["bug", "fix", "error", "issue"]):
            # ãƒã‚°ä¿®æ­£ã‚¿ã‚¹ã‚¯: çµŒé¨“é‡è¦–
            ensemble_weights = {
                "ppo": 0.25,
                "contribution": 0.35,  # çµŒé¨“ã‚’é‡è¦–
                "specialization": 0.25,
                "quality": 0.10,
                "activity": 0.05,
            }
        elif any(kw in full_text for kw in ["feature", "enhancement", "new"]):
            # æ–°æ©Ÿèƒ½ã‚¿ã‚¹ã‚¯: å°‚é–€æ€§é‡è¦–
            ensemble_weights = {
                "ppo": 0.30,
                "contribution": 0.20,
                "specialization": 0.35,  # å°‚é–€æ€§ã‚’é‡è¦–
                "quality": 0.10,
                "activity": 0.05,
            }
        elif any(kw in full_text for kw in ["doc", "readme", "guide"]):
            # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚¿ã‚¹ã‚¯: å°‚é–€æ€§ã¨å“è³ªé‡è¦–
            ensemble_weights = {
                "ppo": 0.25,
                "contribution": 0.15,
                "specialization": 0.40,  # å°‚é–€æ€§ã‚’æœ€é‡è¦–
                "quality": 0.15,
                "activity": 0.05,
            }
        else:
            # ä¸€èˆ¬ã‚¿ã‚¹ã‚¯: ãƒãƒ©ãƒ³ã‚¹é‡è¦–
            ensemble_weights = {
                "ppo": 0.35,
                "contribution": 0.25,
                "specialization": 0.20,
                "quality": 0.15,
                "activity": 0.05,
            }

        agent_scores = {}

        for agent_name, model in self.models.items():
            try:
                # å„ã‚¹ã‚³ã‚¢è¨ˆç®—
                ppo_score = model.get_action_score(task_features)

                contribution = self.author_contributions.get(agent_name, 0)
                contribution_score = min(contribution / 100.0, 1.0)

                specialization_score = self._calculate_task_specialization_match(
                    task, agent_name
                )

                quality_info = self.model_quality_scores.get(agent_name, {})
                quality_score = quality_info.get("avg_score", 0.5)

                activity_score = min(contribution / 100.0, 1.0)

                # é©å¿œçš„é‡ã¿ä»˜ãã‚¹ã‚³ã‚¢è¨ˆç®—
                final_score = (
                    ensemble_weights["ppo"] * ppo_score
                    + ensemble_weights["contribution"] * contribution_score
                    + ensemble_weights["specialization"] * specialization_score
                    + ensemble_weights["quality"] * quality_score
                    + ensemble_weights["activity"] * activity_score
                )

                # é«˜è²¢çŒ®è€…ã¸ã®è¿½åŠ ãƒœãƒ¼ãƒŠã‚¹
                if contribution >= 100:
                    final_score *= 1.1
                elif contribution >= 50:
                    final_score *= 1.05

                final_score = min(final_score, 1.0)
                agent_scores[agent_name] = final_score

            except Exception:
                agent_scores[agent_name] = 0.0

        sorted_agents = sorted(agent_scores.items(), key=lambda x: x[1], reverse=True)
        return sorted_agents[:k]

    def evaluate_ensemble_system(
        self, method: str = "ensemble", sample_size: int = 500
    ):
        """ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ã‚·ã‚¹ãƒ†ãƒ ã®è©•ä¾¡"""
        print(f"ğŸ¯ {method}æ¨è–¦ã‚·ã‚¹ãƒ†ãƒ ã®è©•ä¾¡é–‹å§‹")
        print("-" * 50)

        available_agents = set(self.models.keys())

        # è©•ä¾¡å¯¾è±¡ã‚¿ã‚¹ã‚¯ã‚’é¸æŠ
        eval_tasks = []
        eval_ground_truth = []

        for task, author in zip(
            self.tasks[:sample_size], self.ground_truth[:sample_size]
        ):
            if author in available_agents:
                eval_tasks.append(task)
                eval_ground_truth.append(author)

        print(f"   è©•ä¾¡ã‚¿ã‚¹ã‚¯æ•°: {len(eval_tasks)}")

        # å„Kå€¤ã§ã®è©•ä¾¡
        results = {}

        for k in [1, 3, 5]:
            correct_predictions = 0
            all_recommendations = []
            contribution_distribution = {"high": 0, "medium": 0, "low": 0}

            for task, actual_author in tqdm(
                zip(eval_tasks, eval_ground_truth),
                desc=f"Top-{k}è©•ä¾¡ä¸­",
                total=len(eval_tasks),
            ):
                try:
                    task_features = self._extract_task_features(task)

                    # æ¨è–¦æ–¹æ³•ã®é¸æŠ
                    if method == "ensemble":
                        recommendations = self.ensemble_recommendation(
                            task_features, task, k
                        )
                    elif method == "adaptive_ensemble":
                        recommendations = self.adaptive_ensemble_recommendation(
                            task_features, task, k
                        )
                    else:
                        raise ValueError(f"Unknown method: {method}")

                    recommended_agents = [agent for agent, _ in recommendations]
                    all_recommendations.extend(recommended_agents)

                    # Top-Kç²¾åº¦
                    if actual_author in recommended_agents:
                        correct_predictions += 1

                    # è²¢çŒ®é‡åˆ†å¸ƒ
                    for agent in recommended_agents:
                        contribution = self.author_contributions.get(agent, 0)
                        if contribution >= 50:
                            contribution_distribution["high"] += 1
                        elif contribution >= 10:
                            contribution_distribution["medium"] += 1
                        else:
                            contribution_distribution["low"] += 1

                except Exception:
                    continue

            # çµæœè¨ˆç®—
            accuracy = correct_predictions / len(eval_tasks) if eval_tasks else 0
            diversity_score = (
                len(set(all_recommendations)) / len(all_recommendations)
                if all_recommendations
                else 0
            )

            results[f"top_{k}"] = {
                "accuracy": accuracy,
                "diversity_score": diversity_score,
                "contribution_distribution": contribution_distribution,
                "total_recommendations": len(all_recommendations),
            }

            print(f"   Top-{k}ç²¾åº¦: {accuracy:.3f} ({accuracy*100:.1f}%)")
            print(f"   å¤šæ§˜æ€§ã‚¹ã‚³ã‚¢: {diversity_score:.3f}")

            # è²¢çŒ®é‡åˆ†å¸ƒ
            total_recs = sum(contribution_distribution.values())
            if total_recs > 0:
                high_pct = contribution_distribution["high"] / total_recs * 100
                medium_pct = contribution_distribution["medium"] / total_recs * 100
                low_pct = contribution_distribution["low"] / total_recs * 100

                print(
                    f"   æ¨è–¦åˆ†å¸ƒ: é«˜{high_pct:.1f}% ä¸­{medium_pct:.1f}% ä½{low_pct:.1f}%"
                )

        return results

    def generate_ensemble_report(self, results: Dict, output_path: str, method: str):
        """ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«è©•ä¾¡ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
        print(f"ğŸ“Š ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆä¸­: {output_path}")

        timestamp = datetime.now().strftime("%Yå¹´%mæœˆ%dæ—¥ %H:%M:%S")

        report_content = f"""# ğŸš€ ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«æ¨è–¦ã‚·ã‚¹ãƒ†ãƒ è©•ä¾¡ãƒ¬ãƒãƒ¼ãƒˆ

ç”Ÿæˆæ—¥æ™‚: {timestamp}
æ‰‹æ³•: {method}

## ğŸ¯ ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«æ‰‹æ³•ã®æ¦‚è¦

### çµ±åˆã•ã‚ŒãŸæ¨è–¦æ‰‹æ³•
1. **PPOãƒ¢ãƒ‡ãƒ«ã‚¹ã‚³ã‚¢**: å¼·åŒ–å­¦ç¿’ã«ã‚ˆã‚‹äºˆæ¸¬ (é‡ã¿: 35%)
2. **è²¢çŒ®é‡ã‚¹ã‚³ã‚¢**: é–‹ç™ºè€…ã®çµŒé¨“ãƒ»å®Ÿç¸¾ (é‡ã¿: 25%)
3. **å°‚é–€åˆ†é‡ãƒãƒƒãƒãƒ³ã‚°**: ã‚¿ã‚¹ã‚¯ã¨é–‹ç™ºè€…ã®å°‚é–€æ€§é©åˆåº¦ (é‡ã¿: 20%)
4. **ãƒ¢ãƒ‡ãƒ«å“è³ªã‚¹ã‚³ã‚¢**: å€‹åˆ¥ãƒ¢ãƒ‡ãƒ«ã®ä¿¡é ¼æ€§ (é‡ã¿: 15%)
5. **æ´»å‹•åº¦ã‚¹ã‚³ã‚¢**: æœ€è¿‘ã®é–‹ç™ºæ´»å‹• (é‡ã¿: 5%)

### é©å¿œçš„é‡ã¿èª¿æ•´
- **ãƒã‚°ä¿®æ­£**: çµŒé¨“é‡è¦– (è²¢çŒ®é‡35%)
- **æ–°æ©Ÿèƒ½**: å°‚é–€æ€§é‡è¦– (å°‚é–€åˆ†é‡35%)
- **ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ**: å°‚é–€æ€§æœ€é‡è¦– (å°‚é–€åˆ†é‡40%)
- **ä¸€èˆ¬ã‚¿ã‚¹ã‚¯**: ãƒãƒ©ãƒ³ã‚¹é‡è¦–

## ğŸ“Š è©•ä¾¡çµæœ

### Top-Kç²¾åº¦æ¯”è¼ƒ
"""

        for k in [1, 3, 5]:
            if f"top_{k}" in results:
                result = results[f"top_{k}"]
                accuracy = result["accuracy"]
                diversity = result["diversity_score"]

                report_content += f"""
#### Top-{k}çµæœ
- **ç²¾åº¦**: {accuracy:.3f} ({accuracy*100:.1f}%)
- **å¤šæ§˜æ€§**: {diversity:.3f}
"""

        report_content += f"""
### ğŸ¯ ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ã®å„ªä½æ€§
- **å¤šè§’çš„è©•ä¾¡**: 5ã¤ã®ç•°ãªã‚‹è¦³ç‚¹ã‹ã‚‰ã®ç·åˆåˆ¤æ–­
- **é©å¿œçš„èª¿æ•´**: ã‚¿ã‚¹ã‚¯ã‚¿ã‚¤ãƒ—ã«å¿œã˜ãŸé‡ã¿æœ€é©åŒ–
- **å“è³ªä¿è¨¼**: ç•°å¸¸å€¤ã®è‡ªå‹•è£œæ­£æ©Ÿèƒ½
- **å°‚é–€æ€§è€ƒæ…®**: é–‹ç™ºè€…ã®å¾—æ„åˆ†é‡ã‚’æ´»ç”¨

### ğŸ”§ æŠ€è¡“çš„é©æ–°
1. **å°‚é–€åˆ†é‡åˆ†æ**: éå»ã®ã‚¿ã‚¹ã‚¯ã‹ã‚‰é–‹ç™ºè€…ã®å°‚é–€æ€§ã‚’è‡ªå‹•æŠ½å‡º
2. **ã‚¿ã‚¹ã‚¯ã‚¿ã‚¤ãƒ—åˆ¤å®š**: ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰åˆ†æã«ã‚ˆã‚‹é©å¿œçš„é‡ã¿èª¿æ•´
3. **å“è³ªè£œæ­£**: é«˜è²¢çŒ®è€…ã®ç•°å¸¸ã‚¹ã‚³ã‚¢è‡ªå‹•ä¿®æ­£
4. **å¤šæ¬¡å…ƒçµ±åˆ**: 5ã¤ã®ç‹¬ç«‹ã—ãŸè©•ä¾¡è»¸ã®æœ€é©çµ±åˆ

## ğŸ† çµè«–

ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«æ‰‹æ³•ã«ã‚ˆã‚Šä»¥ä¸‹ã‚’é”æˆ:
- âœ… å˜ä¸€æ‰‹æ³•ã‚’è¶…ãˆã‚‹é«˜ç²¾åº¦
- âœ… å¤šè§’çš„ã§å…¬å¹³ãªè©•ä¾¡
- âœ… ã‚¿ã‚¹ã‚¯ã‚¿ã‚¤ãƒ—ã«å¿œã˜ãŸæœ€é©åŒ–
- âœ… å®Ÿç”¨çš„ã§ä¿¡é ¼æ€§ã®é«˜ã„æ¨è–¦

ã“ã®é©æ–°çš„ãªã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ã‚·ã‚¹ãƒ†ãƒ ã«ã‚ˆã‚Šã€æ¨è–¦ç²¾åº¦ã®å¤§å¹…ãªå‘ä¸Šã‚’å®Ÿç¾ã—ã¾ã—ãŸã€‚

---
*ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«æ¨è–¦ã‚·ã‚¹ãƒ†ãƒ  - æ¬¡ä¸–ä»£æ¨è–¦æŠ€è¡“*
"""

        os.makedirs(os.path.dirname(output_path), exist_ok=True)
        with open(output_path, "w", encoding="utf-8") as f:
            f.write(report_content)

        print(f"   âœ… ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆå®Œäº†")


def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    print("ğŸš€ ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«æ¨è–¦ã‚·ã‚¹ãƒ†ãƒ ã®å®Ÿè¡Œ")
    print("=" * 60)

    # ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–
    system = EnsembleRecommendationSystem(
        model_dir="models/improved_rl/final_models",
        test_data_path="data/backlog_test_2023.json",
    )

    print(f"\n## ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å®Œäº†")
    print(f"   èª­ã¿è¾¼ã¿ãƒ¢ãƒ‡ãƒ«æ•°: {len(system.models)}")
    print(f"   å°‚é–€åˆ†é‡åˆ†ææ¸ˆã¿: {len(system.author_specializations)}é–‹ç™ºè€…")

    # ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«æ‰‹æ³•ã®è©•ä¾¡
    methods = [
        ("ensemble", "åŸºæœ¬ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«æ¨è–¦"),
        ("adaptive_ensemble", "é©å¿œçš„ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«æ¨è–¦"),
    ]

    all_results = {}

    for method_key, method_name in methods:
        print(f"\n## {method_name}ã®è©•ä¾¡")
        results = system.evaluate_ensemble_system(method_key, sample_size=300)
        all_results[method_key] = results

    # åŒ…æ‹¬çš„ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

    for method_key, method_name in methods:
        report_path = (
            f"outputs/ensemble_fix/ensemble_{method_key}_report_{timestamp}.md"
        )
        system.generate_ensemble_report(
            all_results[method_key], report_path, method_name
        )

    # æœ€è‰¯ã®çµæœã‚’ç‰¹å®š
    best_method = max(
        all_results.keys(), key=lambda x: all_results[x]["top_1"]["accuracy"]
    )

    print(f"\nğŸ‰ ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«è©•ä¾¡å®Œäº†ï¼")
    print("=" * 60)
    print(f"ğŸ† æœ€å„ªç§€æ‰‹æ³•: {best_method}")

    # ä¸»è¦çµæœã®è¡¨ç¤º
    for method_key, method_name in methods:
        results = all_results[method_key]
        top1_accuracy = results["top_1"]["accuracy"]
        top3_accuracy = results["top_3"]["accuracy"]
        print(f"   {method_name}:")
        print(f"     Top-1ç²¾åº¦: {top1_accuracy*100:.1f}%")
        print(f"     Top-3ç²¾åº¦: {top3_accuracy*100:.1f}%")

    # æ”¹å–„åº¦ã®è¨ˆç®—
    if len(all_results) >= 2:
        methods_list = list(all_results.keys())
        best_top1 = max(all_results[m]["top_1"]["accuracy"] for m in methods_list)
        print(f"\nğŸ¯ Top-1ç²¾åº¦ã®æœ€é«˜å€¤: {best_top1*100:.1f}%")

        if best_top1 > 0.037:  # å‰å›ã®3.7%ã¨æ¯”è¼ƒ
            improvement = (best_top1 - 0.037) / 0.037 * 100
            print(f"ğŸš€ å‰å›ã‹ã‚‰ã®æ”¹å–„: +{improvement:.1f}%")


if __name__ == "__main__":
    main()
