#!/usr/bin/env python3
"""
æ”¹å–„ç‰ˆGATç‰¹å¾´é‡çµ±åˆå¼·åŒ–å­¦ç¿’æ¨è–¦ã‚·ã‚¹ãƒ†ãƒ  v2

æ”¹å–„ç‚¹:
1. æ­£è§£å ±é…¬ã‚’åœ§å€’çš„ã«é‡è¦ã« (100.0)
2. GATç‰¹å¾´é‡ã¯è£œåŠ©çš„å½¹å‰² (æœ€å¤§5.5)
3. é–“é•ã„ã«å¯¾ã™ã‚‹æ˜ç¢ºãªãƒšãƒŠãƒ«ãƒ†ã‚£ (-20.0)
4. ç„¡åŠ¹è¡Œå‹•ã«å¯¾ã™ã‚‹é‡ã„ãƒšãƒŠãƒ«ãƒ†ã‚£ (-50.0)
"""

import argparse
import json
import pickle
import sys
from collections import Counter, defaultdict
from datetime import datetime
from pathlib import Path

import gymnasium as gym
import numpy as np
import pandas as pd
import torch
import yaml
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import StandardScaler
from stable_baselines3 import DQN, PPO
from stable_baselines3.common.callbacks import EvalCallback
from stable_baselines3.common.vec_env import DummyVecEnv

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
sys.path.append('/Users/kazuki-h/rl/kazoo')
sys.path.append('/Users/kazuki-h/rl/kazoo/src')

from src.kazoo.envs.task import Task
from src.kazoo.features.feature_extractor import FeatureExtractor


class ImprovedGATRLEnvironmentV2(gym.Env):
    """
    æ”¹å–„ç‰ˆGATç‰¹å¾´é‡çµ±åˆå¼·åŒ–å­¦ç¿’ç’°å¢ƒ v2
    - æ­£è§£å ±é…¬ã‚’åœ§å€’çš„ã«é‡è¦ã«
    - GATç‰¹å¾´é‡ã¯è£œåŠ©çš„å½¹å‰²
    """
    
    def __init__(self, config, training_data, dev_profiles):
        super().__init__()
        
        self.config = config
        self.training_data = training_data
        self.dev_profiles = dev_profiles
        
        # 1. ç‰¹å¾´é‡æŠ½å‡ºå™¨ã®åˆæœŸåŒ–
        print("ğŸ”§ GATç‰¹å¾´é‡æŠ½å‡ºå™¨åˆæœŸåŒ–ä¸­...")
        # è¨­å®šã‚’DictConfigé¢¨ã«ãƒ©ãƒƒãƒ—ã—ã¦å¿…è¦ãªè¨­å®šã‚’è¿½åŠ 
        from omegaconf import DictConfig
        if isinstance(config, dict):
            # å¿…è¦ãªè¨­å®šã‚’è¿½åŠ 
            if 'features' not in config:
                config['features'] = {
                    'recent_activity_window_days': 30,
                    'all_labels': ["bug", "enhancement", "documentation", "question", "help wanted"],
                    'label_to_skills': {
                        'bug': ["debugging", "analysis"],
                        'enhancement': ["python", "design"],
                        'documentation': ["writing"],
                        'question': ["communication"],
                        'help wanted': ["collaboration"]
                    }
                }
            if 'gat' not in config:
                config['gat'] = {
                    'model_path': "data/gnn_model_collaborative.pt",
                    'graph_data_path': "data/graph_collaborative.pt"
                }
            config = DictConfig(config)
        self.feature_extractor = FeatureExtractor(config)
        
        # 2. IRLé‡ã¿èª­ã¿è¾¼ã¿
        irl_weights_path = "data/learned_weights_training.npy"
        if Path(irl_weights_path).exists():
            self.irl_weights = torch.tensor(np.load(irl_weights_path), dtype=torch.float32)
            print(f"âœ… IRLé‡ã¿èª­ã¿è¾¼ã¿: {irl_weights_path} ({tuple(self.irl_weights.shape)})")
        else:
            self.irl_weights = torch.zeros(62, dtype=torch.float32)
            print(f"âš ï¸ IRLé‡ã¿ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {irl_weights_path}")
        
        # 3. ã‚¢ã‚¯ãƒ†ã‚£ãƒ–é–‹ç™ºè€…æŠ½å‡ºï¼ˆè¨“ç·´ãƒ‡ãƒ¼ã‚¿ã«åŸºã¥ãï¼‰
        training_pairs = []
        developer_stats = Counter()
        
        for task_data in training_data:
            task_id = task_data.get('id') or task_data.get('number')
            if not task_id:
                continue
            
            assignee = None
            if 'assignees' in task_data and task_data['assignees']:
                assignee = task_data['assignees'][0].get('login')
            elif 'events' in task_data:
                for event in task_data['events']:
                    if event.get('event') == 'assigned' and event.get('assignee'):
                        assignee = event['assignee'].get('login')
                        break
            
            if assignee:
                training_pairs.append({
                    'task_data': task_data,
                    'developer': assignee,
                    'task_id': task_id
                })
                developer_stats[assignee] += 1
        
        print(f"ğŸ“‹ ã‚¢ã‚¯ãƒ†ã‚£ãƒ–é–‹ç™ºè€…æŠ½å‡º: {len(developer_stats)}äºº")
        top_devs = developer_stats.most_common(6)  # ä¸Šä½6äººã«åˆ¶é™
        print("   ä¸Šä½ã‚¢ã‚¯ãƒ†ã‚£ãƒ–é–‹ç™ºè€…:")
        for dev, count in top_devs:
            print(f"     {dev}: {count} ã‚¿ã‚¹ã‚¯")
        
        self.active_developers = [dev for dev, _ in top_devs]
        self.num_developers = len(self.active_developers)
        
        print(f"ğŸ¯ è¡Œå‹•ç©ºé–“ã‚’å®Ÿè¨“ç·´é–‹ç™ºè€…ã«åˆ¶é™: {self.num_developers}äºº")
        
        # 4. ç’°å¢ƒè¨­å®š
        feature_dim = len(self.feature_extractor.feature_names)
        
        self.action_space = gym.spaces.Discrete(self.num_developers)
        self.observation_space = gym.spaces.Box(
            low=-np.inf, 
            high=np.inf, 
            shape=(feature_dim,),  # GATç‰¹å¾´é‡ã®ã¿
            dtype=np.float32
        )
        
        # å­¦ç¿’ãƒšã‚¢
        self.training_pairs = [pair for pair in training_pairs 
                             if pair['developer'] in self.active_developers]
        
        print(f"ğŸ¤– æ”¹å–„ç‰ˆGAT-RLç’°å¢ƒv2åˆæœŸåŒ–å®Œäº†")
        print(f"   ã‚¢ã‚¯ãƒ†ã‚£ãƒ–é–‹ç™ºè€…: {self.num_developers}")
        print(f"   è¦³æ¸¬æ¬¡å…ƒ: {feature_dim} (GATç‰¹å¾´é‡ã®ã¿)")
        print(f"   å­¦ç¿’ãƒšã‚¢æ•°: {len(self.training_pairs)}")
        
        # ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ç®¡ç†
        self.current_pair_idx = 0
        self.current_pair = None
    
    def reset(self, seed=None, options=None):
        """ç’°å¢ƒã‚’ãƒªã‚»ãƒƒãƒˆ"""
        super().reset(seed=seed)
        
        if not self.training_pairs:
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ç©ºã®è¦³æ¸¬
            obs = np.zeros(self.observation_space.shape[0], dtype=np.float32)
            return obs, {}
        
        # ãƒ©ãƒ³ãƒ€ãƒ ãªãƒšã‚¢ã‚’é¸æŠ
        self.current_pair_idx = np.random.randint(0, len(self.training_pairs))
        self.current_pair = self.training_pairs[self.current_pair_idx]
        
        # åˆæœŸè¦³æ¸¬ã‚’å–å¾—
        obs = self._get_observation()
        
        return obs, {}
    
    def step(self, action):
        """ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚’å®Ÿè¡Œ"""
        if action >= self.num_developers:
            # ç„¡åŠ¹ãªã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã«å¯¾ã™ã‚‹é‡ã„ãƒšãƒŠãƒ«ãƒ†ã‚£
            reward = -50.0
            terminated = True
            obs = np.zeros(self.observation_space.shape[0], dtype=np.float32)
            return obs, reward, terminated, False, {}
        
        selected_dev = self.active_developers[action]
        actual_dev = self.current_pair['developer']
        
        # å ±é…¬è¨ˆç®—ï¼ˆæ”¹å–„ç‰ˆv2ï¼‰
        reward = self._calculate_improved_reward_v2(selected_dev, actual_dev)
        
        # ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰çµ‚äº†
        terminated = True
        
        # æ¬¡ã®è¦³æ¸¬ï¼ˆã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰çµ‚äº†ãªã®ã§ç¾åœ¨ã®è¦³æ¸¬ï¼‰
        obs = self._get_observation()
        
        # è©³ç´°æƒ…å ±
        info = {
            'selected_dev': selected_dev,
            'actual_dev': actual_dev,
            'correct': selected_dev == actual_dev,
            'task_id': self.current_pair['task_id'],
            'reward_breakdown': getattr(self, 'last_reward_breakdown', {})
        }
        
        return obs, reward, terminated, False, info
    
    def _get_observation(self):
        """ç¾åœ¨ã®ã‚¿ã‚¹ã‚¯ã¨é¸æŠè‚¢ã®è¦³æ¸¬ã‚’å–å¾—"""
        if not self.current_pair:
            return np.zeros(self.observation_space.shape[0], dtype=np.float32)
        
        try:
            # ã‚¿ã‚¹ã‚¯ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆä½œæˆ
            task_obj = Task(self.current_pair['task_data'])
            
            # ãƒ€ãƒŸãƒ¼é–‹ç™ºè€…ï¼ˆå®Ÿéš›ã®é–‹ç™ºè€…ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½¿ç”¨ï¼‰
            actual_dev = self.current_pair['developer']
            dev_profile = self.dev_profiles.get(actual_dev, {})
            developer_obj = {"name": actual_dev, "profile": dev_profile}
            
            # ãƒ€ãƒŸãƒ¼ç’°å¢ƒ
            dummy_env = type('DummyEnv', (), {
                'backlog': [task_obj],
                'dev_profiles': self.dev_profiles,
                'assignments': {},
                'dev_action_history': {}
            })()
            
            # GATç‰¹å¾´é‡æŠ½å‡º
            features = self.feature_extractor.get_features(
                task_obj, developer_obj, dummy_env
            )
            
            return np.array(features, dtype=np.float32)
            
        except Exception as e:
            print(f"âš ï¸ è¦³æ¸¬å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ã‚¼ãƒ­ãƒ™ã‚¯ãƒˆãƒ«
            return np.zeros(self.observation_space.shape[0], dtype=np.float32)
    
    def _calculate_improved_reward_v2(self, selected_dev, actual_dev):
        """æ”¹å–„ç‰ˆå ±é…¬è¨ˆç®—v2 - æ­£è§£å ±é…¬ã‚’åœ§å€’çš„ã«é‡è¦ã«"""
        total_reward = 0.0
        reward_breakdown = {}
        
        # 1. æ­£è§£å ±é…¬ (åœ§å€’çš„ã«é‡è¦)
        if selected_dev == actual_dev:
            correct_reward = 100.0  # åœ§å€’çš„ãªæ­£è§£å ±é…¬
            total_reward += correct_reward
            reward_breakdown['correct'] = correct_reward
        else:
            # é–“é•ã„ã«å¯¾ã™ã‚‹æ˜ç¢ºãªãƒšãƒŠãƒ«ãƒ†ã‚£
            wrong_penalty = -20.0
            total_reward += wrong_penalty
            reward_breakdown['correct'] = wrong_penalty
        
        # 2. GATç‰¹å¾´é‡å ±é…¬ (è£œåŠ©çš„å½¹å‰²ã®ã¿)
        try:
            # é¸æŠã•ã‚ŒãŸé–‹ç™ºè€…ã§ã®GATç‰¹å¾´é‡æŠ½å‡º
            task_obj = Task(self.current_pair['task_data'])
            selected_dev_profile = self.dev_profiles.get(selected_dev, {})
            selected_dev_obj = {"name": selected_dev, "profile": selected_dev_profile}
            
            dummy_env = type('DummyEnv', (), {
                'backlog': [task_obj],
                'dev_profiles': self.dev_profiles,
                'assignments': {},
                'dev_action_history': {}
            })()
            
            # GATç‰¹å¾´é‡æŠ½å‡º
            features = self.feature_extractor.get_features(
                task_obj, selected_dev_obj, dummy_env
            )
            
            # GATé¡ä¼¼åº¦ç‰¹å¾´é‡ (è£œåŠ©çš„)
            gat_similarity_idx = [i for i, name in enumerate(self.feature_extractor.feature_names) 
                                if name == 'gat_similarity']
            if gat_similarity_idx:
                similarity_score = features[gat_similarity_idx[0]]
                similarity_reward = similarity_score * 2.0  # è£œåŠ©çš„å½¹å‰²
                total_reward += similarity_reward
                reward_breakdown['gat_similarity'] = similarity_reward
            
            # GATå°‚é–€æ€§å ±é…¬ (è£œåŠ©çš„)
            expertise_idx = [i for i, name in enumerate(self.feature_extractor.feature_names) 
                           if name == 'gat_dev_expertise']
            if expertise_idx:
                expertise_score = features[expertise_idx[0]]
                expertise_reward = expertise_score * 1.5  # è£œåŠ©çš„
                total_reward += expertise_reward
                reward_breakdown['gat_expertise'] = expertise_reward
            
            # GATå”åŠ›å¼·åº¦å ±é…¬ (è£œåŠ©çš„)
            collaboration_idx = [i for i, name in enumerate(self.feature_extractor.feature_names) 
                               if name == 'gat_collaboration_strength']
            if collaboration_idx:
                collab_score = features[collaboration_idx[0]]
                collab_reward = collab_score * 1.0  # è£œåŠ©çš„
                total_reward += collab_reward
                reward_breakdown['gat_collaboration'] = collab_reward
            
            # IRLé‡ã¿ã¨ã®çµ±åˆå ±é…¬ (è£œåŠ©çš„)
            features_tensor = torch.tensor(features, dtype=torch.float32)
            irl_score = torch.dot(self.irl_weights, features_tensor).item()
            irl_reward = np.tanh(irl_score) * 1.0  # è£œåŠ©çš„
            total_reward += irl_reward
            reward_breakdown['irl_compatibility'] = irl_reward
            
        except Exception as e:
            print(f"âš ï¸ GATå ±é…¬è¨ˆç®—ã‚¨ãƒ©ãƒ¼: {e}")
            # ã‚¨ãƒ©ãƒ¼æ™‚ã¯GATå ±é…¬ã‚’0ã«
            reward_breakdown['gat_similarity'] = 0.0
            reward_breakdown['gat_expertise'] = 0.0
            reward_breakdown['gat_collaboration'] = 0.0
            reward_breakdown['irl_compatibility'] = 0.0
        
        # å ±é…¬è©³ç´°ã‚’ä¿å­˜
        self.last_reward_breakdown = reward_breakdown
        
        return total_reward


class ImprovedGATRLRecommenderV2:
    """æ”¹å–„ç‰ˆGATç‰¹å¾´é‡çµ±åˆå¼·åŒ–å­¦ç¿’æ¨è–¦ã‚·ã‚¹ãƒ†ãƒ v2"""
    
    def __init__(self, config_path):
        self.config_path = config_path
        
        with open(config_path, 'r', encoding='utf-8') as f:
            self.config = yaml.safe_load(f)
        
        print("ğŸš€ æ”¹å–„ç‰ˆGATçµ±åˆå¼·åŒ–å­¦ç¿’æ¨è–¦ã‚·ã‚¹ãƒ†ãƒ v2åˆæœŸåŒ–å®Œäº†")
    
    def load_data(self, data_path):
        """ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã‚“ã§æ™‚ç³»åˆ—åˆ†å‰²"""
        print("ğŸ“Š ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ä¸­...")
        
        with open(data_path, 'r', encoding='utf-8') as f:
            all_data = json.load(f)
        
        # æ™‚ç³»åˆ—åˆ†å‰²
        training_data = []  # 2022å¹´ä»¥å‰
        test_data = []      # 2023å¹´
        
        for task in all_data:
            created_at = task.get('created_at', '')
            if created_at.startswith('2022'):
                training_data.append(task)
            elif created_at.startswith('2023'):
                test_data.append(task)
        
        print(f"   å­¦ç¿’ãƒ‡ãƒ¼ã‚¿: {len(training_data):,} ã‚¿ã‚¹ã‚¯ (2022å¹´)")
        print(f"   ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿: {len(test_data):,} ã‚¿ã‚¹ã‚¯ (2023å¹´)")
        
        return training_data, test_data
    
    def load_dev_profiles(self, profiles_path="configs/dev_profiles.yaml"):
        """é–‹ç™ºè€…ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿"""
        try:
            with open(profiles_path, 'r', encoding='utf-8') as f:
                profiles = yaml.safe_load(f)
            print(f"ğŸ“‹ é–‹ç™ºè€…ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿: {len(profiles)} äºº")
            return profiles
        except Exception as e:
            print(f"âš ï¸ ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            return {}
    
    def train_improved_rl_agent_v2(self, training_data, dev_profiles, total_timesteps=100000):
        """æ”¹å–„ç‰ˆGATçµ±åˆå¼·åŒ–å­¦ç¿’ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆè¨“ç·´v2"""
        print("ğŸ“ æ”¹å–„ç‰ˆGATçµ±åˆå¼·åŒ–å­¦ç¿’ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆv2è¨“ç·´é–‹å§‹...")
        
        # ç’°å¢ƒä½œæˆ
        def make_env():
            return ImprovedGATRLEnvironmentV2(self.config, training_data, dev_profiles)
        
        env = DummyVecEnv([make_env])
        
        # PPOã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆï¼ˆæœ€é©åŒ–ç‰ˆï¼‰
        self.rl_agent = PPO(
            "MlpPolicy",
            env,
            verbose=1,
            learning_rate=0.0001,  # ä½ã„å­¦ç¿’ç‡ã§å®‰å®šåŒ–
            n_steps=256,           # å°ã•ãªãƒãƒƒãƒã‚µã‚¤ã‚º
            batch_size=64,         # ã•ã‚‰ã«å°ã•ã
            n_epochs=30,           # ã‚¨ãƒãƒƒã‚¯æ•°å¢—åŠ 
            gamma=0.98,            # å‰²å¼•ç‡ã‚’é«˜ã
            gae_lambda=0.95,       # GAE
            clip_range=0.1,        # ä¿å®ˆçš„ãªã‚¯ãƒªãƒƒãƒ—
            ent_coef=0.01,         # ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ä¿‚æ•°
            vf_coef=1.0,           # ä¾¡å€¤é–¢æ•°ä¿‚æ•°
            max_grad_norm=0.5,     # å‹¾é…ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°
            device="auto",
            policy_kwargs=dict(
                net_arch=[256, 256, 128],  # ã‚ˆã‚Šå¤§ããªãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯
                activation_fn=torch.nn.ReLU
            )
        )
        
        # è©•ä¾¡ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯
        eval_env = DummyVecEnv([make_env])
        eval_callback = EvalCallback(
            eval_env,
            best_model_save_path="./models/improved_gat_rl_v2_best/",
            log_path="./logs/improved_gat_rl_v2_eval/",
            eval_freq=2000,
            deterministic=True,
            render=False,
            verbose=1
        )
        
        # è¨“ç·´å®Ÿè¡Œ
        print(f"   è¨“ç·´ã‚¹ãƒ†ãƒƒãƒ—: {total_timesteps:,}")
        print(f"   æ¨å®šæ™‚é–“: {total_timesteps / 1000:.1f}åˆ†")
        
        self.rl_agent.learn(
            total_timesteps=total_timesteps,
            callback=eval_callback,
            progress_bar=False
        )
        
        print("âœ… æ”¹å–„ç‰ˆGATçµ±åˆå¼·åŒ–å­¦ç¿’ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆv2è¨“ç·´å®Œäº†")
    
    def predict_with_improved_gat_rl_v2(self, test_data, dev_profiles):
        """æ”¹å–„ç‰ˆGATå¼·åŒ–å­¦ç¿’v2ã§äºˆæ¸¬"""
        print("ğŸ¤– æ”¹å–„ç‰ˆGATå¼·åŒ–å­¦ç¿’v2äºˆæ¸¬å®Ÿè¡Œä¸­...")
        
        # ãƒ†ã‚¹ãƒˆç’°å¢ƒä½œæˆ
        test_env = ImprovedGATRLEnvironmentV2(self.config, test_data, dev_profiles)
        
        predictions = {}
        prediction_scores = {}
        
        # ãƒ†ã‚¹ãƒˆç”¨ã®ã‚¢ã‚¯ãƒ†ã‚£ãƒ–é–‹ç™ºè€…æŠ½å‡º
        test_developer_stats = Counter()
        test_assignments = {}
        
        for task_data in test_data:
            task_id = task_data.get('id') or task_data.get('number')
            if not task_id:
                continue
            
            assignee = None
            if 'assignees' in task_data and task_data['assignees']:
                assignee = task_data['assignees'][0].get('login')
            elif 'events' in task_data:
                for event in task_data['events']:
                    if event.get('event') == 'assigned' and event.get('assignee'):
                        assignee = event['assignee'].get('login')
                        break
            
            if assignee:
                test_assignments[task_id] = assignee
                test_developer_stats[assignee] += 1
        
        # ã‚¢ã‚¯ãƒ†ã‚£ãƒ–é–‹ç™ºè€…ã«é™å®š
        active_test_tasks = {
            task_id: dev for task_id, dev in test_assignments.items()
            if dev in test_env.active_developers
        }
        
        print(f"   äºˆæ¸¬å¯¾è±¡: {len(active_test_tasks)} ã‚¿ã‚¹ã‚¯ï¼ˆã‚¢ã‚¯ãƒ†ã‚£ãƒ–é–‹ç™ºè€…ã®ã¿ï¼‰")
        
        progress_count = 0
        total_tasks = len(active_test_tasks) * len(test_env.active_developers)
        
        # äºˆæ¸¬å®Ÿè¡Œ
        for task_data in test_data:
            task_id = task_data.get('id') or task_data.get('number')
            if task_id not in active_test_tasks:
                continue
            
            try:
                # ã‚¿ã‚¹ã‚¯ã‚’ã‚»ãƒƒãƒˆ
                test_env.current_pair = {
                    'task_data': task_data,
                    'task_id': task_id,
                    'developer': active_test_tasks[task_id]
                }
                
                # è¦³æ¸¬å–å¾—
                obs = test_env._get_observation()
                if obs is None:
                    continue
                
                # numpyé…åˆ—ã‚’PyTorchãƒ†ãƒ³ã‚µãƒ¼ã«å¤‰æ›
                import torch
                obs_tensor = torch.FloatTensor(obs).unsqueeze(0)
                
                # PPOãƒãƒªã‚·ãƒ¼ã‹ã‚‰è¡Œå‹•ç¢ºç‡åˆ†å¸ƒã‚’å–å¾—
                with torch.no_grad():
                    action_probs = self.rl_agent.policy.get_distribution(obs_tensor).distribution.probs.squeeze().cpu().numpy()
                
                # å„é–‹ç™ºè€…ã®ã‚¹ã‚³ã‚¢ã‚’å–å¾—
                action_scores = []
                for dev_idx in range(test_env.num_developers):
                    dev_name = test_env.active_developers[dev_idx]
                    score = float(action_probs[dev_idx])
                    action_scores.append((dev_name, score))
                
                # ã‚¹ã‚³ã‚¢é †ã«ã‚½ãƒ¼ãƒˆ
                action_scores.sort(key=lambda x: x[1], reverse=True)
                
                # æœ€é«˜ã‚¹ã‚³ã‚¢ã®é–‹ç™ºè€…ã‚’é¸æŠ
                if action_scores:
                    predicted_dev = action_scores[0][0]
                    best_score = action_scores[0][1]
                    
                    predictions[task_id] = predicted_dev
                    prediction_scores[task_id] = {
                        'predicted_dev': predicted_dev,
                        'score': best_score,
                        'all_scores': dict(action_scores)  # å…¨é–‹ç™ºè€…ã®ã‚¹ã‚³ã‚¢ï¼ˆã‚½ãƒ¼ãƒˆæ¸ˆã¿ï¼‰
                    }
                
                progress_count += len(test_env.active_developers)
                if progress_count % 100 == 0:
                    print(f"   é€²æ—: {progress_count // len(test_env.active_developers)}/{len(active_test_tasks)}")
                
            except Exception as e:
                print(f"âš ï¸ äºˆæ¸¬ã‚¨ãƒ©ãƒ¼ (task {task_id}): {e}")
                continue
        
        print(f"   æ”¹å–„GAT-RLv2äºˆæ¸¬å®Œäº†: {len(predictions)} ã‚¿ã‚¹ã‚¯")
        return predictions, prediction_scores, active_test_tasks
    
    def run_full_pipeline_v2(self, data_path, total_timesteps=100000):
        """å®Œå…¨ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®å®Ÿè¡Œv2"""
        print("ğŸš€ æ”¹å–„ç‰ˆGATçµ±åˆå¼·åŒ–å­¦ç¿’æ¨è–¦ã‚·ã‚¹ãƒ†ãƒ v2å®Ÿè¡Œé–‹å§‹")
        print("=" * 80)
        
        # 1. ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
        training_data, test_data = self.load_data(data_path)
        
        # 2. é–‹ç™ºè€…ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿
        dev_profiles = self.load_dev_profiles()
        
        # 3. æ”¹å–„ç‰ˆGATçµ±åˆå¼·åŒ–å­¦ç¿’ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆè¨“ç·´
        self.train_improved_rl_agent_v2(training_data, dev_profiles, total_timesteps)
        
        # 4. äºˆæ¸¬å®Ÿè¡Œ
        predictions, prediction_scores, test_assignments = self.predict_with_improved_gat_rl_v2(
            test_data, dev_profiles
        )
        
        # 5. è©•ä¾¡
        metrics = self.evaluate_predictions_v2(predictions, test_assignments, prediction_scores)
        
        # 6. ãƒ¢ãƒ‡ãƒ«ä¿å­˜
        self.save_model_v2()
        
        print("âœ… æ”¹å–„ç‰ˆGATçµ±åˆå¼·åŒ–å­¦ç¿’æ¨è–¦ã‚·ã‚¹ãƒ†ãƒ v2å®Œäº†")
        return metrics
    
    def evaluate_predictions_v2(self, predictions, test_assignments, prediction_scores=None):
        """äºˆæ¸¬çµæœã®è©•ä¾¡v2ï¼ˆTop-Kè©•ä¾¡å«ã‚€ï¼‰"""
        print("ğŸ“Š æ”¹å–„ç‰ˆGAT-RLv2è©•ä¾¡ä¸­...")
        
        # å…±é€šã‚¿ã‚¹ã‚¯ã§è©•ä¾¡
        common_tasks = set(predictions.keys()) & set(test_assignments.keys())
        print(f"   è©•ä¾¡å¯¾è±¡: {len(common_tasks)} ã‚¿ã‚¹ã‚¯")
        
        if not common_tasks:
            print("âš ï¸ è©•ä¾¡å¯èƒ½ãªã‚¿ã‚¹ã‚¯ãŒã‚ã‚Šã¾ã›ã‚“")
            return {}
        
        # Top-1 (å¾“æ¥ã®)æ­£ç¢ºæ€§è©•ä¾¡
        correct_predictions = 0
        for task_id in common_tasks:
            if predictions[task_id] == test_assignments[task_id]:
                correct_predictions += 1
        
        accuracy = correct_predictions / len(common_tasks)
        
        # Top-Kè©•ä¾¡
        topk_metrics = self._evaluate_topk_accuracy_v2(common_tasks, test_assignments, prediction_scores)
        
        metrics = {
            'improved_gat_rl_v2_accuracy': accuracy,
            'improved_gat_rl_v2_top1_accuracy': accuracy,
            'improved_gat_rl_v2_top3_accuracy': topk_metrics['top3_accuracy'],
            'improved_gat_rl_v2_top5_accuracy': topk_metrics['top5_accuracy'],
            'improved_gat_rl_v2_correct': correct_predictions,
            'improved_gat_rl_v2_total': len(common_tasks)
        }
        
        print(f"ğŸ¤– æ”¹å–„ç‰ˆGATå¼·åŒ–å­¦ç¿’ã‚·ã‚¹ãƒ†ãƒ v2:")
        print(f"   Top-1ç²¾åº¦: {accuracy:.3f} ({correct_predictions}/{len(common_tasks)})")
        print(f"   Top-3ç²¾åº¦: {topk_metrics['top3_accuracy']:.3f} ({topk_metrics['top3_correct']}/{len(common_tasks)})")
        print(f"   Top-5ç²¾åº¦: {topk_metrics['top5_accuracy']:.3f} ({topk_metrics['top5_correct']}/{len(common_tasks)})")
        
        return metrics
    
    def _evaluate_topk_accuracy_v2(self, common_tasks, test_assignments, prediction_scores):
        """Top-K accuracyè©•ä¾¡v2"""
        top3_correct = 0
        top5_correct = 0
        
        for task_id in common_tasks:
            actual_dev = test_assignments[task_id]
            
            if prediction_scores and task_id in prediction_scores:
                # å…¨ã‚¹ã‚³ã‚¢ã‚’å–å¾—
                all_scores = prediction_scores[task_id].get('all_scores', {})
                if not all_scores:
                    continue
                
                # ã‚¹ã‚³ã‚¢é †ã«é–‹ç™ºè€…ã‚’å–å¾—
                sorted_devs = list(all_scores.keys())
                
                # Top-3è©•ä¾¡
                if len(sorted_devs) >= 3:
                    top3_devs = sorted_devs[:3]
                    if actual_dev in top3_devs:
                        top3_correct += 1
                elif actual_dev in sorted_devs:
                    top3_correct += 1
                
                # Top-5è©•ä¾¡
                if len(sorted_devs) >= 5:
                    top5_devs = sorted_devs[:5]
                    if actual_dev in top5_devs:
                        top5_correct += 1
                elif actual_dev in sorted_devs:
                    top5_correct += 1
        
        total_tasks = len(common_tasks)
        return {
            'top3_accuracy': top3_correct / total_tasks if total_tasks > 0 else 0.0,
            'top5_accuracy': top5_correct / total_tasks if total_tasks > 0 else 0.0,
            'top3_correct': top3_correct,
            'top5_correct': top5_correct
        }
    
    def save_model_v2(self, output_dir="models"):
        """ãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜v2"""
        output_dir = Path(output_dir)
        output_dir.mkdir(exist_ok=True)
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        model_path = output_dir / f"improved_gat_rl_recommender_v2_{timestamp}.zip"
        
        self.rl_agent.save(model_path)
        print(f"âœ… æ”¹å–„ç‰ˆGATçµ±åˆRLãƒ¢ãƒ‡ãƒ«v2ä¿å­˜: {model_path}")
        return model_path


def main():
    parser = argparse.ArgumentParser(description='æ”¹å–„ç‰ˆGATçµ±åˆå¼·åŒ–å­¦ç¿’æ¨è–¦ã‚·ã‚¹ãƒ†ãƒ v2')
    parser.add_argument('--config', default='configs/unified_rl.yaml',
                       help='è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹')
    parser.add_argument('--data', default='data/backlog.json',
                       help='çµ±åˆãƒ‡ãƒ¼ã‚¿ãƒ‘ã‚¹')
    parser.add_argument('--timesteps', type=int, default=100000,
                       help='è¨“ç·´ã‚¹ãƒ†ãƒƒãƒ—æ•°')
    
    args = parser.parse_args()
    
    # æ¨è–¦ã‚·ã‚¹ãƒ†ãƒ å®Ÿè¡Œ
    recommender = ImprovedGATRLRecommenderV2(args.config)
    metrics = recommender.run_full_pipeline_v2(args.data, args.timesteps)
    
    print("\nğŸ¯ æœ€çµ‚çµæœ:")
    if metrics:
        for key, value in metrics.items():
            if isinstance(value, (int, float)):
                print(f"   {key}: {value:.3f}")
    
    return 0


if __name__ == "__main__":
    exit(main())
