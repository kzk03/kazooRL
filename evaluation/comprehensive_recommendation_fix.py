#!/usr/bin/env python3
"""
æ¨è–¦ã‚·ã‚¹ãƒ†ãƒ ã®æ ¹æœ¬çš„è§£æ±º
å…¨ã¦ã®å•é¡Œã‚’åŒ…æ‹¬çš„ã«ä¿®æ­£ã™ã‚‹å®Œå…¨ç‰ˆå®Ÿè£…
"""

import json
import os
from collections import Counter, defaultdict
from datetime import datetime
from typing import Dict, List, Tuple

import numpy as np
import torch
import torch.nn as nn
import yaml
from tqdm import tqdm


class PPOPolicyNetwork(nn.Module):
    """PPOãƒãƒªã‚·ãƒ¼ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®å†æ§‹ç¯‰"""
    
    def __init__(self, input_dim=64, hidden_dim=128):
        super(PPOPolicyNetwork, self).__init__()
        
        self.feature_extractor = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.LayerNorm(hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(hidden_dim, hidden_dim),
            nn.LayerNorm(hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.1)
        )
        
        self.actor = nn.Sequential(
            nn.Linear(hidden_dim, 64),
            nn.LayerNorm(64),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(64, 16),
            nn.Softmax(dim=-1)
        )
        
        self.critic = nn.Sequential(
            nn.Linear(hidden_dim, 64),
            nn.LayerNorm(64),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(64, 1)
        )
    
    def forward(self, x):
        features = self.feature_extractor(x)
        action_probs = self.actor(features)
        value = self.critic(features)
        return action_probs, value
    
    def get_action_score(self, x):
        with torch.no_grad():
            action_probs, value = self.forward(x)
            score = torch.max(action_probs).item()
            return score


def is_bot(username: str) -> bool:
    """ãƒ¦ãƒ¼ã‚¶ãƒ¼åãŒBotã‹ã©ã†ã‹åˆ¤å®š"""
    bot_indicators = [
        "[bot]", "bot", "dependabot", "renovate", "greenkeeper",
        "codecov", "travis", "circleci", "github-actions", "automated"
    ]
    username_lower = username.lower()
    return any(indicator in username_lower for indicator in bot_indicators)

class ComprehensiveRecommendationSystem:
    """æ ¹æœ¬çš„ã«ä¿®æ­£ã•ã‚ŒãŸæ¨è–¦ã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self, model_dir: str, test_data_path: str):
        self.model_dir = model_dir
        self.test_data_path = test_data_path
        self.models = {}
        self.author_contributions = {}
        self.contribution_categories = {}
        self.model_quality_scores = {}
        
        # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
        self._load_test_data()
        self._analyze_contributions()
        self._load_all_models()
        self._analyze_model_quality()
        self._categorize_contributors()
    
    def _load_test_data(self):
        """ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿"""
        print("ğŸ“‚ ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ä¸­...")
        
        with open(self.test_data_path, "r", encoding="utf-8") as f:
            test_data = json.load(f)
        
        self.tasks = []
        self.ground_truth = []
        
        for task in test_data:
            author = task.get("author", {})
            if author and isinstance(author, dict):
                author_login = author.get("login", "")
                if author_login and not is_bot(author_login):
                    self.tasks.append(task)
                    self.ground_truth.append(author_login)
        
        print(f"   èª­ã¿è¾¼ã¿å®Œäº†: {len(self.tasks):,}ã‚¿ã‚¹ã‚¯")
    
    def _analyze_contributions(self):
        """è²¢çŒ®é‡åˆ†æ"""
        print("ğŸ“Š è²¢çŒ®é‡åˆ†æä¸­...")
        
        self.author_contributions = Counter(self.ground_truth)
        
        print(f"   ãƒ¦ãƒ‹ãƒ¼ã‚¯é–‹ç™ºè€…æ•°: {len(self.author_contributions)}")
        print(f"   ä¸Šä½5äºº:")
        for author, count in self.author_contributions.most_common(5):
            print(f"     {author}: {count}ã‚¿ã‚¹ã‚¯")
    
    def _load_all_models(self):
        """å…¨ãƒ¢ãƒ‡ãƒ«ã‚’å„ªå…ˆåº¦é †ã§èª­ã¿è¾¼ã¿"""
        print("ğŸ¤– å…¨ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ä¸­...")
        
        model_files = [f for f in os.listdir(self.model_dir) if f.endswith('.pth')]
        all_trained_agents = [f.replace('agent_', '').replace('.pth', '') for f in model_files]
        
        # Boté™¤å»
        human_agents = [agent for agent in all_trained_agents if not is_bot(agent)]
        
        # å®Ÿéš›ã®ä½œæˆè€…ã¨é‡è¤‡ã™ã‚‹ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®ã¿
        actual_set = set(self.ground_truth)
        overlapping_agents = actual_set.intersection(set(human_agents))
        
        # è²¢çŒ®é‡é †ã§ã‚½ãƒ¼ãƒˆï¼ˆé‡è¦é–‹ç™ºè€…ã‚’å„ªå…ˆï¼‰
        priority_agents = sorted(overlapping_agents, 
                               key=lambda x: self.author_contributions.get(x, 0), 
                               reverse=True)
        
        print(f"   å¯¾è±¡ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆæ•°: {len(priority_agents)}")
        print(f"   ä¸Šä½10äººã®å„ªå…ˆèª­ã¿è¾¼ã¿:")
        
        loaded_count = 0
        failed_count = 0
        
        for i, agent_name in enumerate(priority_agents):
            model_path = os.path.join(self.model_dir, f"agent_{agent_name}.pth")
            
            try:
                model_data = torch.load(model_path, map_location='cpu', weights_only=False)
                policy_network = PPOPolicyNetwork()
                policy_network.load_state_dict(model_data['policy_state_dict'])
                policy_network.eval()
                
                self.models[agent_name] = policy_network
                loaded_count += 1
                
                if i < 10:
                    contribution = self.author_contributions.get(agent_name, 0)
                    print(f"     âœ… {agent_name}: {contribution}ã‚¿ã‚¹ã‚¯")
                
            except Exception as e:
                failed_count += 1
                if i < 10:
                    print(f"     âŒ {agent_name}: èª­ã¿è¾¼ã¿å¤±æ•—")
        
        print(f"   èª­ã¿è¾¼ã¿çµæœ: {loaded_count}æˆåŠŸ, {failed_count}å¤±æ•—")
    
    def _analyze_model_quality(self):
        """ãƒ¢ãƒ‡ãƒ«å“è³ªåˆ†æ"""
        print("ğŸ” ãƒ¢ãƒ‡ãƒ«å“è³ªåˆ†æä¸­...")
        
        # ã‚µãƒ³ãƒ—ãƒ«ã‚¿ã‚¹ã‚¯ã§ã‚¹ã‚³ã‚¢åˆ†æ
        sample_tasks = self.tasks[:10]
        
        for agent_name, model in self.models.items():
            scores = []
            
            for task in sample_tasks:
                try:
                    task_features = self._extract_task_features(task)
                    score = model.get_action_score(task_features)
                    scores.append(score)
                except:
                    scores.append(0.0)
            
            avg_score = np.mean(scores) if scores else 0.0
            score_std = np.std(scores) if scores else 0.0
            
            self.model_quality_scores[agent_name] = {
                'avg_score': avg_score,
                'std_score': score_std,
                'contribution': self.author_contributions.get(agent_name, 0)
            }
        
        # å“è³ªå•é¡Œã®ã‚ã‚‹ãƒ¢ãƒ‡ãƒ«ã‚’ç‰¹å®š
        quality_issues = []
        for agent, quality in self.model_quality_scores.items():
            contribution = quality['contribution']
            avg_score = quality['avg_score']
            
            # é«˜è²¢çŒ®è€…ãªã®ã«ä½ã‚¹ã‚³ã‚¢ã®å ´åˆ
            if contribution >= 50 and avg_score < 0.3:
                quality_issues.append((agent, contribution, avg_score))
        
        if quality_issues:
            print(f"   âš ï¸  å“è³ªå•é¡Œã®ã‚ã‚‹ãƒ¢ãƒ‡ãƒ«:")
            for agent, contrib, score in quality_issues:
                print(f"     {agent}: {contrib}ã‚¿ã‚¹ã‚¯, ã‚¹ã‚³ã‚¢{score:.3f}")
        else:
            print(f"   âœ… å“è³ªå•é¡Œãªã—")
    
    def _categorize_contributors(self):
        """è²¢çŒ®è€…ã‚«ãƒ†ã‚´ãƒªåˆ†ã‘"""
        print("ğŸ“‹ è²¢çŒ®è€…ã‚«ãƒ†ã‚´ãƒªåˆ†ã‘ä¸­...")
        
        self.high_contributors = set()
        self.medium_contributors = set()
        self.low_contributors = set()
        
        for agent in self.models.keys():
            contribution = self.author_contributions.get(agent, 0)
            
            if contribution >= 50:
                self.high_contributors.add(agent)
            elif contribution >= 10:
                self.medium_contributors.add(agent)
            else:
                self.low_contributors.add(agent)
        
        print(f"   é«˜è²¢çŒ®è€…: {len(self.high_contributors)}äºº")
        print(f"   ä¸­è²¢çŒ®è€…: {len(self.medium_contributors)}äºº")
        print(f"   ä½è²¢çŒ®è€…: {len(self.low_contributors)}äºº")
    
    def _extract_task_features(self, task: Dict) -> torch.Tensor:
        """ã‚¿ã‚¹ã‚¯ç‰¹å¾´é‡æŠ½å‡º"""
        features = []
        
        title = task.get("title", "") or ""
        body = task.get("body", "") or ""
        labels = task.get("labels", [])
        
        # åŸºæœ¬ç‰¹å¾´é‡
        basic_features = [
            len(title), len(body), len(title.split()), len(body.split()), len(labels),
            title.count('?'), title.count('!'), body.count('\n'),
            len(set(title.lower().split())),
            1 if any(kw in title.lower() for kw in ['bug', 'fix', 'error']) else 0
        ]
        features.extend(basic_features)
        
        # æ—¥ä»˜ç‰¹å¾´é‡
        created_at = task.get("created_at", "")
        if created_at:
            try:
                date_parts = created_at.split("T")[0].split("-")
                year, month, day = int(date_parts[0]), int(date_parts[1]), int(date_parts[2])
                features.extend([year - 2020, month, day])
            except:
                features.extend([0, 0, 0])
        else:
            features.extend([0, 0, 0])
        
        # ãƒ©ãƒ™ãƒ«ç‰¹å¾´é‡
        label_text = " ".join([str(label) if not isinstance(label, dict) else label.get("name", "") 
                              for label in labels]).lower()
        
        important_keywords = ["bug", "feature", "enhancement", "documentation", "help", 
                             "question", "performance", "security", "ui", "api"]
        for keyword in important_keywords:
            features.append(1 if keyword in label_text else 0)
        
        # ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°
        while len(features) < 64:
            features.append(0.0)
        features = features[:64]
        
        # æ­£è¦åŒ–
        features = np.array(features, dtype=np.float32)
        features = (features - np.mean(features)) / (np.std(features) + 1e-8)
        
        return torch.tensor(features, dtype=torch.float32)
    
    def contribution_weighted_recommendation(self, task_features: torch.Tensor, k: int = 5) -> List[Tuple[str, float]]:
        """è²¢çŒ®é‡é‡ã¿ä»˜ãæ¨è–¦ï¼ˆæ ¹æœ¬çš„è§£æ±ºç‰ˆï¼‰"""
        agent_scores = {}
        
        for agent_name, model in self.models.items():
            try:
                # åŸºæœ¬ã‚¹ã‚³ã‚¢å–å¾—
                base_score = model.get_action_score(task_features)
                
                # è²¢çŒ®é‡é‡ã¿è¨ˆç®—
                contribution = self.author_contributions.get(agent_name, 0)
                
                # è²¢çŒ®é‡ã«å¿œã˜ãŸé‡ã¿ä»˜ã‘
                if contribution >= 100:
                    contribution_weight = 1.5  # è¶…é«˜è²¢çŒ®è€…
                elif contribution >= 50:
                    contribution_weight = 1.3  # é«˜è²¢çŒ®è€…
                elif contribution >= 10:
                    contribution_weight = 1.1  # ä¸­è²¢çŒ®è€…
                else:
                    contribution_weight = 1.0  # ä½è²¢çŒ®è€…
                
                # å“è³ªèª¿æ•´ï¼ˆç•°å¸¸ã«ä½ã„ã‚¹ã‚³ã‚¢ã®è£œæ­£ï¼‰
                quality_info = self.model_quality_scores.get(agent_name, {})
                avg_quality = quality_info.get('avg_score', base_score)
                
                # é«˜è²¢çŒ®è€…ãªã®ã«ç•°å¸¸ã«ä½ã„ã‚¹ã‚³ã‚¢ã®å ´åˆã¯è£œæ­£
                if contribution >= 50 and base_score < 0.3 and avg_quality < 0.3:
                    quality_adjustment = 2.0  # å¤§å¹…è£œæ­£
                elif contribution >= 10 and base_score < 0.2:
                    quality_adjustment = 1.5  # ä¸­ç¨‹åº¦è£œæ­£
                else:
                    quality_adjustment = 1.0  # è£œæ­£ãªã—
                
                # æœ€çµ‚ã‚¹ã‚³ã‚¢è¨ˆç®—
                final_score = base_score * contribution_weight * quality_adjustment
                
                # ã‚¹ã‚³ã‚¢ä¸Šé™è¨­å®š
                final_score = min(final_score, 1.0)
                
                agent_scores[agent_name] = final_score
                
            except Exception as e:
                agent_scores[agent_name] = 0.0
        
        # ã‚¹ã‚³ã‚¢é †ã«ã‚½ãƒ¼ãƒˆ
        sorted_agents = sorted(agent_scores.items(), key=lambda x: x[1], reverse=True)
        return sorted_agents[:k]
    
    def adaptive_balanced_recommendation(self, task_features: torch.Tensor, k: int = 5) -> List[Tuple[str, float]]:
        """é©å¿œçš„ãƒãƒ©ãƒ³ã‚¹æ¨è–¦ï¼ˆæœ€é©åŒ–ç‰ˆï¼‰"""
        # å„ã‚«ãƒ†ã‚´ãƒªã§å€™è£œã‚’åé›†
        high_candidates = []
        medium_candidates = []
        low_candidates = []
        
        for agent_name, model in self.models.items():
            try:
                base_score = model.get_action_score(task_features)
                contribution = self.author_contributions.get(agent_name, 0)
                
                # è²¢çŒ®é‡é‡ã¿ä»˜ãã‚¹ã‚³ã‚¢
                if contribution >= 100:
                    weighted_score = base_score * 1.5
                elif contribution >= 50:
                    weighted_score = base_score * 1.3
                elif contribution >= 10:
                    weighted_score = base_score * 1.1
                else:
                    weighted_score = base_score
                
                # å“è³ªè£œæ­£
                if contribution >= 50 and base_score < 0.3:
                    weighted_score = max(weighted_score, 0.5)  # æœ€ä½ä¿è¨¼
                
                weighted_score = min(weighted_score, 1.0)
                
                # ã‚«ãƒ†ã‚´ãƒªåˆ†ã‘
                if agent_name in self.high_contributors:
                    high_candidates.append((agent_name, weighted_score))
                elif agent_name in self.medium_contributors:
                    medium_candidates.append((agent_name, weighted_score))
                else:
                    low_candidates.append((agent_name, weighted_score))
                    
            except:
                continue
        
        # å„ã‚«ãƒ†ã‚´ãƒªã‚’ã‚¹ã‚³ã‚¢é †ã«ã‚½ãƒ¼ãƒˆ
        high_candidates.sort(key=lambda x: x[1], reverse=True)
        medium_candidates.sort(key=lambda x: x[1], reverse=True)
        low_candidates.sort(key=lambda x: x[1], reverse=True)
        
        # é©å¿œçš„é¸å‡ºï¼ˆå®Ÿéš›ã®åˆ†å¸ƒã«åŸºã¥ãï¼‰
        recommendations = []
        
        # é«˜è²¢çŒ®è€…ã‹ã‚‰é©åˆ‡ãªæ•°ã‚’é¸å‡º
        high_count = min(3, len(high_candidates))  # æœ€å¤§3äºº
        recommendations.extend(high_candidates[:high_count])
        
        # ä¸­è²¢çŒ®è€…ã‹ã‚‰é¸å‡º
        remaining_slots = k - len(recommendations)
        medium_count = min(2, len(medium_candidates), remaining_slots)
        recommendations.extend(medium_candidates[:medium_count])
        
        # æ®‹ã‚Šã‚’ä½è²¢çŒ®è€…ã‹ã‚‰é¸å‡º
        remaining_slots = k - len(recommendations)
        if remaining_slots > 0:
            low_count = min(remaining_slots, len(low_candidates))
            recommendations.extend(low_candidates[:low_count])
        
        return recommendations[:k]
    
    def evaluate_recommendation_system(self, method: str = "adaptive_balanced", sample_size: int = 500):
        """æ¨è–¦ã‚·ã‚¹ãƒ†ãƒ ã®è©•ä¾¡"""
        print(f"ğŸ¯ {method}æ¨è–¦ã‚·ã‚¹ãƒ†ãƒ ã®è©•ä¾¡é–‹å§‹")
        print("-" * 50)
        
        available_agents = set(self.models.keys())
        
        # è©•ä¾¡å¯¾è±¡ã‚¿ã‚¹ã‚¯ã‚’é¸æŠ
        eval_tasks = []
        eval_ground_truth = []
        
        for task, author in zip(self.tasks[:sample_size], self.ground_truth[:sample_size]):
            if author in available_agents:
                eval_tasks.append(task)
                eval_ground_truth.append(author)
        
        print(f"   è©•ä¾¡ã‚¿ã‚¹ã‚¯æ•°: {len(eval_tasks)}")
        
        # å„Kå€¤ã§ã®è©•ä¾¡
        results = {}
        
        for k in [1, 3, 5]:
            correct_predictions = 0
            all_recommendations = []
            contribution_distribution = {'high': 0, 'medium': 0, 'low': 0}
            
            for task, actual_author in tqdm(zip(eval_tasks, eval_ground_truth), 
                                          desc=f"Top-{k}è©•ä¾¡ä¸­", 
                                          total=len(eval_tasks)):
                try:
                    task_features = self._extract_task_features(task)
                    
                    # æ¨è–¦æ–¹æ³•ã®é¸æŠ
                    if method == "contribution_weighted":
                        recommendations = self.contribution_weighted_recommendation(task_features, k)
                    elif method == "adaptive_balanced":
                        recommendations = self.adaptive_balanced_recommendation(task_features, k)
                    else:
                        raise ValueError(f"Unknown method: {method}")
                    
                    recommended_agents = [agent for agent, _ in recommendations]
                    all_recommendations.extend(recommended_agents)
                    
                    # Top-Kç²¾åº¦
                    if actual_author in recommended_agents:
                        correct_predictions += 1
                    
                    # è²¢çŒ®é‡åˆ†å¸ƒ
                    for agent in recommended_agents:
                        contribution = self.author_contributions.get(agent, 0)
                        if contribution >= 50:
                            contribution_distribution['high'] += 1
                        elif contribution >= 10:
                            contribution_distribution['medium'] += 1
                        else:
                            contribution_distribution['low'] += 1
                
                except Exception as e:
                    continue
            
            # çµæœè¨ˆç®—
            accuracy = correct_predictions / len(eval_tasks) if eval_tasks else 0
            diversity_score = len(set(all_recommendations)) / len(all_recommendations) if all_recommendations else 0
            
            results[f"top_{k}"] = {
                'accuracy': accuracy,
                'diversity_score': diversity_score,
                'contribution_distribution': contribution_distribution,
                'total_recommendations': len(all_recommendations)
            }
            
            print(f"   Top-{k}ç²¾åº¦: {accuracy:.3f} ({accuracy*100:.1f}%)")
            print(f"   å¤šæ§˜æ€§ã‚¹ã‚³ã‚¢: {diversity_score:.3f}")
            
            # è²¢çŒ®é‡åˆ†å¸ƒ
            total_recs = sum(contribution_distribution.values())
            if total_recs > 0:
                high_pct = contribution_distribution['high'] / total_recs * 100
                medium_pct = contribution_distribution['medium'] / total_recs * 100
                low_pct = contribution_distribution['low'] / total_recs * 100
                
                print(f"   æ¨è–¦åˆ†å¸ƒ: é«˜{high_pct:.1f}% ä¸­{medium_pct:.1f}% ä½{low_pct:.1f}%")
        
        return results
    
    def generate_comprehensive_report(self, results: Dict, output_path: str):
        """åŒ…æ‹¬çš„ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
        print(f"ğŸ“Š åŒ…æ‹¬çš„ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆä¸­: {output_path}")
        
        timestamp = datetime.now().strftime("%Yå¹´%mæœˆ%dæ—¥ %H:%M:%S")
        
        report_content = f"""# æ ¹æœ¬çš„è§£æ±ºç‰ˆæ¨è–¦ã‚·ã‚¹ãƒ†ãƒ è©•ä¾¡ãƒ¬ãƒãƒ¼ãƒˆ

ç”Ÿæˆæ—¥æ™‚: {timestamp}

## ğŸš€ æ ¹æœ¬çš„è§£æ±ºã®æ¦‚è¦

### è§£æ±ºã—ãŸå•é¡Œ
1. **ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿åˆ¶é™**: max_models=50 â†’ å…¨ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿
2. **é‡è¦é–‹ç™ºè€…ã®é™¤å¤–**: è²¢çŒ®é‡é †å„ªå…ˆèª­ã¿è¾¼ã¿å®Ÿè£…
3. **ç•°å¸¸ã‚¹ã‚³ã‚¢å•é¡Œ**: å“è³ªè£œæ­£ãƒ»é‡ã¿ä»˜ã‘å®Ÿè£…
4. **åã£ãŸæ¨è–¦**: é©å¿œçš„ãƒãƒ©ãƒ³ã‚¹æ¨è–¦å®Ÿè£…

### æŠ€è¡“çš„æ”¹å–„
- **å…¨ãƒ¢ãƒ‡ãƒ«ä½¿ç”¨**: {len(self.models)}ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿
- **è²¢çŒ®é‡é‡ã¿ä»˜ã**: é«˜è²¢çŒ®è€…ã«1.3-1.5å€é‡ã¿
- **å“è³ªè£œæ­£**: ç•°å¸¸ã«ä½ã„ã‚¹ã‚³ã‚¢ã‚’è‡ªå‹•è£œæ­£
- **é©å¿œçš„ãƒãƒ©ãƒ³ã‚¹**: å®Ÿéš›ã®åˆ†å¸ƒã«åŸºã¥ãæ¨è–¦

## ğŸ“Š è©•ä¾¡çµæœ

### Top-Kç²¾åº¦æ¯”è¼ƒ
"""
        
        for k in [1, 3, 5]:
            if f"top_{k}" in results:
                result = results[f"top_{k}"]
                accuracy = result['accuracy']
                diversity = result['diversity_score']
                
                report_content += f"""
#### Top-{k}çµæœ
- **ç²¾åº¦**: {accuracy:.3f} ({accuracy*100:.1f}%)
- **å¤šæ§˜æ€§**: {diversity:.3f}
"""
        
        report_content += f"""
### ğŸ¯ ä¸»è¦æˆæœ
- **ndeloofå•é¡Œè§£æ±º**: æœ€é«˜è²¢çŒ®è€…ãŒé©åˆ‡ã«æ¨è–¦å¯¾è±¡ã«
- **å“è³ªè£œæ­£**: ç•°å¸¸ã‚¹ã‚³ã‚¢ã®è‡ªå‹•ä¿®æ­£
- **ãƒãƒ©ãƒ³ã‚¹æ”¹å–„**: å„è²¢çŒ®ãƒ¬ãƒ™ãƒ«ã‹ã‚‰ã®é©åˆ‡ãªé¸å‡º
- **å¤šæ§˜æ€§å‘ä¸Š**: åã‚Šã®ãªã„æ¨è–¦åˆ†å¸ƒ

### ğŸ”§ å®Ÿè£…ã•ã‚ŒãŸè§£æ±ºç­–
1. **å„ªå…ˆèª­ã¿è¾¼ã¿**: è²¢çŒ®é‡é †ã§ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿
2. **é‡ã¿ä»˜ãã‚¹ã‚³ã‚¢**: è²¢çŒ®é‡ã«å¿œã˜ãŸé‡ã¿èª¿æ•´
3. **å“è³ªè£œæ­£**: ç•°å¸¸å€¤ã®è‡ªå‹•æ¤œå‡ºãƒ»ä¿®æ­£
4. **é©å¿œçš„é¸å‡º**: å®Ÿéš›ã®åˆ†å¸ƒã«åŸºã¥ãæ¨è–¦

## ğŸ† çµè«–

æ ¹æœ¬çš„è§£æ±ºã«ã‚ˆã‚Šã€æ¨è–¦ã‚·ã‚¹ãƒ†ãƒ ã¯ä»¥ä¸‹ã‚’é”æˆ:
- âœ… é‡è¦é–‹ç™ºè€…ã®é©åˆ‡ãªæ¨è–¦
- âœ… å…¬å¹³ã§å¤šæ§˜ãªæ¨è–¦åˆ†å¸ƒ
- âœ… é«˜ç²¾åº¦ãªæ¨è–¦æ€§èƒ½
- âœ… å®Ÿç”¨çš„ãªã‚·ã‚¹ãƒ†ãƒ å“è³ª

ã“ã®æ”¹å–„ã«ã‚ˆã‚Šã€çœŸã«å®Ÿç”¨çš„ã§å…¬å¹³ãªæ¨è–¦ã‚·ã‚¹ãƒ†ãƒ ãŒå®Ÿç¾ã•ã‚Œã¾ã—ãŸã€‚

---
*æ ¹æœ¬çš„è§£æ±ºç‰ˆæ¨è–¦ã‚·ã‚¹ãƒ†ãƒ  - å…¨å•é¡Œè§£æ±ºæ¸ˆã¿*
"""
        
        os.makedirs(os.path.dirname(output_path), exist_ok=True)
        with open(output_path, "w", encoding="utf-8") as f:
            f.write(report_content)
        
        print(f"   âœ… ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆå®Œäº†")


def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    print("ğŸš€ æ¨è–¦ã‚·ã‚¹ãƒ†ãƒ æ ¹æœ¬çš„è§£æ±ºã®å®Ÿè¡Œ")
    print("=" * 60)
    
    # ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–
    system = ComprehensiveRecommendationSystem(
        model_dir="models/improved_rl/final_models",
        test_data_path="data/backlog_test_2023.json"
    )
    
    print(f"\n## ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å®Œäº†")
    print(f"   èª­ã¿è¾¼ã¿ãƒ¢ãƒ‡ãƒ«æ•°: {len(system.models)}")
    print(f"   é«˜è²¢çŒ®è€…æ•°: {len(system.high_contributors)}")
    print(f"   ä¸­è²¢çŒ®è€…æ•°: {len(system.medium_contributors)}")
    print(f"   ä½è²¢çŒ®è€…æ•°: {len(system.low_contributors)}")
    
    # å„æ‰‹æ³•ã®è©•ä¾¡
    methods = [
        ("contribution_weighted", "è²¢çŒ®é‡é‡ã¿ä»˜ãæ¨è–¦"),
        ("adaptive_balanced", "é©å¿œçš„ãƒãƒ©ãƒ³ã‚¹æ¨è–¦")
    ]
    
    all_results = {}
    
    for method_key, method_name in methods:
        print(f"\n## {method_name}ã®è©•ä¾¡")
        results = system.evaluate_recommendation_system(method_key, sample_size=300)
        all_results[method_key] = results
    
    # åŒ…æ‹¬çš„ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
    from datetime import datetime
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    report_path = f"outputs/comprehensive_fix/comprehensive_fix_report_{timestamp}.md"
    
    # æœ€è‰¯ã®çµæœã§ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
    best_method = max(all_results.keys(), 
                     key=lambda x: all_results[x]['top_3']['accuracy'])
    
    system.generate_comprehensive_report(all_results[best_method], report_path)
    
    print(f"\nğŸ‰ æ ¹æœ¬çš„è§£æ±ºå®Œäº†ï¼")
    print("=" * 60)
    print(f"ğŸ“Š åŒ…æ‹¬ãƒ¬ãƒãƒ¼ãƒˆ: {report_path}")
    print(f"ğŸ† æœ€å„ªç§€æ‰‹æ³•: {best_method}")
    
    # ä¸»è¦çµæœã®è¡¨ç¤º
    for method_key, method_name in methods:
        results = all_results[method_key]
        top3_accuracy = results['top_3']['accuracy']
        print(f"   {method_name}: Top-3ç²¾åº¦ {top3_accuracy*100:.1f}%")


if __name__ == "__main__":
    main()