# 改良されたPPO訓練設定
# PPO 0%精度問題の解決を目指した設定

# --- 環境設定 ---
env:
  backlog_path: "data/backlog_training_2022.json"
  dev_profiles_path: "configs/dev_profiles_training_2022.yaml"
  expert_trajectories_path: "data/expert_trajectories_bot_excluded.pkl"
  simulation:
    time_step_hours: 4 # より細かい時間刻み
    max_days: 730 # 訓練期間を2年に延長

  # 改良された報酬システム
  reward_system:
    type: "improved"
    components:
      task_completion: 1.0
      skill_match: 0.4
      workload_balance: 0.3
      collaboration: 0.2
      learning: 0.15
    normalization:
      enabled: true
      window_size: 1000
      clip_range: [-2.0, 2.0]

  # 行動空間縮小
  action_space:
    type: "reduced"
    max_candidates: 15 # 最大候補数を15に制限
    min_candidates: 5 # 最小候補数
    filtering:
      skill_match: 0.4
      activity_level: 0.3
      workload: 0.2
      collaboration: 0.1

  # 改良された観測空間
  observation_space:
    type: "improved"
    total_dim: 104 # 固定次元数
    components:
      agent_features: 16
      task_context: 32
      environment_state: 16
      temporal_features: 8
      graph_embeddings: 32
    normalization:
      enabled: true
      method: "standard"

  # ボット除外設定
  bot_filtering:
    enabled: true
    bot_patterns: ["[bot]", "stale[bot]", "dependabot[bot]", "codecov[bot]"]
    exclude_from_recommendations: true
    exclude_from_training: true

num_developers: 50 # 開発者数を50に削減（学習を容易に）

# --- 時系列分割設定 ---
temporal_split:
  irl_end_date: "2021-12-31"
  train_start_date: "2022-01-01"
  train_end_date: "2022-12-31"
  test_start_date: "2023-01-01"

# --- 特徴量設定 ---
features:
  recent_activity_window_days: 30
  skill_categories:
    - "python"
    - "javascript"
    - "debugging"
    - "documentation"
    - "testing"
  complexity_factors:
    - "comments_count"
    - "labels_count"
    - "participants_count"

# --- IRL設定（ボット除外版データを使用） ---
irl:
  expert_path: "data/expert_trajectories_bot_excluded.pkl"
  output_weights_path: "data/learned_weights_bot_excluded.npy"
  use_existing_weights: false # 新しいボット除外データで再学習

# --- 改良されたPPO設定 ---
rl:
  # 大幅に増加した訓練ステップ数
  total_timesteps: 500000 # 50万ステップ（10倍増）

  # 学習率スケジューリング
  learning_rate: 0.0003 # 初期学習率を上げる
  lr_schedule: "linear" # 線形減衰
  lr_final: 0.00001 # 最終学習率

  # PPOハイパーパラメータ
  gamma: 0.995 # 割引率を上げる（長期報酬重視）
  gae_lambda: 0.98 # GAEパラメータ
  eps_clip: 0.15 # クリッピング範囲を狭める
  k_epochs: 6 # 更新エポック数を増やす

  # バッチサイズとロールアウト
  rollout_len: 256 # ロールアウト長を2倍に
  batch_size: 64 # バッチサイズ
  minibatch_size: 16 # ミニバッチサイズ

  # 価値関数とエントロピー
  vf_coef: 0.25 # 価値関数の重み
  ent_coef: 0.02 # エントロピーボーナス（探索促進）
  ent_decay: 0.995 # エントロピー減衰

  # 勾配クリッピング
  max_grad_norm: 0.5 # 勾配ノルムクリッピング

  # 早期停止
  early_stopping:
    enabled: true
    patience: 50 # 50回更新で改善なしなら停止
    min_improvement: 0.01 # 最小改善幅

  # 学習率適応
  adaptive_lr:
    enabled: true
    factor: 0.8 # 学習率削減係数
    patience: 20 # 改善なしの回数
    min_lr: 0.00001 # 最小学習率

  # カリキュラム学習
  curriculum:
    enabled: true
    stages:
      - name: "easy"
        timesteps: 100000
        num_developers: 20 # 少数の開発者から開始
        max_candidates: 8 # 少ない候補数
      - name: "medium"
        timesteps: 200000
        num_developers: 35 # 徐々に増加
        max_candidates: 12
      - name: "hard"
        timesteps: 200000
        num_developers: 50 # 最終的な開発者数
        max_candidates: 15

  # 正則化
  regularization:
    l2_coef: 0.0001 # L2正則化
    dropout: 0.1 # ドロップアウト

  # モデル保存
  save_freq: 10000 # 1万ステップごとに保存
  output_model_dir: "models/improved_ppo/"

  # ログ設定
  logging:
    tensorboard: true
    log_freq: 1000 # 1000ステップごとにログ
    eval_freq: 5000 # 5000ステップごとに評価

# --- ネットワークアーキテクチャ ---
network:
  # より深いネットワーク
  actor:
    hidden_layers: [128, 128, 64] # 3層の隠れ層
    activation: "tanh"
    dropout: 0.1
    batch_norm: true

  critic:
    hidden_layers: [128, 128, 64]
    activation: "tanh"
    dropout: 0.1
    batch_norm: true

  # 特徴量抽出器
  feature_extractor:
    type: "mlp"
    hidden_dim: 64
    num_layers: 2

# --- 評価設定 ---
evaluation:
  eval_episodes: 100 # 評価エピソード数
  eval_freq: 10000 # 評価頻度
  success_threshold: 0.1 # 成功判定閾値（10%精度）

  # メトリクス
  metrics:
    - "top_1_accuracy"
    - "top_3_accuracy"
    - "top_5_accuracy"
    - "average_reward"
    - "episode_length"
    - "success_rate"

# --- デバッグ設定 ---
debug:
  enabled: true
  log_level: "INFO"
  save_trajectories: true # 軌跡を保存
  plot_learning_curves: true # 学習曲線をプロット

  # 詳細ログ
  detailed_logging:
    reward_breakdown: true # 報酬の内訳をログ
    action_distribution: true # 行動分布をログ
    observation_stats: true # 観測統計をログ

# --- 実験設定 ---
experiment:
  name: "improved_ppo_v1"
  description: "PPO 0%精度問題の解決を目指した改良版"
  tags: ["ppo", "improved", "reward_shaping", "action_space_reduction"]

  # 再現性
  seed: 42
  deterministic: true

  # 並列化
  num_workers: 4 # 並列ワーカー数

  # チェックポイント
  checkpoint:
    enabled: true
    save_best: true
    save_last: true
    save_freq: 50000
