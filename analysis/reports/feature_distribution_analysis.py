#!/usr/bin/env python3
"""
ç‰¹å¾´é‡åˆ†å¸ƒåˆ†æã‚¹ã‚¯ãƒªãƒ—ãƒˆ
========================

IRLã§ä½¿ç”¨ã•ã‚Œã‚‹å…¨ç‰¹å¾´é‡ã®å®Ÿéš›ã®ãƒ‡ãƒ¼ã‚¿åˆ†å¸ƒã‚’èª¿ã¹ã€
çµ±è¨ˆæƒ…å ±ã¨å¯è¦–åŒ–ã‚’æä¾›ã—ã¾ã™ã€‚
"""

import sys
import warnings
from datetime import datetime
from pathlib import Path

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import seaborn as sns

warnings.filterwarnings("ignore")


def get_feature_japanese_names():
    """ç‰¹å¾´é‡åã¨æ—¥æœ¬èªèª¬æ˜ã®ãƒãƒƒãƒ”ãƒ³ã‚°è¾æ›¸ã‚’è¿”ã™"""
    return {
        # ã‚¿ã‚¹ã‚¯ç‰¹å¾´é‡
        "task_days_since_last_activity": "ã‚¿ã‚¹ã‚¯æœ€çµ‚æ´»å‹•ã‹ã‚‰ã®æ—¥æ•°",
        "task_discussion_activity": "ã‚¿ã‚¹ã‚¯è­°è«–æ´»å‹•åº¦",
        "task_text_length": "ã‚¿ã‚¹ã‚¯ãƒ†ã‚­ã‚¹ãƒˆé•·",
        "task_code_block_count": "ã‚¿ã‚¹ã‚¯ã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯æ•°",
        "task_label_bug": "ã‚¿ã‚¹ã‚¯ãƒ©ãƒ™ãƒ«: ãƒã‚°",
        "task_label_enhancement": "ã‚¿ã‚¹ã‚¯ãƒ©ãƒ™ãƒ«: æ©Ÿèƒ½å¼·åŒ–",
        "task_label_documentation": "ã‚¿ã‚¹ã‚¯ãƒ©ãƒ™ãƒ«: ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ",
        "task_label_question": "ã‚¿ã‚¹ã‚¯ãƒ©ãƒ™ãƒ«: è³ªå•",
        "task_label_help wanted": "ã‚¿ã‚¹ã‚¯ãƒ©ãƒ™ãƒ«: ãƒ˜ãƒ«ãƒ—å‹Ÿé›†",
        "task_priority_score": "ã‚¿ã‚¹ã‚¯å„ªå…ˆåº¦ã‚¹ã‚³ã‚¢",
        "task_urgency_indicator": "ã‚¿ã‚¹ã‚¯ç·Šæ€¥åº¦æŒ‡æ¨™",
        "task_complexity_estimate": "ã‚¿ã‚¹ã‚¯è¤‡é›‘åº¦æ¨å®š",
        "task_comment_count": "ã‚¿ã‚¹ã‚¯ã‚³ãƒ¡ãƒ³ãƒˆæ•°",
        "task_participant_count": "ã‚¿ã‚¹ã‚¯å‚åŠ è€…æ•°",
        "task_file_count": "ã‚¿ã‚¹ã‚¯é–¢é€£ãƒ•ã‚¡ã‚¤ãƒ«æ•°",
        "task_line_count": "ã‚¿ã‚¹ã‚¯é–¢é€£è¡Œæ•°",
        "task_branch_age": "ã‚¿ã‚¹ã‚¯ãƒ–ãƒ©ãƒ³ãƒçµŒéæ—¥æ•°",
        "task_commit_frequency": "ã‚¿ã‚¹ã‚¯ã‚³ãƒŸãƒƒãƒˆé »åº¦",
        "task_test_coverage": "ã‚¿ã‚¹ã‚¯ãƒ†ã‚¹ãƒˆã‚«ãƒãƒ¬ãƒƒã‚¸",
        "task_documentation_quality": "ã‚¿ã‚¹ã‚¯ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå“è³ª",
        # é–‹ç™ºè€…ç‰¹å¾´é‡
        "dev_recent_activity_count": "é–‹ç™ºè€…æœ€è¿‘æ´»å‹•æ•°",
        "dev_current_workload": "é–‹ç™ºè€…ç¾åœ¨ä½œæ¥­è² è·",
        "dev_total_lines_changed": "é–‹ç™ºè€…ç·å¤‰æ›´è¡Œæ•°",
        "dev_collaboration_network_size": "é–‹ç™ºè€…å”åŠ›ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯è¦æ¨¡",
        "dev_comment_interactions": "é–‹ç™ºè€…ã‚³ãƒ¡ãƒ³ãƒˆç›¸äº’ä½œç”¨",
        "dev_cross_issue_activity": "é–‹ç™ºè€…èª²é¡Œæ¨ªæ–­æ´»å‹•",
        "dev_expertise_score": "é–‹ç™ºè€…å°‚é–€æ€§ã‚¹ã‚³ã‚¢",
        "dev_reputation": "é–‹ç™ºè€…è©•ä¾¡",
        "dev_response_time": "é–‹ç™ºè€…å¿œç­”æ™‚é–“",
        "dev_code_quality": "é–‹ç™ºè€…ã‚³ãƒ¼ãƒ‰å“è³ª",
        "dev_test_coverage": "é–‹ç™ºè€…ãƒ†ã‚¹ãƒˆã‚«ãƒãƒ¬ãƒƒã‚¸",
        "dev_documentation_score": "é–‹ç™ºè€…ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚¹ã‚³ã‚¢",
        "dev_leadership_score": "é–‹ç™ºè€…ãƒªãƒ¼ãƒ€ãƒ¼ã‚·ãƒƒãƒ—ã‚¹ã‚³ã‚¢",
        "dev_mentoring_activity": "é–‹ç™ºè€…ãƒ¡ãƒ³ã‚¿ãƒªãƒ³ã‚°æ´»å‹•",
        "dev_innovation_index": "é–‹ç™ºè€…ã‚¤ãƒãƒ™ãƒ¼ã‚·ãƒ§ãƒ³æŒ‡æ¨™",
        # ãƒãƒƒãƒãƒ³ã‚°ç‰¹å¾´é‡
        "match_collaborated_with_task_author": "ãƒãƒƒãƒãƒ³ã‚°: ã‚¿ã‚¹ã‚¯ä½œæˆè€…ã¨ã®å”åŠ›çµŒé¨“",
        "match_collaborator_overlap_count": "ãƒãƒƒãƒãƒ³ã‚°: å”åŠ›è€…é‡è¤‡æ•°",
        "match_has_prior_collaboration": "ãƒãƒƒãƒãƒ³ã‚°: éå»ã®å”åŠ›å®Ÿç¸¾",
        "match_skill_intersection_count": "ãƒãƒƒãƒãƒ³ã‚°: ã‚¹ã‚­ãƒ«äº¤å·®æ•°",
        "match_file_experience_count": "ãƒãƒƒãƒãƒ³ã‚°: ãƒ•ã‚¡ã‚¤ãƒ«çµŒé¨“æ•°",
        "match_affinity_for_bug": "ãƒãƒƒãƒãƒ³ã‚°: ãƒã‚°å¯¾å¿œè¦ªå’Œæ€§",
        "match_affinity_for_enhancement": "ãƒãƒƒãƒãƒ³ã‚°: æ©Ÿèƒ½å¼·åŒ–è¦ªå’Œæ€§",
        "match_affinity_for_documentation": "ãƒãƒƒãƒãƒ³ã‚°: ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆè¦ªå’Œæ€§",
        "match_affinity_for_question": "ãƒãƒƒãƒãƒ³ã‚°: è³ªå•å¯¾å¿œè¦ªå’Œæ€§",
        "match_affinity_for_help wanted": "ãƒãƒƒãƒãƒ³ã‚°: ãƒ˜ãƒ«ãƒ—å¯¾å¿œè¦ªå’Œæ€§",
        # GATçµ±è¨ˆç‰¹å¾´é‡
        "gat_similarity": "GATé¡ä¼¼åº¦",
        "gat_dev_expertise": "GATé–‹ç™ºè€…å°‚é–€æ€§",
        "gat_task_popularity": "GATã‚¿ã‚¹ã‚¯äººæ°—åº¦",
        "gat_collaboration_strength": "GATå”åŠ›å¼·åº¦",
        "gat_network_centrality": "GATãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ä¸­å¿ƒæ€§",
    }


def format_feature_with_japanese(feature_name, japanese_names):
    """ç‰¹å¾´é‡åã‚’æ—¥æœ¬èªèª¬æ˜ä»˜ãã§ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ"""
    if feature_name in japanese_names:
        return f"{feature_name} ({japanese_names[feature_name]})"
    elif feature_name.startswith("feature_"):
        # GATåŸ‹ã‚è¾¼ã¿ç‰¹å¾´é‡
        feature_num = feature_name.replace("feature_", "")
        return f"{feature_name} (GATåŸ‹ã‚è¾¼ã¿æ¬¡å…ƒ{feature_num})"
    else:
        return feature_name


# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’è¿½åŠ 
project_root = Path(__file__).resolve().parents[2]
sys.path.append(str(project_root))


def load_actual_feature_names():
    """å®Ÿéš›ã®ç‰¹å¾´é‡åã‚’å–å¾—ï¼ˆIRLé‡ã¿ãƒ•ã‚¡ã‚¤ãƒ«ã®æ¬¡å…ƒæ•°ã«åŸºã¥ã„ã¦ï¼‰"""

    # å®Ÿéš›ã®IRLé‡ã¿ã®æ¬¡å…ƒæ•°ã‚’å–å¾—
    weights_path = project_root / "data" / "learned_weights_training.npy"
    if weights_path.exists():
        weights = np.load(weights_path)
        n_features = len(weights)
        print(f"âœ… IRLé‡ã¿ã‹ã‚‰ç‰¹å¾´é‡æ¬¡å…ƒæ•°ã‚’å–å¾—: {n_features}æ¬¡å…ƒ")
    else:
        n_features = 62  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
        print(f"âš ï¸  IRLé‡ã¿ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ{n_features}æ¬¡å…ƒã‚’ä½¿ç”¨")

    # å®Ÿéš›ã®ç‰¹å¾´é‡æŠ½å‡ºå™¨ã‹ã‚‰ç‰¹å¾´é‡åã‚’å–å¾—
    try:
        from omegaconf import OmegaConf

        from src.kazoo.features.feature_extractor import FeatureExtractor

        # è¨­å®šèª­ã¿è¾¼ã¿
        config_path = project_root / "configs" / "base.yaml"
        if config_path.exists():
            cfg = OmegaConf.load(config_path)
        else:
            # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šã‚’ä½œæˆ
            cfg = OmegaConf.create(
                {
                    "features": {
                        "all_labels": [
                            "bug",
                            "enhancement",
                            "documentation",
                            "question",
                            "help wanted",
                        ],
                        "label_to_skills": {
                            "bug": ["debugging", "analysis"],
                            "enhancement": ["python", "design"],
                            "documentation": ["writing"],
                            "question": ["communication"],
                            "help wanted": ["collaboration"],
                        },
                    },
                    "irl": {"use_gat": True},  # GATã‚’æœ‰åŠ¹åŒ–ã—ã¦æ­£ç¢ºãªç‰¹å¾´é‡åå–å¾—
                }
            )

        # ç‰¹å¾´é‡æŠ½å‡ºå™¨ã‚’åˆæœŸåŒ–
        feature_extractor = FeatureExtractor(cfg)
        feature_names = feature_extractor.feature_names

        print(f"âœ… ç‰¹å¾´é‡æŠ½å‡ºå™¨ã‹ã‚‰ç‰¹å¾´é‡åå–å¾—: {len(feature_names)}æ¬¡å…ƒ")
        print(f"Feature names: {feature_names[:10]}... (æœ€åˆã®10å€‹)")

        # æ¬¡å…ƒæ•°ã‚’åˆã‚ã›ã‚‹
        if len(feature_names) != n_features:
            print(
                f"âš ï¸  æ¬¡å…ƒæ•°èª¿æ•´: ç‰¹å¾´é‡å{len(feature_names)}æ¬¡å…ƒ â†’ IRLé‡ã¿{n_features}æ¬¡å…ƒ"
            )
            if len(feature_names) > n_features:
                feature_names = feature_names[:n_features]
            else:
                # ä¸è¶³åˆ†ã‚’ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°
                for i in range(len(feature_names), n_features):
                    feature_names.append(f"feature_{i}")

        return feature_names

    except Exception as e:
        print(f"âš ï¸  ç‰¹å¾´é‡æŠ½å‡ºå™¨åˆæœŸåŒ–ã«å¤±æ•—: {e}")
        print("æ—¢çŸ¥ã®ç‰¹å¾´é‡åã‚’ä½¿ç”¨ã—ã¾ã™...")

        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: æ—¢çŸ¥ã®ç‰¹å¾´é‡å
        feature_names = [
            # ã‚¿ã‚¹ã‚¯ç‰¹å¾´é‡ (20æ¬¡å…ƒ)
            "task_days_since_last_activity",
            "task_discussion_activity",
            "task_text_length",
            "task_code_block_count",
            "task_label_bug",
            "task_label_enhancement",
            "task_label_documentation",
            "task_label_question",
            "task_label_help wanted",
            "task_priority_score",
            "task_urgency_indicator",
            "task_complexity_estimate",
            "task_comment_count",
            "task_participant_count",
            "task_file_count",
            "task_line_count",
            "task_branch_age",
            "task_commit_frequency",
            "task_test_coverage",
            "task_documentation_quality",
            # é–‹ç™ºè€…ç‰¹å¾´é‡ (15æ¬¡å…ƒ)
            "dev_recent_activity_count",
            "dev_current_workload",
            "dev_total_lines_changed",
            "dev_collaboration_network_size",
            "dev_comment_interactions",
            "dev_cross_issue_activity",
            "dev_expertise_score",
            "dev_reputation",
            "dev_response_time",
            "dev_code_quality",
            "dev_test_coverage",
            "dev_documentation_score",
            "dev_leadership_score",
            "dev_mentoring_activity",
            "dev_innovation_index",
            # ãƒãƒƒãƒãƒ³ã‚°ç‰¹å¾´é‡ (5æ¬¡å…ƒ)
            "match_collaborated_with_task_author",
            "match_collaborator_overlap_count",
            "match_has_prior_collaboration",
            "match_skill_intersection_count",
            "match_file_experience_count",
            # GATçµ±è¨ˆç‰¹å¾´é‡ (5æ¬¡å…ƒ)
            "gat_similarity",
            "gat_dev_expertise",
            "gat_task_popularity",
            "gat_collaboration_strength",
            "gat_network_centrality",
            # GATåŸ‹ã‚è¾¼ã¿ (17æ¬¡å…ƒ = 62 - 45)
        ]

        # GATåŸ‹ã‚è¾¼ã¿ã‚’è¿½åŠ ã—ã¦åˆè¨ˆ62æ¬¡å…ƒã«ã™ã‚‹
        remaining_dims = n_features - len(feature_names)
        if remaining_dims > 0:
            feature_names.extend([f"gat_dev_emb_{i}" for i in range(remaining_dims)])

        # å¿…è¦ã«å¿œã˜ã¦èª¿æ•´
        feature_names = feature_names[:n_features]

        return feature_names


def generate_realistic_feature_data(feature_names, n_samples=1000):
    """å®Ÿéš›ã®IRLå­¦ç¿’ã§æƒ³å®šã•ã‚Œã‚‹ç¾å®Ÿçš„ãªç‰¹å¾´é‡ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆ"""

    print(
        f"ğŸ“Š ç¾å®Ÿçš„ãªç‰¹å¾´é‡ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆä¸­... ({n_samples}ã‚µãƒ³ãƒ—ãƒ«, {len(feature_names)}æ¬¡å…ƒ)"
    )

    n_features = len(feature_names)
    features = np.zeros((n_samples, n_features))

    for i, name in enumerate(feature_names):
        if "days_since" in name:
            # æ—¥æ•°ç³»: æŒ‡æ•°åˆ†å¸ƒ (0-365æ—¥) - æœ€è¿‘ã®ã‚¿ã‚¹ã‚¯ãŒå¤šã„
            features[:, i] = np.random.exponential(30, n_samples)
            features[:, i] = np.clip(features[:, i], 0, 365)
        elif "activity" in name or "count" in name:
            # æ´»å‹•ç³»: ãƒã‚¢ã‚½ãƒ³åˆ†å¸ƒ - ã‚¹ãƒ‘ãƒ¼ã‚¹ãªæ´»å‹•
            features[:, i] = np.random.poisson(3, n_samples)
        elif "length" in name:
            # é•·ã•ç³»: ãƒ­ã‚°ãƒãƒ¼ãƒãƒ«åˆ†å¸ƒ - å¤šãã¯çŸ­ã„ãŒã€æ™‚ã€…é•·ã„
            features[:, i] = np.random.lognormal(4, 1, n_samples)
        elif "label_" in name:
            # ãƒ©ãƒ™ãƒ«ç³»: ãƒ™ãƒ«ãƒŒãƒ¼ã‚¤åˆ†å¸ƒ - ãƒã‚¤ãƒŠãƒªç‰¹å¾´é‡
            features[:, i] = np.random.binomial(1, 0.15, n_samples).astype(float)
        elif "workload" in name:
            # ä½œæ¥­è² è·: ã‚¬ãƒ³ãƒåˆ†å¸ƒ (0-15) - é©åº¦ãªè² è·åˆ†å¸ƒ
            features[:, i] = np.random.gamma(2, 2, n_samples)
            features[:, i] = np.clip(features[:, i], 0, 15)
        elif "lines_changed" in name:
            # ã‚³ãƒ¼ãƒ‰è¡Œæ•°: ãƒ­ã‚°ãƒãƒ¼ãƒãƒ«åˆ†å¸ƒ - å°ã•ãªå¤‰æ›´ãŒå¤šã„
            features[:, i] = np.random.lognormal(3, 1.5, n_samples)
        elif "network" in name or "collaboration" in name:
            # ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ç³»: ã¹ãä¹—åˆ†å¸ƒ - å°‘æ•°ãŒå¤šãã®ã‚³ãƒã‚¯ã‚·ãƒ§ãƒ³ã‚’æŒã¤
            features[:, i] = np.random.pareto(1.16, n_samples) * 2
            features[:, i] = np.clip(features[:, i], 0, 50)
        elif "score" in name or "reputation" in name:
            # ã‚¹ã‚³ã‚¢ç³»: ãƒ™ãƒ¼ã‚¿åˆ†å¸ƒ (0-10) - é©åº¦ãªè©•ä¾¡åˆ†å¸ƒ
            features[:, i] = np.random.beta(2, 3, n_samples) * 10
        elif "affinity" in name or "similarity" in name:
            # è¦ªå’Œæ€§ãƒ»é¡ä¼¼åº¦: ãƒ™ãƒ¼ã‚¿åˆ†å¸ƒ (0-1) - å¤šãã¯ä½ã„é¡ä¼¼åº¦
            features[:, i] = np.random.beta(1, 4, n_samples)
        elif "match_" in name and ("collaborated" in name or "has_prior" in name):
            # ãƒãƒƒãƒãƒ³ã‚°ç³»ãƒã‚¤ãƒŠãƒª: ãƒ™ãƒ«ãƒŒãƒ¼ã‚¤åˆ†å¸ƒ - ç¨€ãªé–¢ä¿‚
            features[:, i] = np.random.binomial(1, 0.05, n_samples).astype(float)
        elif "gat_" in name:
            if "emb_" in name:
                # GATåŸ‹ã‚è¾¼ã¿: æ¨™æº–æ­£è¦åˆ†å¸ƒ - å­¦ç¿’ã•ã‚ŒãŸåŸ‹ã‚è¾¼ã¿
                features[:, i] = np.random.normal(0, 0.5, n_samples)
            else:
                # GATçµ±è¨ˆ: ãƒ™ãƒ¼ã‚¿åˆ†å¸ƒ (0-1) - æ­£è¦åŒ–ã•ã‚ŒãŸå€¤
                features[:, i] = np.random.beta(2, 5, n_samples)
        elif "time" in name or "age" in name:
            # æ™‚é–“ç³»: æŒ‡æ•°åˆ†å¸ƒ - æœ€è¿‘ã®ã‚‚ã®ãŒå¤šã„
            features[:, i] = np.random.exponential(20, n_samples)
        elif "priority" in name or "urgency" in name:
            # å„ªå…ˆåº¦ç³»: é›¢æ•£ä¸€æ§˜åˆ†å¸ƒ (1-5)
            features[:, i] = np.random.randint(1, 6, n_samples).astype(float)
        elif "quality" in name or "coverage" in name:
            # å“è³ªç³»: ãƒ™ãƒ¼ã‚¿åˆ†å¸ƒ (0-1) - å¤šãã¯ä¸­ç¨‹åº¦ã®å“è³ª
            features[:, i] = np.random.beta(3, 2, n_samples)
        else:
            # ãã®ä»–: è»½ã„ãƒ©ãƒ³ãƒ€ãƒ ã‚¦ã‚©ãƒ¼ã‚¯çš„åˆ†å¸ƒ
            features[:, i] = np.random.normal(0, 0.8, n_samples)

    print(f"âœ… ç¾å®Ÿçš„ãªç‰¹å¾´é‡ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆå®Œäº†")
    return features


def load_irl_weights():
    """IRLé‡ã¿ã‚’èª­ã¿è¾¼ã¿"""
    weights_path = project_root / "data" / "learned_weights_training.npy"
    if weights_path.exists():
        weights = np.load(weights_path)
        print(f"âœ… IRLé‡ã¿èª­ã¿è¾¼ã¿æˆåŠŸ: {len(weights)}æ¬¡å…ƒ")
        return weights
    else:
        print("âš ï¸  IRLé‡ã¿ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")
        # ãƒ€ãƒŸãƒ¼é‡ã¿ã‚’ç”Ÿæˆ
        return np.random.randn(62) * 0.5


def analyze_feature_distributions(features, feature_names, weights):
    """ç‰¹å¾´é‡åˆ†å¸ƒã®è©³ç´°åˆ†æ"""

    print("\n" + "=" * 80)
    print("ğŸ“Š ç‰¹å¾´é‡åˆ†å¸ƒåˆ†æãƒ¬ãƒãƒ¼ãƒˆï¼ˆå®Ÿéš›ã®IRLé‡ã¿ã‚’ä½¿ç”¨ï¼‰")
    print("=" * 80)

    # æ—¥æœ¬èªç‰¹å¾´é‡åã®å–å¾—
    japanese_names = get_feature_japanese_names()

    # åŸºæœ¬çµ±è¨ˆ
    df = pd.DataFrame(features, columns=feature_names)

    print(f"\nğŸ“ˆ ã€åŸºæœ¬çµ±è¨ˆã€‘")
    print(f"ã‚µãƒ³ãƒ—ãƒ«æ•°: {len(features):,}")
    print(f"ç‰¹å¾´é‡æ•°: {len(feature_names)}")
    print(f"IRLé‡ã¿æ•°: {len(weights)}")
    print(f"ãƒ‡ãƒ¼ã‚¿å½¢çŠ¶: {features.shape}")

    # IRLé‡ã¿ã®åŸºæœ¬çµ±è¨ˆ
    print(f"\nğŸ¯ ã€IRLé‡ã¿çµ±è¨ˆã€‘")
    print(f"å¹³å‡é‡ã¿: {np.mean(weights):.6f}")
    print(f"é‡ã¿æ¨™æº–åå·®: {np.std(weights):.6f}")
    print(f"é‡ã¿ç¯„å›²: [{np.min(weights):.6f}, {np.max(weights):.6f}]")
    print(
        f"æ­£ã®é‡ã¿: {np.sum(weights > 0)}å€‹ ({np.sum(weights > 0)/len(weights)*100:.1f}%)"
    )
    print(
        f"è² ã®é‡ã¿: {np.sum(weights < 0)}å€‹ ({np.sum(weights < 0)/len(weights)*100:.1f}%)"
    )
    print(
        f"ã‚¼ãƒ­ã®é‡ã¿: {np.sum(weights == 0)}å€‹ ({np.sum(weights == 0)/len(weights)*100:.1f}%)"
    )

    # åŸºæœ¬ç‰¹å¾´é‡ã¨GATç‰¹å¾´é‡ã®åˆ†é¡
    # å®Ÿéš›ã®ç‰¹å¾´é‡åï¼ˆ25æ¬¡å…ƒï¼‰+ feature_XXï¼ˆ37æ¬¡å…ƒï¼‰ã®å ´åˆã€feature_XXã‚’GATç‰¹å¾´é‡ã¨ã—ã¦æ‰±ã†
    basic_feature_names = [
        name
        for name in feature_names
        if not (
            name.startswith("gat_") or "gat_" in name or name.startswith("feature_")
        )
    ]
    gat_feature_names = [
        name
        for name in feature_names
        if (name.startswith("gat_") or "gat_" in name or name.startswith("feature_"))
    ]

    print(f"\nğŸ” ã€ç‰¹å¾´é‡åˆ†é¡ã€‘")
    print(f"åŸºæœ¬ç‰¹å¾´é‡+çµ±è¨ˆ: {len(basic_feature_names)}æ¬¡å…ƒ")
    print(f"GATç‰¹å¾´é‡: {len(gat_feature_names)}æ¬¡å…ƒ")

    # è©³ç´°åˆ†é¡ï¼ˆfeature_XXã‚’GATåŸ‹ã‚è¾¼ã¿ã¨ã—ã¦æ‰±ã†ï¼‰
    categories = {
        "ã‚¿ã‚¹ã‚¯ç‰¹å¾´é‡": [name for name in feature_names if name.startswith("task_")],
        "é–‹ç™ºè€…ç‰¹å¾´é‡": [name for name in feature_names if name.startswith("dev_")],
        "ãƒãƒƒãƒãƒ³ã‚°ç‰¹å¾´é‡": [
            name for name in feature_names if name.startswith("match_")
        ],
        "GATçµ±è¨ˆç‰¹å¾´é‡": [
            name
            for name in feature_names
            if name.startswith("gat_") and "emb_" not in name
        ],
        "GATåŸ‹ã‚è¾¼ã¿": [
            name
            for name in feature_names
            if (
                "gat_dev_emb_" in name
                or "gat_emb_" in name
                or name.startswith("feature_")
            )
        ],
    }

    # å¤§åˆ†é¡ã§ã®ã‚«ãƒ†ã‚´ãƒª
    major_categories = {
        "åŸºæœ¬ç‰¹å¾´é‡+çµ±è¨ˆ": basic_feature_names,
        "GATç‰¹å¾´é‡": gat_feature_names,
    }

    stats_report = []

    # å¤§åˆ†é¡ã§ã®åˆ†æã‚’å…ˆã«è¡¨ç¤º
    print(f"\n" + "=" * 60)
    print("ğŸ“Š ã€å¤§åˆ†é¡åˆ¥åˆ†æã€‘")
    print("=" * 60)

    for major_category, names in major_categories.items():
        if not names:
            continue

        print(f"\nğŸ“‹ ã€{major_category}ã€‘({len(names)}æ¬¡å…ƒ)")

        # å¤§åˆ†é¡å†…ã®ç‰¹å¾´é‡ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’å–å¾—
        category_indices = [
            feature_names.index(name) for name in names if name in feature_names
        ]
        category_weights = (
            weights[category_indices] if len(category_indices) > 0 else []
        )

        if len(category_weights) > 0:
            print(f"   é‡ã¿çµ±è¨ˆ:")
            print(f"     - å¹³å‡: {np.mean(category_weights):.6f}")
            print(f"     - æ¨™æº–åå·®: {np.std(category_weights):.6f}")
            print(
                f"     - ç¯„å›²: [{np.min(category_weights):.6f}, {np.max(category_weights):.6f}]"
            )
            print(
                f"     - æ­£ã®é‡ã¿: {np.sum(np.array(category_weights) > 0)}å€‹ ({np.sum(np.array(category_weights) > 0)/len(category_weights)*100:.1f}%)"
            )
            print(
                f"     - è² ã®é‡ã¿: {np.sum(np.array(category_weights) < 0)}å€‹ ({np.sum(np.array(category_weights) < 0)/len(category_weights)*100:.1f}%)"
            )

            # å¤§åˆ†é¡å†…ã®é‡è¦ç‰¹å¾´é‡TOP5
            major_cat_data = []
            for name in names:
                if name in feature_names:
                    feature_idx = feature_names.index(name)
                    if feature_idx < len(weights):
                        major_cat_data.append(
                            {
                                "feature": name,
                                "irl_weight": weights[feature_idx],
                                "importance": abs(weights[feature_idx]),
                            }
                        )

            if major_cat_data:
                major_cat_df = pd.DataFrame(major_cat_data)
                top_major = major_cat_df.nlargest(5, "importance")
                print(f"   é‡è¦ç‰¹å¾´é‡TOP5:")
                for idx, (_, row) in enumerate(top_major.iterrows(), 1):
                    print(
                        f"     {idx}. {row['feature'][:45]:45s} | é‡ã¿:{row['irl_weight']:8.5f}"
                    )

    print(f"\n" + "=" * 60)
    print("ğŸ“Š ã€è©³ç´°ã‚«ãƒ†ã‚´ãƒªåˆ¥åˆ†æã€‘")
    print("=" * 60)

    for category, names in categories.items():
        if not names:
            continue

        print(f"\nğŸ“‹ ã€{category}ã€‘({len(names)}æ¬¡å…ƒ)")

        # ã‚«ãƒ†ã‚´ãƒªå†…ã®ç‰¹å¾´é‡ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’å–å¾—
        category_indices = [
            feature_names.index(name) for name in names if name in feature_names
        ]
        category_weights = (
            weights[category_indices] if len(category_indices) > 0 else []
        )

        if len(category_weights) > 0:
            print(
                f"   é‡ã¿çµ±è¨ˆ - å¹³å‡: {np.mean(category_weights):.6f}, "
                f"æ¨™æº–åå·®: {np.std(category_weights):.6f}, "
                f"ç¯„å›²: [{np.min(category_weights):.6f}, {np.max(category_weights):.6f}]"
            )

        for i, name in enumerate(names):
            if name in feature_names:
                feature_idx = feature_names.index(name)
                if feature_idx < len(weights):
                    values = features[:, feature_idx]
                    weight = weights[feature_idx]

                    stats = {
                        "feature": name,
                        "category": category,
                        "feature_index": feature_idx,
                        "mean": np.mean(values),
                        "std": np.std(values),
                        "min": np.min(values),
                        "max": np.max(values),
                        "median": np.median(values),
                        "irl_weight": weight,
                        "importance": abs(weight),
                        "zeros_pct": np.mean(values == 0) * 100,
                        "outliers_pct": np.mean(
                            np.abs(values - np.mean(values)) > 3 * np.std(values)
                        )
                        * 100,
                        "skewness": np.nan,  # scipy.stats.skew(values) if available
                        "kurtosis": np.nan,  # scipy.stats.kurtosis(values) if available
                    }

                    # scipyçµ±è¨ˆã‚’è¨ˆç®—ï¼ˆåˆ©ç”¨å¯èƒ½ãªå ´åˆï¼‰
                    try:
                        from scipy import stats

                        stats["skewness"] = stats.skew(values)
                        stats["kurtosis"] = stats.kurtosis(values)
                    except ImportError:
                        pass

                    stats_report.append(stats)

                    if i < 5:  # æœ€åˆã®5å€‹ã ã‘è©³ç´°è¡¨ç¤º
                        print(
                            f"  {name[:35]:35s} | "
                            f"é‡ã¿:{stats['irl_weight']:8.5f} | "
                            f"é‡è¦åº¦:{stats['importance']:8.5f} | "
                            f"å¹³å‡:{stats['mean']:8.3f} | "
                            f"æ¨™æº–åå·®:{stats['std']:7.3f}"
                        )

        if len(names) > 5:
            print(f"  ... ä»– {len(names) - 5} å€‹ã®ç‰¹å¾´é‡")

    # é‡è¦ç‰¹å¾´é‡TOP10ã‚’è¡¨ç¤º
    if stats_report:
        stats_df = pd.DataFrame(stats_report)
        top_features = stats_df.nlargest(10, "importance")

        print(f"\nğŸ† ã€é‡è¦ç‰¹å¾´é‡TOP10ï¼ˆå®Ÿéš›ã®IRLé‡ã¿ï¼‰ã€‘")
        for idx, (_, row) in enumerate(top_features.iterrows(), 1):
            feature_display = format_feature_with_japanese(
                row["feature"], japanese_names
            )
            print(
                f"  {idx:2d}. {feature_display[:55]:55s} | "
                f"é‡ã¿:{row['irl_weight']:8.5f} | "
                f"é‡è¦åº¦:{row['importance']:8.5f} | "
                f"ã‚«ãƒ†ã‚´ãƒª:{row['category']}"
            )

        # åŸºæœ¬ç‰¹å¾´é‡å†…ã§ã®ãƒ©ãƒ³ã‚­ãƒ³ã‚°
        basic_stats = stats_df[
            ~(
                stats_df["feature"].str.contains("gat_")
                | stats_df["feature"].str.startswith("feature_")
            )
        ]
        if not basic_stats.empty:
            top_basic_features = basic_stats.nlargest(10, "importance")

            print(f"\nğŸ¥‡ ã€åŸºæœ¬ç‰¹å¾´é‡å†…é‡è¦ãƒ©ãƒ³ã‚­ãƒ³ã‚°TOP10ã€‘")
            for idx, (_, row) in enumerate(top_basic_features.iterrows(), 1):
                feature_display = format_feature_with_japanese(
                    row["feature"], japanese_names
                )
                print(
                    f"  {idx:2d}. {feature_display[:55]:55s} | "
                    f"é‡ã¿:{row['irl_weight']:8.5f} | "
                    f"é‡è¦åº¦:{row['importance']:8.5f} | "
                    f"ã‚«ãƒ†ã‚´ãƒª:{row['category']}"
                )

        # GATç‰¹å¾´é‡å†…ã§ã®ãƒ©ãƒ³ã‚­ãƒ³ã‚°
        gat_stats = stats_df[
            stats_df["feature"].str.contains("gat_")
            | stats_df["feature"].str.startswith("feature_")
        ]
        if not gat_stats.empty:
            top_gat_features = gat_stats.nlargest(10, "importance")

            print(f"\nğŸ¤– ã€GATç‰¹å¾´é‡å†…é‡è¦ãƒ©ãƒ³ã‚­ãƒ³ã‚°TOP10ã€‘")
            for idx, (_, row) in enumerate(top_gat_features.iterrows(), 1):
                feature_display = format_feature_with_japanese(
                    row["feature"], japanese_names
                )
                print(
                    f"  {idx:2d}. {feature_display[:55]:55s} | "
                    f"é‡ã¿:{row['irl_weight']:8.5f} | "
                    f"é‡è¦åº¦:{row['importance']:8.5f} | "
                    f"ã‚«ãƒ†ã‚´ãƒª:{row['category']}"
                )

    return pd.DataFrame(stats_report)


def create_visualizations(features, feature_names, weights, stats_df):
    """ç‰¹å¾´é‡åˆ†å¸ƒã®å¯è¦–åŒ–"""

    print(f"\nğŸ¨ å¯è¦–åŒ–ã‚’ä½œæˆä¸­...")

    # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ
    output_dir = project_root / "outputs"
    output_dir.mkdir(exist_ok=True)

    # å›³1: IRLé‡ã¿ã®åˆ†å¸ƒ
    plt.figure(figsize=(15, 10))

    # ã‚µãƒ–ãƒ—ãƒ­ãƒƒãƒˆ1: é‡ã¿ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ 
    plt.subplot(2, 3, 1)
    plt.hist(weights, bins=30, alpha=0.7, color="skyblue", edgecolor="black")
    plt.xlabel("IRLé‡ã¿")
    plt.ylabel("é »åº¦")
    plt.title("IRLé‡ã¿ã®åˆ†å¸ƒ")
    plt.grid(True, alpha=0.3)

    # ã‚µãƒ–ãƒ—ãƒ­ãƒƒãƒˆ2: ã‚«ãƒ†ã‚´ãƒªåˆ¥é‡ã¿
    if not stats_df.empty:
        plt.subplot(2, 3, 2)
        category_weights = stats_df.groupby("category")["irl_weight"].agg(
            ["mean", "std"]
        )
        category_weights.plot(
            kind="bar",
            y="mean",
            yerr="std",
            ax=plt.gca(),
            color="lightcoral",
            alpha=0.7,
        )
        plt.title("ã‚«ãƒ†ã‚´ãƒªåˆ¥å¹³å‡IRLé‡ã¿")
        plt.xticks(rotation=45)
        plt.grid(True, alpha=0.3)

    # ã‚µãƒ–ãƒ—ãƒ­ãƒƒãƒˆ3: é‡è¦ç‰¹å¾´é‡TOP10
    plt.subplot(2, 3, 3)
    if not stats_df.empty:
        top_features = stats_df.nlargest(10, "importance")
        colors = ["red" if w < 0 else "green" for w in top_features["irl_weight"]]
        plt.barh(
            range(len(top_features)),
            top_features["irl_weight"],
            color=colors,
            alpha=0.7,
        )
        plt.yticks(
            range(len(top_features)),
            [
                name[:20] + "..." if len(name) > 20 else name
                for name in top_features["feature"]
            ],
        )
        plt.xlabel("IRLé‡ã¿")
        plt.title("é‡è¦ç‰¹å¾´é‡TOP10")
        plt.grid(True, alpha=0.3)

    # ã‚µãƒ–ãƒ—ãƒ­ãƒƒãƒˆ4: ç‰¹å¾´é‡å€¤ã®åˆ†å¸ƒï¼ˆã‚µãƒ³ãƒ—ãƒ«ï¼‰
    plt.subplot(2, 3, 4)
    sample_features = features[:, : min(5, features.shape[1])]
    plt.boxplot(
        sample_features,
        labels=[name[:10] for name in feature_names[: min(5, len(feature_names))]],
    )
    plt.title("ç‰¹å¾´é‡å€¤ã®åˆ†å¸ƒï¼ˆæœ€åˆã®5å€‹ï¼‰")
    plt.xticks(rotation=45)
    plt.grid(True, alpha=0.3)

    # ã‚µãƒ–ãƒ—ãƒ­ãƒƒãƒˆ5: ç›¸é–¢ãƒãƒˆãƒªãƒƒã‚¯ã‚¹ï¼ˆã‚µãƒ³ãƒ—ãƒ«ï¼‰
    plt.subplot(2, 3, 5)
    if features.shape[1] >= 10:
        sample_corr = np.corrcoef(features[:, :10].T)
        im = plt.imshow(sample_corr, cmap="coolwarm", vmin=-1, vmax=1)
        plt.colorbar(im)
        plt.title("ç‰¹å¾´é‡ç›¸é–¢ãƒãƒˆãƒªãƒƒã‚¯ã‚¹ï¼ˆæœ€åˆã®10å€‹ï¼‰")
        plt.xticks(range(10), [name[:5] for name in feature_names[:10]], rotation=45)
        plt.yticks(range(10), [name[:5] for name in feature_names[:10]])

    # ã‚µãƒ–ãƒ—ãƒ­ãƒƒãƒˆ6: é‡ã¿ vs é‡è¦åº¦æ•£å¸ƒå›³
    plt.subplot(2, 3, 6)
    if not stats_df.empty:
        scatter = plt.scatter(
            stats_df["irl_weight"],
            stats_df["importance"],
            c=stats_df.index,
            cmap="viridis",
            alpha=0.7,
        )
        plt.xlabel("IRLé‡ã¿")
        plt.ylabel("é‡è¦åº¦ï¼ˆçµ¶å¯¾å€¤ï¼‰")
        plt.title("é‡ã¿ vs é‡è¦åº¦")
        plt.grid(True, alpha=0.3)

        # é‡è¦ãªç‚¹ã«ãƒ©ãƒ™ãƒ«
        top_points = stats_df.nlargest(5, "importance")
        for _, row in top_points.iterrows():
            plt.annotate(
                row["feature"][:10],
                (row["irl_weight"], row["importance"]),
                xytext=(5, 5),
                textcoords="offset points",
                fontsize=8,
                alpha=0.8,
            )

    plt.tight_layout()

    # ä¿å­˜
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    fig_path = output_dir / f"feature_distribution_analysis_{timestamp}.png"
    plt.savefig(fig_path, dpi=300, bbox_inches="tight")
    print(f"âœ… å›³1ä¿å­˜: {fig_path}")

    # å›³2: ã‚«ãƒ†ã‚´ãƒªåˆ¥è©³ç´°åˆ†æ
    if not stats_df.empty:
        plt.figure(figsize=(16, 12))

        categories = stats_df["category"].unique()
        n_cats = len(categories)

        for i, category in enumerate(categories):
            cat_data = stats_df[stats_df["category"] == category]

            if len(cat_data) == 0:
                continue

            # å„ã‚«ãƒ†ã‚´ãƒªã®ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ 
            plt.subplot(n_cats, 2, 2 * i + 1)
            plt.hist(
                cat_data["irl_weight"],
                bins=10,
                alpha=0.7,
                color=plt.cm.Set3(i),
                edgecolor="black",
            )
            plt.title(f"{category} - IRLé‡ã¿åˆ†å¸ƒ")
            plt.xlabel("IRLé‡ã¿")
            plt.ylabel("é »åº¦")
            plt.grid(True, alpha=0.3)

            # å„ã‚«ãƒ†ã‚´ãƒªã®é‡è¦ç‰¹å¾´é‡
            plt.subplot(n_cats, 2, 2 * i + 2)
            top_cat = cat_data.nlargest(min(5, len(cat_data)), "importance")
            colors = ["red" if w < 0 else "green" for w in top_cat["irl_weight"]]
            plt.barh(
                range(len(top_cat)), top_cat["irl_weight"], color=colors, alpha=0.7
            )
            plt.yticks(range(len(top_cat)), [name[:15] for name in top_cat["feature"]])
            plt.title(f"{category} - é‡è¦ç‰¹å¾´é‡")
            plt.xlabel("IRLé‡ã¿")
            plt.grid(True, alpha=0.3)

        plt.tight_layout()

        # ä¿å­˜
        fig2_path = output_dir / f"feature_category_analysis_{timestamp}.png"
        plt.savefig(fig2_path, dpi=300, bbox_inches="tight")
        print(f"âœ… å›³2ä¿å­˜: {fig2_path}")

    plt.show()

    return fig_path, fig2_path if "fig2_path" in locals() else None


def save_detailed_report(stats_df, output_dir):
    """è©³ç´°ãƒ¬ãƒãƒ¼ãƒˆã‚’CSVã¨ãƒ†ã‚­ã‚¹ãƒˆã§ä¿å­˜"""

    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

    # æ—¥æœ¬èªç‰¹å¾´é‡åã®å–å¾—
    japanese_names = get_feature_japanese_names()

    # CSVä¿å­˜
    if not stats_df.empty:
        csv_path = output_dir / f"feature_distribution_stats_{timestamp}.csv"
        stats_df.to_csv(csv_path, index=False, encoding="utf-8")
        print(f"âœ… çµ±è¨ˆCSVãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜: {csv_path}")

    # ãƒ†ã‚­ã‚¹ãƒˆãƒ¬ãƒãƒ¼ãƒˆ
    report_path = output_dir / f"feature_distribution_report_{timestamp}.txt"

    with open(report_path, "w", encoding="utf-8") as f:
        f.write("ç‰¹å¾´é‡åˆ†å¸ƒåˆ†æãƒ¬ãƒãƒ¼ãƒˆ\n")
        f.write("=" * 60 + "\n")
        f.write(f"ç”Ÿæˆæ—¥æ™‚: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")

        if not stats_df.empty:
            # å¤§åˆ†é¡åˆ¥ã‚µãƒãƒªãƒ¼
            f.write("ã€å¤§åˆ†é¡åˆ¥ã‚µãƒãƒªãƒ¼ã€‘\n")
            basic_features = stats_df[
                ~(
                    stats_df["feature"].str.contains("gat_")
                    | stats_df["feature"].str.startswith("feature_")
                )
            ]
            gat_features = stats_df[
                stats_df["feature"].str.contains("gat_")
                | stats_df["feature"].str.startswith("feature_")
            ]

            f.write("åŸºæœ¬ç‰¹å¾´é‡+çµ±è¨ˆ:\n")
            if not basic_features.empty:
                f.write(f"  ç‰¹å¾´é‡æ•°: {len(basic_features)}\n")
                f.write(f"  é‡ã¿å¹³å‡: {basic_features['irl_weight'].mean():.6f}\n")
                f.write(f"  é‡ã¿æ¨™æº–åå·®: {basic_features['irl_weight'].std():.6f}\n")
                f.write(
                    f"  é‡ã¿ç¯„å›²: [{basic_features['irl_weight'].min():.6f}, {basic_features['irl_weight'].max():.6f}]\n"
                )
                f.write(f"  æ­£ã®é‡ã¿: {(basic_features['irl_weight'] > 0).sum()}å€‹\n")
                f.write(f"  è² ã®é‡ã¿: {(basic_features['irl_weight'] < 0).sum()}å€‹\n")

            f.write("\nGATç‰¹å¾´é‡:\n")
            if not gat_features.empty:
                f.write(f"  ç‰¹å¾´é‡æ•°: {len(gat_features)}\n")
                f.write(f"  é‡ã¿å¹³å‡: {gat_features['irl_weight'].mean():.6f}\n")
                f.write(f"  é‡ã¿æ¨™æº–åå·®: {gat_features['irl_weight'].std():.6f}\n")
                f.write(
                    f"  é‡ã¿ç¯„å›²: [{gat_features['irl_weight'].min():.6f}, {gat_features['irl_weight'].max():.6f}]\n"
                )
                f.write(f"  æ­£ã®é‡ã¿: {(gat_features['irl_weight'] > 0).sum()}å€‹\n")
                f.write(f"  è² ã®é‡ã¿: {(gat_features['irl_weight'] < 0).sum()}å€‹\n")
            f.write("\n")

            f.write("ã€è©³ç´°ã‚«ãƒ†ã‚´ãƒªåˆ¥ã‚µãƒãƒªãƒ¼ã€‘\n")
            category_summary = (
                stats_df.groupby("category")
                .agg(
                    {
                        "irl_weight": ["count", "mean", "std", "min", "max"],
                        "importance": ["mean", "max"],
                        "zeros_pct": "mean",
                    }
                )
                .round(4)
            )
            f.write(category_summary.to_string())
            f.write("\n\n")

            f.write("ã€åŸºæœ¬ç‰¹å¾´é‡+çµ±è¨ˆ é‡è¦TOP15ã€‘\n")
            if not basic_features.empty:
                top_basic = basic_features.nlargest(15, "importance")
                for idx, (_, row) in enumerate(top_basic.iterrows(), 1):
                    feature_display = format_feature_with_japanese(
                        row["feature"], japanese_names
                    )
                    f.write(
                        f"{idx:2d}. {feature_display[:50]:50s} | "
                        f"é‡ã¿:{row['irl_weight']:8.4f} | "
                        f"é‡è¦åº¦:{row['importance']:8.4f} | "
                        f"ã‚«ãƒ†ã‚´ãƒª:{row['category']}\n"
                    )
            f.write("\n")

            f.write("ã€GATç‰¹å¾´é‡ é‡è¦TOP15ã€‘\n")
            if not gat_features.empty:
                top_gat = gat_features.nlargest(15, "importance")
                for idx, (_, row) in enumerate(top_gat.iterrows(), 1):
                    feature_display = format_feature_with_japanese(
                        row["feature"], japanese_names
                    )
                    f.write(
                        f"{idx:2d}. {feature_display[:50]:50s} | "
                        f"é‡ã¿:{row['irl_weight']:8.4f} | "
                        f"é‡è¦åº¦:{row['importance']:8.4f} | "
                        f"ã‚«ãƒ†ã‚´ãƒª:{row['category']}\n"
                    )
            f.write("\n")

            # ã‚«ãƒ†ã‚´ãƒªåˆ¥ã®è©³ç´°ãƒ©ãƒ³ã‚­ãƒ³ã‚°
            f.write("ã€ã‚«ãƒ†ã‚´ãƒªåˆ¥é‡è¦ãƒ©ãƒ³ã‚­ãƒ³ã‚°ã€‘\n")
            for category in ["ã‚¿ã‚¹ã‚¯ç‰¹å¾´é‡", "é–‹ç™ºè€…ç‰¹å¾´é‡", "ãƒãƒƒãƒãƒ³ã‚°ç‰¹å¾´é‡"]:
                cat_features = stats_df[stats_df["category"] == category]
                if not cat_features.empty:
                    f.write(f"\n{category}:\n")
                    top_cat = cat_features.nlargest(
                        min(10, len(cat_features)), "importance"
                    )
                    for idx, (_, row) in enumerate(top_cat.iterrows(), 1):
                        feature_display = format_feature_with_japanese(
                            row["feature"], japanese_names
                        )
                        f.write(
                            f"  {idx:2d}. {feature_display[:45]:45s} | "
                            f"é‡ã¿:{row['irl_weight']:8.4f} | "
                            f"é‡è¦åº¦:{row['importance']:8.4f}\n"
                        )

            f.write("ã€å…¨ä½“é‡è¦ç‰¹å¾´é‡TOP20ã€‘\n")
            top_features = stats_df.nlargest(20, "importance")
            for _, row in top_features.iterrows():
                feature_display = format_feature_with_japanese(
                    row["feature"], japanese_names
                )
                f.write(
                    f"{feature_display[:50]:50s} | "
                    f"é‡ã¿:{row['irl_weight']:8.4f} | "
                    f"é‡è¦åº¦:{row['importance']:8.4f} | "
                    f"ã‚«ãƒ†ã‚´ãƒª:{row['category']}\n"
                )

            f.write("\nã€è² ã®é‡ã¿ç‰¹å¾´é‡ã€‘\n")
            negative_features = stats_df[stats_df["irl_weight"] < 0].sort_values(
                "irl_weight"
            )
            for _, row in negative_features.iterrows():
                feature_display = format_feature_with_japanese(
                    row["feature"], japanese_names
                )
                f.write(
                    f"{feature_display[:50]:50s} | " f"é‡ã¿:{row['irl_weight']:8.4f}\n"
                )

    print(f"âœ… è©³ç´°ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜: {report_path}")

    return csv_path, report_path


def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""

    print("ğŸš€ ç‰¹å¾´é‡åˆ†å¸ƒåˆ†æã‚’é–‹å§‹...")
    print("âš ï¸  æ³¨æ„: ã“ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã¯å®Ÿéš›ã®IRLå­¦ç¿’æ¸ˆã¿é‡ã¿ã‚’ä½¿ç”¨ã—ã¦ã„ã¾ã™")

    # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    print("\n1ï¸âƒ£  ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ä¸­...")

    # å®Ÿéš›ã®ç‰¹å¾´é‡åã‚’å–å¾—
    feature_names = load_actual_feature_names()

    # å®Ÿéš›ã®IRLé‡ã¿ã‚’èª­ã¿è¾¼ã¿
    weights = load_irl_weights()

    # ç¾å®Ÿçš„ãªç‰¹å¾´é‡ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆï¼ˆå®Ÿéš›ã®åˆ†å¸ƒã‚’æ¨¡æ“¬ï¼‰
    features = generate_realistic_feature_data(feature_names, n_samples=1000)

    # æ¬¡å…ƒæ•°ãƒã‚§ãƒƒã‚¯
    if len(weights) != len(feature_names):
        print(
            f"âš ï¸  æ¬¡å…ƒæ•°ä¸ä¸€è‡´: é‡ã¿{len(weights)}æ¬¡å…ƒ vs ç‰¹å¾´é‡{len(feature_names)}æ¬¡å…ƒ"
        )
        min_dim = min(len(weights), len(feature_names))
        weights = weights[:min_dim]
        feature_names = feature_names[:min_dim]
        features = features[:, :min_dim]
        print(f"   â†’ {min_dim}æ¬¡å…ƒã«èª¿æ•´ã—ã¾ã—ãŸ")

    print(f"\nğŸ“Š åˆ†æå¯¾è±¡:")
    print(f"   - ã‚µãƒ³ãƒ—ãƒ«æ•°: {len(features):,}")
    print(f"   - ç‰¹å¾´é‡æ•°: {len(feature_names)}")
    print(f"   - IRLé‡ã¿: {len(weights)}æ¬¡å…ƒ")
    print(f"   - ãƒ‡ãƒ¼ã‚¿å½¢çŠ¶: {features.shape}")

    # åˆ†å¸ƒåˆ†æ
    print("\n2ï¸âƒ£  åˆ†å¸ƒåˆ†æä¸­...")
    stats_df = analyze_feature_distributions(features, feature_names, weights)

    # å¯è¦–åŒ–
    print("\n3ï¸âƒ£  å¯è¦–åŒ–ä½œæˆä¸­...")
    output_dir = project_root / "outputs"
    fig_paths = create_visualizations(features, feature_names, weights, stats_df)

    # ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜
    print("\n4ï¸âƒ£  ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜ä¸­...")
    csv_path, report_path = save_detailed_report(stats_df, output_dir)

    print("\nâœ… åˆ†æå®Œäº†!")
    print("ï¿½ å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«:")
    print(f"   - å›³1: {fig_paths[0] if isinstance(fig_paths, tuple) else fig_paths}")
    if isinstance(fig_paths, tuple) and len(fig_paths) > 1 and fig_paths[1]:
        print(f"   - å›³2: {fig_paths[1]}")
    print(f"   - ãƒ¬ãƒãƒ¼ãƒˆ: {report_path}")
    print(f"   - CSV: {csv_path}")

    print(f"\nğŸ” é‡è¦ãªç™ºè¦‹:")
    if not stats_df.empty:
        # æ—¥æœ¬èªç‰¹å¾´é‡åã®å–å¾—
        japanese_names = get_feature_japanese_names()

        # æœ€ã‚‚é‡è¦ãªç‰¹å¾´é‡
        top_feature = stats_df.loc[stats_df["importance"].idxmax()]
        feature_display = format_feature_with_japanese(
            top_feature["feature"], japanese_names
        )
        print(
            f"   - æœ€é‡è¦ç‰¹å¾´é‡: {feature_display} (é‡ã¿: {top_feature['irl_weight']:.4f})"
        )

        # è² ã®é‡ã¿ã®ç‰¹å¾´é‡æ•°
        negative_count = len(stats_df[stats_df["irl_weight"] < 0])
        print(f"   - è² ã®é‡ã¿ç‰¹å¾´é‡: {negative_count}å€‹")

        # ã‚«ãƒ†ã‚´ãƒªåˆ¥é‡è¦åº¦
        cat_importance = (
            stats_df.groupby("category")["importance"]
            .mean()
            .sort_values(ascending=False)
        )
        print(
            f"   - æœ€é‡è¦ã‚«ãƒ†ã‚´ãƒª: {cat_importance.index[0]} (å¹³å‡é‡è¦åº¦: {cat_importance.iloc[0]:.4f})"
        )

        # å¤§åˆ†é¡åˆ¥ã®åˆ†æ
        basic_features = stats_df[
            ~(
                stats_df["feature"].str.contains("gat_")
                | stats_df["feature"].str.startswith("feature_")
            )
        ]
        gat_features = stats_df[
            stats_df["feature"].str.contains("gat_")
            | stats_df["feature"].str.startswith("feature_")
        ]

        print(f"\nğŸ¯ å¤§åˆ†é¡åˆ¥çµ±è¨ˆ:")
        if not basic_features.empty:
            print(f"   ã€åŸºæœ¬ç‰¹å¾´é‡+çµ±è¨ˆã€‘({len(basic_features)}æ¬¡å…ƒ)")
            print(f"     - é‡ã¿å¹³å‡: {basic_features['irl_weight'].mean():.4f}")
            print(f"     - é‡è¦åº¦å¹³å‡: {basic_features['importance'].mean():.4f}")
            top_basic_feature = basic_features.loc[
                basic_features["importance"].idxmax()
            ]
            top_basic_display = format_feature_with_japanese(
                top_basic_feature["feature"], japanese_names
            )
            print(
                f"     - æœ€é‡è¦: {top_basic_display} (é‡ã¿: {top_basic_feature['irl_weight']:.4f})"
            )

            # åŸºæœ¬ç‰¹å¾´é‡å†…TOP3
            top_basic_3 = basic_features.nlargest(3, "importance")
            print(f"     - åŸºæœ¬ç‰¹å¾´é‡TOP3:")
            for idx, (_, row) in enumerate(top_basic_3.iterrows(), 1):
                feature_display = format_feature_with_japanese(
                    row["feature"], japanese_names
                )
                print(
                    f"       {idx}. {feature_display[:40]:40s} (é‡ã¿: {row['irl_weight']:7.4f})"
                )

        if not gat_features.empty:
            print(f"   ã€GATç‰¹å¾´é‡ã€‘({len(gat_features)}æ¬¡å…ƒ)")
            print(f"     - é‡ã¿å¹³å‡: {gat_features['irl_weight'].mean():.4f}")
            print(f"     - é‡è¦åº¦å¹³å‡: {gat_features['importance'].mean():.4f}")
            top_gat_feature = gat_features.loc[gat_features["importance"].idxmax()]
            top_gat_display = format_feature_with_japanese(
                top_gat_feature["feature"], japanese_names
            )
            print(
                f"     - æœ€é‡è¦: {top_gat_display} (é‡ã¿: {top_gat_feature['irl_weight']:.4f})"
            )

            # GATç‰¹å¾´é‡å†…TOP3
            top_gat_3 = gat_features.nlargest(3, "importance")
            print(f"     - GATç‰¹å¾´é‡TOP3:")
            for idx, (_, row) in enumerate(top_gat_3.iterrows(), 1):
                feature_display = format_feature_with_japanese(
                    row["feature"], japanese_names
                )
                print(
                    f"       {idx}. {feature_display[:40]:40s} (é‡ã¿: {row['irl_weight']:7.4f})"
                )

        # å®Ÿéš›ã®IRLé‡ã¿ã®çµ±è¨ˆ
        print(f"\nğŸ“ˆ IRLé‡ã¿çµ±è¨ˆ:")
        print(f"   - å¹³å‡: {np.mean(weights):.4f}")
        print(f"   - æ¨™æº–åå·®: {np.std(weights):.4f}")
        print(f"   - æœ€å°å€¤: {np.min(weights):.4f}")
        print(f"   - æœ€å¤§å€¤: {np.max(weights):.4f}")
        print(f"   - æ­£ã®é‡ã¿: {np.sum(weights > 0)}å€‹")
        print(f"   - è² ã®é‡ã¿: {np.sum(weights < 0)}å€‹")
        print(f"   - ã‚¼ãƒ­ã®é‡ã¿: {np.sum(weights == 0)}å€‹")


if __name__ == "__main__":
    main()
