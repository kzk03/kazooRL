#!/usr/bin/env python3
"""
ç‰¹å¾´é‡åˆ†æåŸºç›¤ã®ä½¿ç”¨ä¾‹
====================

å®Ÿè£…ã—ãŸç‰¹å¾´é‡åˆ†æã‚¯ãƒ©ã‚¹ã®å®Ÿéš›ã®ä½¿ç”¨æ–¹æ³•ã‚’ç¤ºã—ã¾ã™ã€‚
"""

import sys
from pathlib import Path

import numpy as np

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’è¿½åŠ 
project_root = Path(__file__).resolve().parents[3]
sys.path.append(str(project_root))
sys.path.append(str(Path(__file__).parent))

from feature_correlation_analyzer import FeatureCorrelationAnalyzer
from feature_distribution_analyzer import FeatureDistributionAnalyzer
from feature_importance_analyzer import FeatureImportanceAnalyzer


def load_real_irl_data():
    """å®Ÿéš›ã®IRLãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ï¼ˆåˆ©ç”¨å¯èƒ½ãªå ´åˆï¼‰"""
    data_dir = project_root / "kazoo" / "data"
    
    # IRLé‡ã¿ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¢ã™
    weights_files = [
        "learned_weights_bot_excluded.npy",
        "learned_weights_test.npy"
    ]
    
    weights_path = None
    for weights_file in weights_files:
        candidate_path = data_dir / weights_file
        if candidate_path.exists():
            weights_path = candidate_path
            break
    
    if weights_path is None:
        print("âš ï¸  å®Ÿéš›ã®IRLé‡ã¿ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")
        return None, None, None
    
    # é‡ã¿ã‚’èª­ã¿è¾¼ã¿
    weights = np.load(weights_path)
    print(f"âœ… IRLé‡ã¿èª­ã¿è¾¼ã¿æˆåŠŸ: {weights_path.name} ({len(weights)}æ¬¡å…ƒ)")
    
    # ç‰¹å¾´é‡åã‚’å–å¾—ï¼ˆFeatureExtractorã‹ã‚‰ï¼‰
    try:
        from omegaconf import OmegaConf

        from kazoo.src.kazoo.features.feature_extractor import FeatureExtractor

        # è¨­å®šã‚’ä½œæˆ
        cfg = OmegaConf.create({
            "features": {
                "all_labels": ["bug", "enhancement", "documentation", "question", "help wanted"],
                "label_to_skills": {
                    "bug": ["debugging", "analysis"],
                    "enhancement": ["python", "design"],
                    "documentation": ["writing"],
                    "question": ["communication"],
                    "help wanted": ["collaboration"]
                }
            },
            "irl": {"use_gat": True}
        })
        
        feature_extractor = FeatureExtractor(cfg)
        feature_names = feature_extractor.feature_names
        
        # æ¬¡å…ƒæ•°ã‚’åˆã‚ã›ã‚‹
        if len(feature_names) != len(weights):
            min_dim = min(len(feature_names), len(weights))
            feature_names = feature_names[:min_dim]
            weights = weights[:min_dim]
        
        print(f"âœ… ç‰¹å¾´é‡åå–å¾—æˆåŠŸ: {len(feature_names)}å€‹")
        
        # ã‚µãƒ³ãƒ—ãƒ«ç‰¹å¾´é‡ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆï¼ˆå®Ÿéš›ã®ãƒ‡ãƒ¼ã‚¿ãŒãªã„å ´åˆï¼‰
        n_samples = 1000
        feature_data = generate_realistic_feature_data(feature_names, n_samples)
        
        return weights, feature_names, feature_data
        
    except Exception as e:
        print(f"âš ï¸  ç‰¹å¾´é‡åå–å¾—ã«å¤±æ•—: {e}")
        return None, None, None


def generate_realistic_feature_data(feature_names, n_samples=1000):
    """ç¾å®Ÿçš„ãªç‰¹å¾´é‡ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆ"""
    n_features = len(feature_names)
    features = np.zeros((n_samples, n_features))
    
    for i, name in enumerate(feature_names):
        if "days_since" in name:
            # æŒ‡æ•°åˆ†å¸ƒ (0-365æ—¥)
            features[:, i] = np.random.exponential(30, n_samples)
            features[:, i] = np.clip(features[:, i], 0, 365)
        elif "activity" in name or "count" in name:
            # ãƒã‚¢ã‚½ãƒ³åˆ†å¸ƒ
            features[:, i] = np.random.poisson(3, n_samples)
        elif "length" in name:
            # ãƒ­ã‚°ãƒãƒ¼ãƒãƒ«åˆ†å¸ƒ
            features[:, i] = np.random.lognormal(4, 1, n_samples)
        elif "label_" in name or "collaborated" in name or "has_prior" in name:
            # ãƒã‚¤ãƒŠãƒªç‰¹å¾´é‡
            features[:, i] = np.random.binomial(1, 0.15, n_samples).astype(float)
        elif "workload" in name:
            # ã‚¬ãƒ³ãƒåˆ†å¸ƒ
            features[:, i] = np.random.gamma(2, 2, n_samples)
            features[:, i] = np.clip(features[:, i], 0, 15)
        elif "lines_changed" in name:
            # ãƒ­ã‚°ãƒãƒ¼ãƒãƒ«åˆ†å¸ƒ
            features[:, i] = np.random.lognormal(3, 1.5, n_samples)
        elif "network" in name or "collaboration" in name:
            # ã¹ãä¹—åˆ†å¸ƒ
            features[:, i] = np.random.pareto(1.16, n_samples) * 2
            features[:, i] = np.clip(features[:, i], 0, 50)
        elif "affinity" in name or "similarity" in name:
            # ãƒ™ãƒ¼ã‚¿åˆ†å¸ƒ (0-1)
            features[:, i] = np.random.beta(1, 4, n_samples)
        elif name.startswith("feature_"):
            # GATåŸ‹ã‚è¾¼ã¿: æ¨™æº–æ­£è¦åˆ†å¸ƒ
            features[:, i] = np.random.normal(0, 0.5, n_samples)
        else:
            # ãã®ä»–: è»½ã„ãƒ©ãƒ³ãƒ€ãƒ ã‚¦ã‚©ãƒ¼ã‚¯
            features[:, i] = np.random.normal(0, 0.8, n_samples)
    
    return features


def demonstrate_feature_analysis():
    """ç‰¹å¾´é‡åˆ†æã®ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³"""
    print("ğŸš€ ç‰¹å¾´é‡åˆ†æåŸºç›¤ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³")
    print("="*80)
    
    # ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿
    weights, feature_names, feature_data = load_real_irl_data()
    
    if weights is None:
        # ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨
        print("ğŸ“Š ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆä¸­...")
        n_samples, n_features = 1000, 25
        feature_names = [
            # ã‚¿ã‚¹ã‚¯ç‰¹å¾´é‡
            "task_days_since_last_activity", "task_discussion_activity", 
            "task_text_length", "task_code_block_count",
            "task_label_bug", "task_label_enhancement",
            # é–‹ç™ºè€…ç‰¹å¾´é‡
            "dev_recent_activity_count", "dev_current_workload",
            "dev_total_lines_changed", "dev_collaboration_network_size",
            "dev_comment_interactions", "dev_cross_issue_activity",
            # ãƒãƒƒãƒãƒ³ã‚°ç‰¹å¾´é‡
            "match_collaborated_with_task_author", "match_collaborator_overlap_count",
            "match_has_prior_collaboration", "match_skill_intersection_count",
            "match_file_experience_count", "match_affinity_for_bug",
            "match_affinity_for_enhancement",
            # GATç‰¹å¾´é‡
            "gat_similarity", "gat_dev_expertise"
        ]
        
        # æ®‹ã‚Šã‚’GATåŸ‹ã‚è¾¼ã¿ã§åŸ‹ã‚ã‚‹
        while len(feature_names) < n_features:
            feature_names.append(f"feature_{len(feature_names) - 21}")
        
        feature_data = generate_realistic_feature_data(feature_names, n_samples)
        weights = np.random.randn(n_features) * 0.5
    
    print(f"ğŸ“ˆ ãƒ‡ãƒ¼ã‚¿æ¦‚è¦:")
    print(f"   - ã‚µãƒ³ãƒ—ãƒ«æ•°: {feature_data.shape[0]:,}")
    print(f"   - ç‰¹å¾´é‡æ•°: {len(feature_names)}")
    print(f"   - IRLé‡ã¿ç¯„å›²: [{np.min(weights):.4f}, {np.max(weights):.4f}]")
    
    # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
    output_dir = Path("outputs")
    output_dir.mkdir(exist_ok=True)
    
    # é‡ã¿ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¸€æ™‚ä¿å­˜
    weights_path = "temp_weights.npy"
    np.save(weights_path, weights)
    
    try:
        # 1. ç‰¹å¾´é‡é‡è¦åº¦åˆ†æ
        print("\n" + "="*60)
        print("ğŸ“Š ç‰¹å¾´é‡é‡è¦åº¦åˆ†æ")
        print("="*60)
        
        importance_analyzer = FeatureImportanceAnalyzer(weights_path, feature_names)
        importance_results = importance_analyzer.analyze_importance()
        
        print("ğŸ† é‡è¦åº¦ãƒ©ãƒ³ã‚­ãƒ³ã‚° TOP10:")
        for i, (name, weight, importance) in enumerate(importance_results['importance_ranking'][:10], 1):
            print(f"  {i:2d}. {name:35s} | é‡ã¿:{weight:8.5f} | é‡è¦åº¦:{importance:8.5f}")
        
        print("\nğŸ“‹ ã‚«ãƒ†ã‚´ãƒªåˆ¥é‡è¦åº¦:")
        for category, stats in importance_results['category_importance'].items():
            print(f"  {category:20s}: å¹³å‡é‡è¦åº¦ {stats['mean_importance']:.5f} ({stats['count']}å€‹)")
        
        # ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
        importance_report = importance_analyzer.generate_importance_report(
            output_dir / "importance_analysis_report.txt"
        )
        print(f"âœ… é‡è¦åº¦åˆ†æãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ: {importance_report}")
        
        # 2. ç‰¹å¾´é‡ç›¸é–¢åˆ†æ
        print("\n" + "="*60)
        print("ğŸ”— ç‰¹å¾´é‡ç›¸é–¢åˆ†æ")
        print("="*60)
        
        correlation_analyzer = FeatureCorrelationAnalyzer(feature_data, feature_names)
        correlation_results = correlation_analyzer.analyze_correlations(correlation_threshold=0.6)
        
        corr_stats = correlation_results['correlation_statistics']
        print(f"ğŸ“Š ç›¸é–¢çµ±è¨ˆ:")
        print(f"   - å¹³å‡çµ¶å¯¾ç›¸é–¢: {corr_stats['mean_abs_correlation']:.4f}")
        print(f"   - é«˜ç›¸é–¢ãƒšã‚¢(|r|â‰¥0.6): {corr_stats['high_corr_count_06']}å€‹")
        print(f"   - ä½ç›¸é–¢ãƒšã‚¢(|r|â‰¤0.2): {corr_stats['low_corr_count_02']}å€‹")
        
        if correlation_results['high_correlation_pairs']:
            print("\nğŸ” é«˜ç›¸é–¢ãƒšã‚¢ TOP5:")
            for i, (feat1, feat2, corr) in enumerate(correlation_results['high_correlation_pairs'][:5], 1):
                print(f"  {i}. {feat1[:25]:25s} - {feat2[:25]:25s} | r={corr:6.3f}")
        
        if correlation_results['redundant_features']:
            print(f"\nâš ï¸  å†—é•·ç‰¹å¾´é‡å€™è£œ: {len(correlation_results['redundant_features'])}å€‹")
            for item in correlation_results['redundant_features'][:3]:
                print(f"   å‰Šé™¤å€™è£œ: {item['redundant_feature'][:30]:30s} (ç›¸é–¢: {item['correlation']:.3f})")
        
        # ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
        correlation_report = correlation_analyzer.generate_correlation_report(
            output_dir / "correlation_analysis_report.txt"
        )
        print(f"âœ… ç›¸é–¢åˆ†æãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ: {correlation_report}")
        
        # 3. ç‰¹å¾´é‡åˆ†å¸ƒåˆ†æ
        print("\n" + "="*60)
        print("ğŸ“ˆ ç‰¹å¾´é‡åˆ†å¸ƒåˆ†æ")
        print("="*60)
        
        distribution_analyzer = FeatureDistributionAnalyzer(feature_data, feature_names)
        distribution_results = distribution_analyzer.analyze_distributions()
        
        scale_info = distribution_results['scale_imbalance']
        print(f"âš–ï¸  ã‚¹ã‚±ãƒ¼ãƒ«ä¸å‡è¡¡:")
        print(f"   - æ¨™æº–åå·®æ¯”ï¼ˆæœ€å¤§/æœ€å°ï¼‰: {scale_info['std_ratio_max_min']:.2f}")
        print(f"   - ãƒ¬ãƒ³ã‚¸æ¯”ï¼ˆæœ€å¤§/æœ€å°ï¼‰: {scale_info['range_ratio_max_min']:.2f}")
        print(f"   - æ·±åˆ»ãªã‚¹ã‚±ãƒ¼ãƒ«ä¸å‡è¡¡: {'ã¯ã„' if scale_info['scale_imbalance_severe'] else 'ã„ã„ãˆ'}")
        
        quality_issues = distribution_results['data_quality_issues']
        print(f"\nğŸš¨ ãƒ‡ãƒ¼ã‚¿å“è³ªå•é¡Œ:")
        total_issues = sum(len(features) for features in quality_issues.values())
        if total_issues > 0:
            for issue_type, features_list in quality_issues.items():
                if features_list:
                    print(f"   - {issue_type}: {len(features_list)}å€‹")
        else:
            print("   - é‡å¤§ãªå“è³ªå•é¡Œã¯æ¤œå‡ºã•ã‚Œã¾ã›ã‚“ã§ã—ãŸ")
        
        # æ­£è¦æ€§æ¤œå®šçµæœ
        normality_tests = distribution_results['normality_tests']
        normal_count = sum(1 for test in normality_tests.values() if test.get('is_normal_shapiro', False))
        print(f"\nğŸ“Š æ­£è¦æ€§æ¤œå®š:")
        print(f"   - æ­£è¦åˆ†å¸ƒã«å¾“ã†ç‰¹å¾´é‡: {normal_count}/{len(normality_tests)}")
        
        # ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
        distribution_report = distribution_analyzer.generate_distribution_report(
            output_dir / "distribution_analysis_report.txt"
        )
        print(f"âœ… åˆ†å¸ƒåˆ†æãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ: {distribution_report}")
        
        # 4. çµ±åˆåˆ†æã‚µãƒãƒªãƒ¼
        print("\n" + "="*80)
        print("ğŸ“‹ çµ±åˆåˆ†æã‚µãƒãƒªãƒ¼")
        print("="*80)
        
        print("ğŸ¯ ä¸»è¦ãªç™ºè¦‹:")
        
        # æœ€é‡è¦ç‰¹å¾´é‡
        top_feature = importance_results['importance_ranking'][0]
        print(f"   - æœ€é‡è¦ç‰¹å¾´é‡: {top_feature[0]} (é‡è¦åº¦: {top_feature[2]:.5f})")
        
        # åŸºæœ¬ç‰¹å¾´é‡ vs GATç‰¹å¾´é‡
        comparison = importance_results['basic_vs_gat_comparison']
        if 'error' not in comparison:
            basic_importance = comparison['basic_features']['mean_importance']
            gat_importance = comparison['gat_features']['mean_importance']
            print(f"   - åŸºæœ¬ç‰¹å¾´é‡å¹³å‡é‡è¦åº¦: {basic_importance:.5f}")
            print(f"   - GATç‰¹å¾´é‡å¹³å‡é‡è¦åº¦: {gat_importance:.5f}")
            
            if basic_importance > gat_importance:
                print("   â†’ åŸºæœ¬ç‰¹å¾´é‡ã®æ–¹ãŒé‡è¦åº¦ãŒé«˜ã„å‚¾å‘")
            else:
                print("   â†’ GATç‰¹å¾´é‡ã®æ–¹ãŒé‡è¦åº¦ãŒé«˜ã„å‚¾å‘")
        
        # ç›¸é–¢å•é¡Œ
        high_corr_count = len(correlation_results['high_correlation_pairs'])
        if high_corr_count > 0:
            print(f"   - é«˜ç›¸é–¢ãƒšã‚¢: {high_corr_count}å€‹ â†’ å†—é•·æ€§ã®å¯èƒ½æ€§")
        
        # ã‚¹ã‚±ãƒ¼ãƒ«å•é¡Œ
        if scale_info['scale_imbalance_severe']:
            print("   - æ·±åˆ»ãªã‚¹ã‚±ãƒ¼ãƒ«ä¸å‡è¡¡ â†’ æ­£è¦åŒ–ãŒå¿…è¦")
        
        print("\nğŸ’¡ æ¨å¥¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³:")
        
        # å†—é•·ç‰¹å¾´é‡ã®å‰Šé™¤
        if correlation_results['redundant_features']:
            print(f"   1. {len(correlation_results['redundant_features'])}å€‹ã®å†—é•·ç‰¹å¾´é‡å€™è£œã®å‰Šé™¤ã‚’æ¤œè¨")
        
        # ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°
        if scale_info['std_ratio_max_min'] > 10:
            print("   2. ç‰¹å¾´é‡ã®æ¨™æº–åŒ–ã¾ãŸã¯min-maxæ­£è¦åŒ–ã‚’å®Ÿæ–½")
        
        # åˆ†å¸ƒã®æ”¹å–„
        extreme_skew_count = sum(1 for stats in distribution_results['distribution_stats'].values() 
                               if abs(stats.get('skewness', 0)) > 2)
        if extreme_skew_count > 0:
            print(f"   3. {extreme_skew_count}å€‹ã®æ¥µç«¯ã«æ­ªã‚“ã ç‰¹å¾´é‡ã®å¯¾æ•°å¤‰æ›ã‚’æ¤œè¨")
        
        print(f"\nğŸ“ ç”Ÿæˆã•ã‚ŒãŸãƒ¬ãƒãƒ¼ãƒˆ:")
        print(f"   - {importance_report}")
        print(f"   - {correlation_report}")
        print(f"   - {distribution_report}")
        
        print("\nğŸ‰ ç‰¹å¾´é‡åˆ†æåŸºç›¤ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³å®Œäº†ï¼")
        
    finally:
        # ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
        Path(weights_path).unlink(missing_ok=True)


if __name__ == "__main__":
    demonstrate_feature_analysis()