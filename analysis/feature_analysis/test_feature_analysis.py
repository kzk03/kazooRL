#!/usr/bin/env python3
"""
ç‰¹å¾´é‡åˆ†æåŸºç›¤ã®ãƒ†ã‚¹ãƒˆã‚¹ã‚¯ãƒªãƒ—ãƒˆ
==============================

å®Ÿè£…ã—ãŸç‰¹å¾´é‡åˆ†æã‚¯ãƒ©ã‚¹ã®å‹•ä½œç¢ºèªã‚’è¡Œã„ã¾ã™ã€‚
"""

import sys
from pathlib import Path

import numpy as np

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’è¿½åŠ 
project_root = Path(__file__).resolve().parents[3]
sys.path.append(str(project_root))

from kazoo.analysis.feature_analysis import (
    FeatureCorrelationAnalyzer,
    FeatureDistributionAnalyzer,
    FeatureImportanceAnalyzer,
)


def generate_test_data(n_samples=1000, n_features=25):
    """ãƒ†ã‚¹ãƒˆç”¨ã®ç‰¹å¾´é‡ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆ"""
    print(f"ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ç”Ÿæˆä¸­... ({n_samples}ã‚µãƒ³ãƒ—ãƒ«, {n_features}ç‰¹å¾´é‡)")
    
    # ç‰¹å¾´é‡åã‚’å®šç¾©
    feature_names = [
        # ã‚¿ã‚¹ã‚¯ç‰¹å¾´é‡
        "task_days_since_last_activity",
        "task_discussion_activity", 
        "task_text_length",
        "task_code_block_count",
        "task_label_bug",
        "task_label_enhancement",
        # é–‹ç™ºè€…ç‰¹å¾´é‡
        "dev_recent_activity_count",
        "dev_current_workload",
        "dev_total_lines_changed",
        "dev_collaboration_network_size",
        "dev_comment_interactions",
        "dev_cross_issue_activity",
        # ãƒãƒƒãƒãƒ³ã‚°ç‰¹å¾´é‡
        "match_collaborated_with_task_author",
        "match_collaborator_overlap_count",
        "match_has_prior_collaboration",
        "match_skill_intersection_count",
        "match_file_experience_count",
        "match_affinity_for_bug",
        "match_affinity_for_enhancement",
        # GATç‰¹å¾´é‡
        "gat_similarity",
        "gat_dev_expertise",
        "feature_0",
        "feature_1", 
        "feature_2",
        "feature_3"
    ]
    
    # å®Ÿéš›ã®ç‰¹å¾´é‡æ•°ã«åˆã‚ã›ã‚‹
    if len(feature_names) > n_features:
        feature_names = feature_names[:n_features]
    elif len(feature_names) < n_features:
        for i in range(len(feature_names), n_features):
            feature_names.append(f"feature_{i}")
    
    # ç¾å®Ÿçš„ãªç‰¹å¾´é‡ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆ
    features = np.zeros((n_samples, n_features))
    
    for i, name in enumerate(feature_names):
        if "days_since" in name:
            # æŒ‡æ•°åˆ†å¸ƒ (0-365æ—¥)
            features[:, i] = np.random.exponential(30, n_samples)
            features[:, i] = np.clip(features[:, i], 0, 365)
        elif "activity" in name or "count" in name:
            # ãƒã‚¢ã‚½ãƒ³åˆ†å¸ƒ
            features[:, i] = np.random.poisson(3, n_samples)
        elif "length" in name:
            # ãƒ­ã‚°ãƒãƒ¼ãƒãƒ«åˆ†å¸ƒ
            features[:, i] = np.random.lognormal(4, 1, n_samples)
        elif "label_" in name or "collaborated" in name or "has_prior" in name:
            # ãƒã‚¤ãƒŠãƒªç‰¹å¾´é‡
            features[:, i] = np.random.binomial(1, 0.15, n_samples).astype(float)
        elif "workload" in name:
            # ã‚¬ãƒ³ãƒåˆ†å¸ƒ
            features[:, i] = np.random.gamma(2, 2, n_samples)
            features[:, i] = np.clip(features[:, i], 0, 15)
        elif "lines_changed" in name:
            # ãƒ­ã‚°ãƒãƒ¼ãƒãƒ«åˆ†å¸ƒ
            features[:, i] = np.random.lognormal(3, 1.5, n_samples)
        elif "network" in name or "collaboration" in name:
            # ã¹ãä¹—åˆ†å¸ƒ
            features[:, i] = np.random.pareto(1.16, n_samples) * 2
            features[:, i] = np.clip(features[:, i], 0, 50)
        elif "affinity" in name or "similarity" in name:
            # ãƒ™ãƒ¼ã‚¿åˆ†å¸ƒ (0-1)
            features[:, i] = np.random.beta(1, 4, n_samples)
        elif name.startswith("feature_"):
            # GATåŸ‹ã‚è¾¼ã¿: æ¨™æº–æ­£è¦åˆ†å¸ƒ
            features[:, i] = np.random.normal(0, 0.5, n_samples)
        else:
            # ãã®ä»–: è»½ã„ãƒ©ãƒ³ãƒ€ãƒ ã‚¦ã‚©ãƒ¼ã‚¯
            features[:, i] = np.random.normal(0, 0.8, n_samples)
    
    # IRLé‡ã¿ã‚’ç”Ÿæˆ
    weights = np.random.randn(n_features) * 0.5
    
    print("âœ… ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ç”Ÿæˆå®Œäº†")
    return features, feature_names, weights


def test_feature_importance_analyzer():
    """FeatureImportanceAnalyzerã®ãƒ†ã‚¹ãƒˆ"""
    print("\n" + "="*60)
    print("ğŸ§ª FeatureImportanceAnalyzer ãƒ†ã‚¹ãƒˆé–‹å§‹")
    print("="*60)
    
    # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
    features, feature_names, weights = generate_test_data()
    
    # é‡ã¿ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¸€æ™‚ä¿å­˜
    weights_path = "test_weights.npy"
    np.save(weights_path, weights)
    
    try:
        # ã‚¢ãƒŠãƒ©ã‚¤ã‚¶ãƒ¼åˆæœŸåŒ–
        analyzer = FeatureImportanceAnalyzer(weights_path, feature_names)
        
        # é‡è¦åº¦åˆ†æå®Ÿè¡Œ
        results = analyzer.analyze_importance()
        
        print("âœ… é‡è¦åº¦åˆ†æå®Œäº†")
        print(f"   - é‡è¦åº¦ãƒ©ãƒ³ã‚­ãƒ³ã‚°: {len(results['importance_ranking'])}é …ç›®")
        print(f"   - ã‚«ãƒ†ã‚´ãƒªåˆ¥åˆ†æ: {len(results['category_importance'])}ã‚«ãƒ†ã‚´ãƒª")
        print(f"   - çµ±è¨ˆçš„æœ‰æ„æ€§: {len(results['statistical_significance'])}é …ç›®")
        
        # TOP5ã‚’è¡¨ç¤º
        print("\né‡è¦åº¦TOP5:")
        for i, (name, weight, importance) in enumerate(results['importance_ranking'][:5], 1):
            print(f"  {i}. {name:30s} | é‡ã¿:{weight:7.4f} | é‡è¦åº¦:{importance:7.4f}")
        
        # ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆãƒ†ã‚¹ãƒˆ
        report_path = analyzer.generate_importance_report("test_importance_report.txt")
        print(f"âœ… ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆå®Œäº†: {report_path}")
        
        # å¯è¦–åŒ–ãƒ†ã‚¹ãƒˆ
        fig_path = analyzer.visualize_importance("outputs", show_plot=False)
        print(f"âœ… å¯è¦–åŒ–å®Œäº†: {fig_path}")
        
    finally:
        # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤
        Path(weights_path).unlink(missing_ok=True)
    
    print("âœ… FeatureImportanceAnalyzer ãƒ†ã‚¹ãƒˆå®Œäº†")


def test_feature_correlation_analyzer():
    """FeatureCorrelationAnalyzerã®ãƒ†ã‚¹ãƒˆ"""
    print("\n" + "="*60)
    print("ğŸ§ª FeatureCorrelationAnalyzer ãƒ†ã‚¹ãƒˆé–‹å§‹")
    print("="*60)
    
    # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
    features, feature_names, _ = generate_test_data()
    
    # ã‚¢ãƒŠãƒ©ã‚¤ã‚¶ãƒ¼åˆæœŸåŒ–
    analyzer = FeatureCorrelationAnalyzer(features, feature_names)
    
    # ç›¸é–¢åˆ†æå®Ÿè¡Œ
    results = analyzer.analyze_correlations(correlation_threshold=0.3)  # é–¾å€¤ã‚’ä¸‹ã’ã¦ãƒ†ã‚¹ãƒˆ
    
    print("âœ… ç›¸é–¢åˆ†æå®Œäº†")
    print(f"   - ç›¸é–¢è¡Œåˆ—: {results['correlation_matrix'].shape}")
    print(f"   - é«˜ç›¸é–¢ãƒšã‚¢: {len(results['high_correlation_pairs'])}ãƒšã‚¢")
    print(f"   - å†—é•·ç‰¹å¾´é‡å€™è£œ: {len(results['redundant_features'])}å€‹")
    print(f"   - ã‚«ãƒ†ã‚´ãƒªé–“ç›¸é–¢: {len(results['category_correlations'])}é …ç›®")
    
    # é«˜ç›¸é–¢ãƒšã‚¢TOP5ã‚’è¡¨ç¤º
    if results['high_correlation_pairs']:
        print("\né«˜ç›¸é–¢ãƒšã‚¢TOP5:")
        for i, (feat1, feat2, corr) in enumerate(results['high_correlation_pairs'][:5], 1):
            print(f"  {i}. {feat1[:20]:20s} - {feat2[:20]:20s} | r={corr:6.3f}")
    
    # ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆãƒ†ã‚¹ãƒˆ
    report_path = analyzer.generate_correlation_report("test_correlation_report.txt")
    print(f"âœ… ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆå®Œäº†: {report_path}")
    
    # å¯è¦–åŒ–ãƒ†ã‚¹ãƒˆ
    fig_path = analyzer.visualize_correlations("outputs", show_plot=False)
    print(f"âœ… å¯è¦–åŒ–å®Œäº†: {fig_path}")
    
    print("âœ… FeatureCorrelationAnalyzer ãƒ†ã‚¹ãƒˆå®Œäº†")


def test_feature_distribution_analyzer():
    """FeatureDistributionAnalyzerã®ãƒ†ã‚¹ãƒˆ"""
    print("\n" + "="*60)
    print("ğŸ§ª FeatureDistributionAnalyzer ãƒ†ã‚¹ãƒˆé–‹å§‹")
    print("="*60)
    
    # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
    features, feature_names, _ = generate_test_data()
    
    # ã‚¢ãƒŠãƒ©ã‚¤ã‚¶ãƒ¼åˆæœŸåŒ–
    analyzer = FeatureDistributionAnalyzer(features, feature_names)
    
    # åˆ†å¸ƒåˆ†æå®Ÿè¡Œ
    results = analyzer.analyze_distributions()
    
    print("âœ… åˆ†å¸ƒåˆ†æå®Œäº†")
    print(f"   - åˆ†å¸ƒçµ±è¨ˆ: {len(results['distribution_stats'])}ç‰¹å¾´é‡")
    print(f"   - æ­£è¦æ€§æ¤œå®š: {len(results['normality_tests'])}ç‰¹å¾´é‡")
    print(f"   - å¤–ã‚Œå€¤æ¤œå‡º: {len(results['outlier_detection'])}ç‰¹å¾´é‡")
    print(f"   - ã‚«ãƒ†ã‚´ãƒªåˆ¥åˆ†å¸ƒ: {len(results['category_distributions'])}ã‚«ãƒ†ã‚´ãƒª")
    
    # ãƒ‡ãƒ¼ã‚¿å“è³ªå•é¡Œã‚’è¡¨ç¤º
    quality_issues = results['data_quality_issues']
    print("\nãƒ‡ãƒ¼ã‚¿å“è³ªå•é¡Œ:")
    for issue_type, features_list in quality_issues.items():
        if features_list:
            print(f"  - {issue_type}: {len(features_list)}å€‹")
    
    # ã‚¹ã‚±ãƒ¼ãƒ«ä¸å‡è¡¡ã‚’è¡¨ç¤º
    scale_imbalance = results['scale_imbalance']
    print(f"\nã‚¹ã‚±ãƒ¼ãƒ«ä¸å‡è¡¡:")
    print(f"  - æ¨™æº–åå·®æ¯”ï¼ˆæœ€å¤§/æœ€å°ï¼‰: {scale_imbalance['std_ratio_max_min']:.2f}")
    print(f"  - ãƒ¬ãƒ³ã‚¸æ¯”ï¼ˆæœ€å¤§/æœ€å°ï¼‰: {scale_imbalance['range_ratio_max_min']:.2f}")
    
    # ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆãƒ†ã‚¹ãƒˆ
    report_path = analyzer.generate_distribution_report("test_distribution_report.txt")
    print(f"âœ… ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆå®Œäº†: {report_path}")
    
    # å¯è¦–åŒ–ãƒ†ã‚¹ãƒˆ
    fig_path = analyzer.visualize_distributions("outputs", show_plot=False, max_features=12)
    print(f"âœ… å¯è¦–åŒ–å®Œäº†: {fig_path}")
    
    print("âœ… FeatureDistributionAnalyzer ãƒ†ã‚¹ãƒˆå®Œäº†")


def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    print("ğŸš€ ç‰¹å¾´é‡åˆ†æåŸºç›¤ãƒ†ã‚¹ãƒˆé–‹å§‹")
    print("="*80)
    
    # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
    Path("outputs").mkdir(exist_ok=True)
    
    try:
        # å„ã‚¢ãƒŠãƒ©ã‚¤ã‚¶ãƒ¼ã®ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
        test_feature_importance_analyzer()
        test_feature_correlation_analyzer()
        test_feature_distribution_analyzer()
        
        print("\n" + "="*80)
        print("ğŸ‰ å…¨ãƒ†ã‚¹ãƒˆå®Œäº†ï¼")
        print("="*80)
        
        print("\nç”Ÿæˆã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«:")
        for file_path in Path(".").glob("test_*.txt"):
            print(f"  - {file_path}")
        
        output_dir = Path("outputs")
        if output_dir.exists():
            for file_path in output_dir.glob("*.png"):
                print(f"  - {file_path}")
        
    except Exception as e:
        print(f"\nâŒ ãƒ†ã‚¹ãƒˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        import traceback
        traceback.print_exc()
        return 1
    
    return 0


if __name__ == "__main__":
    exit(main())