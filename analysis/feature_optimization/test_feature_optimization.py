#!/usr/bin/env python3
"""
ç‰¹å¾´é‡æœ€é©åŒ–ã®å®Ÿè£…ç¢ºèª
====================

å®Ÿè£…ã—ãŸç‰¹å¾´é‡æœ€é©åŒ–ã‚¯ãƒ©ã‚¹ãŒæ­£ã—ãå‹•ä½œã™ã‚‹ã‹ã‚’ç¢ºèªã—ã¾ã™ã€‚
"""

import sys
from pathlib import Path

import numpy as np
import pandas as pd

# ç¾åœ¨ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’è¿½åŠ 
sys.path.append(str(Path(__file__).parent))


def generate_test_data(n_samples=1000, n_features=50):
    """ãƒ†ã‚¹ãƒˆç”¨ã®ç‰¹å¾´é‡ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆ"""
    print(f"ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ç”Ÿæˆä¸­... ({n_samples}ã‚µãƒ³ãƒ—ãƒ«, {n_features}ç‰¹å¾´é‡)")

    np.random.seed(42)

    # æ§˜ã€…ãªã‚¿ã‚¤ãƒ—ã®ç‰¹å¾´é‡ã‚’ç”Ÿæˆ
    features = []
    feature_names = []

    # 1. æ•°å€¤ç‰¹å¾´é‡ï¼ˆæ­£è¦åˆ†å¸ƒï¼‰
    for i in range(15):
        features.append(np.random.normal(0, 1, n_samples))
        feature_names.append(f"numerical_{i}")

    # 2. æ­ªã‚“ã æ•°å€¤ç‰¹å¾´é‡ï¼ˆå¯¾æ•°æ­£è¦åˆ†å¸ƒï¼‰
    for i in range(10):
        features.append(np.random.lognormal(0, 1, n_samples))
        feature_names.append(f"skewed_{i}")

    # 3. ãƒã‚¤ãƒŠãƒªç‰¹å¾´é‡
    for i in range(8):
        features.append(np.random.binomial(1, 0.3, n_samples).astype(float))
        feature_names.append(f"binary_{i}")

    # 4. ã‚«ãƒ†ã‚´ãƒªã‚«ãƒ«ç‰¹å¾´é‡ï¼ˆæ•°å€¤ã§ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ï¼‰
    for i in range(5):
        categories = np.random.choice(
            [0, 1, 2, 3], n_samples, p=[0.4, 0.3, 0.2, 0.1]
        ).astype(float)
        features.append(categories)
        feature_names.append(f"categorical_{i}")

    # 5. æ™‚ç³»åˆ—ç‰¹å¾´é‡
    for i in range(7):
        time_values = np.random.uniform(0, 365, n_samples)  # æ—¥æ•°
        features.append(time_values)
        feature_names.append(f"time_since_{i}")

    # 6. å¤–ã‚Œå€¤ã‚’å«ã‚€ç‰¹å¾´é‡
    for i in range(5):
        base_values = np.random.normal(0, 1, n_samples)
        # 5%ã®å¤–ã‚Œå€¤ã‚’è¿½åŠ 
        outlier_mask = np.random.random(n_samples) < 0.05
        base_values[outlier_mask] += np.random.normal(0, 10, np.sum(outlier_mask))
        features.append(base_values)
        feature_names.append(f"outlier_prone_{i}")

    # ãƒ‡ãƒ¼ã‚¿ã‚’çµåˆ
    X = np.column_stack(features)

    # ç›®çš„å¤‰æ•°ã‚’ç”Ÿæˆï¼ˆåˆ†é¡ç”¨ï¼‰
    # æœ€åˆã®10å€‹ã®ç‰¹å¾´é‡ã«åŸºã¥ã„ã¦ç›®çš„å¤‰æ•°ã‚’ç”Ÿæˆ
    y_continuous = (
        X[:, 0] * 0.5
        + X[:, 1] * 0.3
        + X[:, 2] * 0.2
        + np.random.normal(0, 0.1, n_samples)
    )
    y_binary = (y_continuous > np.median(y_continuous)).astype(int)

    print("âœ… ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ç”Ÿæˆå®Œäº†")
    return X, y_binary, y_continuous, feature_names


def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    print("ğŸš€ ç‰¹å¾´é‡æœ€é©åŒ–å®Ÿè£…ç¢ºèªé–‹å§‹")
    print("=" * 60)

    # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
    X, y_binary, y_continuous, feature_names = generate_test_data()

    try:
        # 1. FeatureScaler ã®ãƒ†ã‚¹ãƒˆ
        print("\nâš–ï¸ FeatureScaler ãƒ†ã‚¹ãƒˆ")
        print("-" * 40)

        from feature_scaler import FeatureScaler

        scaler = FeatureScaler()
        print("âœ… åˆæœŸåŒ–æˆåŠŸ")

        # ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã®å­¦ç¿’ã¨é©ç”¨
        X_scaled = scaler.fit_transform(X, feature_names)
        print(f"âœ… ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°å®Œäº†: {X.shape} â†’ {X_scaled.shape}")

        # ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°æƒ…å ±ã®å–å¾—
        scaling_info = scaler.get_scaling_info()
        print(f"   - å…¥åŠ›ç‰¹å¾´é‡æ•°: {scaling_info['n_features_in']}")
        print(f"   - å‡ºåŠ›ç‰¹å¾´é‡æ•°: {scaling_info['n_features_out']}")
        print(f"   - æˆ¦ç•¥çµ±è¨ˆ: {scaling_info['strategy_counts']}")

        # å¤‰æ›å¾Œã®ç‰¹å¾´é‡å
        feature_names_out = scaler.get_feature_names_out()
        print(f"   - å¤‰æ›å¾Œç‰¹å¾´é‡åä¾‹: {feature_names_out[:5]}...")

        # 2. FeatureSelector ã®ãƒ†ã‚¹ãƒˆ
        print("\nğŸ¯ FeatureSelector ãƒ†ã‚¹ãƒˆ")
        print("-" * 40)

        from feature_selector import FeatureSelector

        selector = FeatureSelector()
        print("âœ… åˆæœŸåŒ–æˆåŠŸ")

        # ç‰¹å¾´é‡é¸æŠã®å­¦ç¿’ã¨é©ç”¨ï¼ˆåˆ†é¡ã‚¿ã‚¹ã‚¯ï¼‰
        X_selected = selector.fit_transform(
            X_scaled,
            y_binary,
            feature_names=feature_names_out,
            methods=["univariate", "rfe", "importance_based"],
        )
        print(f"âœ… ç‰¹å¾´é‡é¸æŠå®Œäº†: {X_scaled.shape} â†’ {X_selected.shape}")

        # é¸æŠçµæœã®å–å¾—
        selection_summary = selector.get_selection_summary()
        print(f"   - å…ƒç‰¹å¾´é‡æ•°: {selection_summary['n_features_original']}")
        print(f"   - ä½¿ç”¨æ‰‹æ³•: {selection_summary['methods_used']}")

        for method, result in selection_summary["selection_results"].items():
            print(
                f"   - {method}: {result['n_selected']}å€‹é¸æŠ ({result['selection_ratio']:.1%})"
            )

        # é¸æŠã•ã‚ŒãŸç‰¹å¾´é‡å
        selected_names = selector.get_selected_feature_names("ensemble")
        print(f"   - ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«é¸æŠ: {len(selected_names)}å€‹")
        print(f"   - é¸æŠç‰¹å¾´é‡ä¾‹: {selected_names[:5]}...")

        # 3. DimensionReducer ã®ãƒ†ã‚¹ãƒˆ
        print("\nğŸ“‰ DimensionReducer ãƒ†ã‚¹ãƒˆ")
        print("-" * 40)

        from dimension_reducer import DimensionReducer

        reducer = DimensionReducer()
        print("âœ… åˆæœŸåŒ–æˆåŠŸ")

        # æ¬¡å…ƒå‰Šæ¸›ã®å­¦ç¿’ã¨é©ç”¨
        X_reduced = reducer.fit_transform(
            X_selected, feature_names=selected_names, methods=["pca", "truncated_svd"]
        )
        print(f"âœ… æ¬¡å…ƒå‰Šæ¸›å®Œäº†: {X_selected.shape} â†’ {X_reduced.shape}")

        # å‰Šæ¸›çµæœã®å–å¾—
        reduction_summary = reducer.get_reduction_summary()
        print(f"   - å…ƒç‰¹å¾´é‡æ•°: {reduction_summary['n_features_original']}")
        print(f"   - ä½¿ç”¨æ‰‹æ³•: {reduction_summary['methods_used']}")

        for method, result in reduction_summary["reduction_results"].items():
            print(
                f"   - {method}: {result['n_components']}æ¬¡å…ƒ (å‰Šæ¸›ç‡: {1-result['reduction_ratio']:.1%})"
            )
            if "variance_explained" in result:
                print(f"     åˆ†æ•£èª¬æ˜ç‡: {result['variance_explained']:.1%}")

        # PCAã®æˆåˆ†è§£é‡ˆ
        if "pca" in reduction_summary["methods_used"]:
            try:
                interpretation = reducer.get_component_interpretation(
                    "pca", n_top_features=3
                )
                print(f"   - PCAæˆåˆ†è§£é‡ˆ:")
                for component, features in list(interpretation.items())[
                    :2
                ]:  # æœ€åˆã®2æˆåˆ†
                    print(f"     {component}: {[f[0][:15] for f in features]}")
            except Exception as e:
                print(f"   - æˆåˆ†è§£é‡ˆã‚¨ãƒ©ãƒ¼: {e}")

        # 4. çµ±åˆãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ãƒ†ã‚¹ãƒˆ
        print("\nğŸ”„ çµ±åˆãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ãƒ†ã‚¹ãƒˆ")
        print("-" * 40)

        # æ–°ã—ã„ãƒ‡ãƒ¼ã‚¿ã§ã®å¤‰æ›ãƒ†ã‚¹ãƒˆ
        X_test, _, _, _ = generate_test_data(n_samples=100, n_features=50)

        # ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³é©ç”¨
        X_test_scaled = scaler.transform(X_test)
        X_test_selected = selector.transform(X_test_scaled, method="ensemble")
        X_test_reduced = reducer.transform(X_test_selected, method="pca")

        print(f"âœ… ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å¤‰æ›å®Œäº†:")
        print(f"   - å…ƒãƒ‡ãƒ¼ã‚¿: {X_test.shape}")
        print(f"   - ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°å¾Œ: {X_test_scaled.shape}")
        print(f"   - ç‰¹å¾´é‡é¸æŠå¾Œ: {X_test_selected.shape}")
        print(f"   - æ¬¡å…ƒå‰Šæ¸›å¾Œ: {X_test_reduced.shape}")

        # 5. ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆãƒ†ã‚¹ãƒˆ
        print("\nğŸ“Š ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆãƒ†ã‚¹ãƒˆ")
        print("-" * 40)

        # å„ã‚¯ãƒ©ã‚¹ã®ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
        scaling_report = scaler.generate_scaling_report("test_scaling_report.txt")
        selection_report = selector.generate_selection_report(
            "test_selection_report.txt"
        )
        reduction_report = reducer.generate_reduction_report(
            "test_reduction_report.txt"
        )

        print(f"âœ… ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆå®Œäº†:")
        print(f"   - ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°: {scaling_report}")
        print(f"   - ç‰¹å¾´é‡é¸æŠ: {selection_report}")
        print(f"   - æ¬¡å…ƒå‰Šæ¸›: {reduction_report}")

        # 6. ä¿å­˜ãƒ»èª­ã¿è¾¼ã¿ãƒ†ã‚¹ãƒˆ
        print("\nğŸ’¾ ä¿å­˜ãƒ»èª­ã¿è¾¼ã¿ãƒ†ã‚¹ãƒˆ")
        print("-" * 40)

        # ä¿å­˜
        scaler.save("test_scaler.pkl")
        selector.save("test_selector.pkl")
        reducer.save("test_reducer.pkl")
        print("âœ… ä¿å­˜å®Œäº†")

        # èª­ã¿è¾¼ã¿
        scaler_loaded = FeatureScaler.load("test_scaler.pkl")
        selector_loaded = FeatureSelector.load("test_selector.pkl")
        reducer_loaded = DimensionReducer.load("test_reducer.pkl")
        print("âœ… èª­ã¿è¾¼ã¿å®Œäº†")

        # èª­ã¿è¾¼ã¿å¾Œã®å¤‰æ›ãƒ†ã‚¹ãƒˆ
        X_test_scaled_loaded = scaler_loaded.transform(X_test)
        X_test_selected_loaded = selector_loaded.transform(
            X_test_scaled_loaded, method="ensemble"
        )
        X_test_reduced_loaded = reducer_loaded.transform(
            X_test_selected_loaded, method="pca"
        )

        # çµæœã®ä¸€è‡´ç¢ºèª
        scaling_match = np.allclose(X_test_scaled, X_test_scaled_loaded, rtol=1e-10)
        selection_match = np.allclose(
            X_test_selected, X_test_selected_loaded, rtol=1e-10
        )
        reduction_match = np.allclose(X_test_reduced, X_test_reduced_loaded, rtol=1e-10)

        print(f"âœ… å¤‰æ›çµæœä¸€è‡´ç¢ºèª:")
        print(f"   - ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°: {'âœ“' if scaling_match else 'âœ—'}")
        print(f"   - ç‰¹å¾´é‡é¸æŠ: {'âœ“' if selection_match else 'âœ—'}")
        print(f"   - æ¬¡å…ƒå‰Šæ¸›: {'âœ“' if reduction_match else 'âœ—'}")

        print("\nğŸ‰ å…¨ã¦ã®å®Ÿè£…ç¢ºèªå®Œäº†ï¼")
        print("=" * 60)
        print("âœ… FeatureScaler: å‹•ä½œç¢ºèªæ¸ˆã¿")
        print("âœ… FeatureSelector: å‹•ä½œç¢ºèªæ¸ˆã¿")
        print("âœ… DimensionReducer: å‹•ä½œç¢ºèªæ¸ˆã¿")
        print("\nğŸ“ è¦ä»¶3.1, 3.2, 3.3ã®å®Ÿè£…ãŒå®Œäº†ã—ã¾ã—ãŸã€‚")

        # çµ±è¨ˆã‚µãƒãƒªãƒ¼
        print(f"\nğŸ“Š æœ€é©åŒ–çµ±è¨ˆ:")
        print(f"   - å…ƒç‰¹å¾´é‡æ•°: {X.shape[1]}")
        print(f"   - ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°å¾Œ: {X_scaled.shape[1]}")
        print(f"   - ç‰¹å¾´é‡é¸æŠå¾Œ: {X_selected.shape[1]}")
        print(f"   - æ¬¡å…ƒå‰Šæ¸›å¾Œ: {X_reduced.shape[1]}")
        print(f"   - ç·å‰Šæ¸›ç‡: {1 - X_reduced.shape[1] / X.shape[1]:.1%}")

        # ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
        for file in [
            "test_scaler.pkl",
            "test_selector.pkl",
            "test_reducer.pkl",
            "test_scaling_report.txt",
            "test_selection_report.txt",
            "test_reduction_report.txt",
        ]:
            try:
                Path(file).unlink()
            except:
                pass

        return True

    except Exception as e:
        print(f"\nâŒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        import traceback

        traceback.print_exc()
        return False


if __name__ == "__main__":
    success = main()
    exit(0 if success else 1)
