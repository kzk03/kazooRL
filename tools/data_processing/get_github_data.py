import gzip
import json
import os
import time
from datetime import datetime, timedelta

import requests

# --- è¨­å®š ---
REPO_NAME = "docker/compose"
START_DATE = datetime(2020, 8, 21)
END_DATE = datetime(2025, 6, 19)
OUTPUT_FILE_BASENAME = "gharchive_docker_compose_events"
OUTPUT_DIR = "./results"

RETRY_WAIT_SECONDS = 5  # æœ€åˆã®å†è©¦è¡Œå¾…ã¡æ™‚é–“
MAX_RETRY_WAIT_SECONDS = 300 # æœ€å¤§å†è©¦è¡Œå¾…ã¡æ™‚é–“ (5åˆ†)
MAX_RETRIES = 5 # æœ€å¤§ãƒªãƒˆãƒ©ã‚¤å›æ•°

# GitHub APIã®ãƒ¬ãƒ¼ãƒˆãƒªãƒŸãƒƒãƒˆé–¢é€£ãƒ˜ãƒƒãƒ€ãƒ¼ (GitHub Archiveã«ã¯ç›´æ¥é–¢ä¿‚ãªã„ãŒã€APIåˆ©ç”¨æ™‚ã®å‚è€ƒã¨ã—ã¦)
# X-RateLimit-Limit: ãƒªã‚¯ã‚¨ã‚¹ãƒˆä¸Šé™
# X-RateLimit-Remaining: æ®‹ã‚Šãƒªã‚¯ã‚¨ã‚¹ãƒˆæ•°
# X-RateLimit-Reset: ãƒªã‚»ãƒƒãƒˆã•ã‚Œã‚‹UNIXã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—
# --- ---

FAILED_LOG_PATH = os.path.join(OUTPUT_DIR, f"{OUTPUT_FILE_BASENAME}_failed_downloads.log")

def log_failed_download(date, hour, url, reason):
    """ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¤±æ•—ã—ãŸURLã¨ç†ç”±ã‚’ãƒ­ã‚°ã«è¨˜éŒ²ã™ã‚‹"""
    with open(FAILED_LOG_PATH, 'a', encoding='utf-8') as f:
        timestamp = f"{date.strftime('%Y-%m-%d')}-{hour:02d}"
        f.write(f"{timestamp}\t{url}\t{reason}\n")

def download_and_filter_archive(output_basename):
    """GitHub Archiveã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã€æŒ‡å®šãƒªãƒã‚¸ãƒˆãƒªã®ã‚¤ãƒ™ãƒ³ãƒˆã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã—ã¦ä¿å­˜ã™ã‚‹"""
    os.makedirs(OUTPUT_DIR, exist_ok=True)
    
    current_date = START_DATE
    output_file_handle = None
    current_file_month = None

    try:
        while current_date <= END_DATE:
            year_month = current_date.strftime("%Y-%m")

            # æœˆãŒå¤‰ã‚ã£ãŸã‚‰æ–°ã—ã„å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ã‚’é–‹ã
            if year_month != current_file_month:
                if output_file_handle:
                    output_file_handle.close()
                    print(f"\nâœ… {current_file_month} ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜ã—ã¾ã—ãŸã€‚")
                
                current_file_month = year_month
                output_filename = os.path.join(OUTPUT_DIR, f"{output_basename}_{current_file_month}.jsonl")
                print(f"\n>>>> {current_file_month} ã®ãƒ‡ãƒ¼ã‚¿ã‚’å‡¦ç†ã—ã¾ã™ã€‚ä¿å­˜å…ˆ: {output_filename} <<<<\n")
                output_file_handle = open(output_filename, 'a', encoding='utf-8') # 'a' ã§è¿½è¨˜ãƒ¢ãƒ¼ãƒ‰ã«å¤‰æ›´

            for hour in range(24):
                url = f"https://data.gharchive.org/{current_date.strftime('%Y-%m-%d')}-{hour}.json.gz"
                print(f"ğŸ”— {current_date.strftime('%Y-%m-%d')} {hour:02d}æ™‚ ã®ãƒ‡ãƒ¼ã‚¿å–å¾—é–‹å§‹ â†’ {url}")
                
                attempt = 0
                current_wait_time = RETRY_WAIT_SECONDS
                download_successful = False # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰æˆåŠŸãƒ•ãƒ©ã‚°

                while attempt < MAX_RETRIES:
                    try:
                        response = requests.get(url, stream=True, timeout=30) # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã‚’è¿½åŠ 
                        response.raise_for_status() # 2xx ä»¥å¤–ã®å ´åˆã€HTTPError ã‚’ç™ºç”Ÿã•ã›ã‚‹
                        
                        with gzip.GzipFile(fileobj=response.raw) as gz:
                            for line in gz:
                                try:
                                    event = json.loads(line)
                                    if event.get('repo', {}).get('name') == REPO_NAME:
                                        output_file_handle.write(json.dumps(event) + '\n')
                                except json.JSONDecodeError:
                                    # JSONãƒ‡ã‚³ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼ã¯ã‚¹ã‚­ãƒƒãƒ—ã—ã€ãƒ­ã‚°ã«è¨˜éŒ²ã—ãªã„ï¼ˆãƒ‡ãƒ¼ã‚¿ãŒå£Šã‚Œã¦ã„ã‚‹å¯èƒ½æ€§ï¼‰
                                    continue
                        
                        print(f"âœ… æˆåŠŸ: {url}")
                        download_successful = True
                        break # æˆåŠŸã—ãŸã®ã§ãƒ«ãƒ¼ãƒ—ã‚’æŠœã‘ã‚‹
                    
                    except requests.exceptions.HTTPError as e:
                        if e.response.status_code == 404:
                            # 404 Not Found ã®å ´åˆ
                            print(f"âš ï¸ URLãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ (404)ã€‚ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™: {url}")
                            log_failed_download(current_date, hour, url, "Not Found (404)")
                            download_successful = True # è¦‹ã¤ã‹ã‚‰ãªã‹ã£ãŸãŒã€ã‚¨ãƒ©ãƒ¼ã¨ã—ã¦ãƒªãƒˆãƒ©ã‚¤ã¯ã—ãªã„
                            break # ã‚¹ã‚­ãƒƒãƒ—ã—ã¦æ¬¡ã®æ™‚é–“ã¸
                        elif e.response.status_code in [403, 429]:
                            # 403 Forbidden (APIåˆ¶é™ãªã©)ã€429 Too Many Requests (ãƒ¬ãƒ¼ãƒˆãƒªãƒŸãƒƒãƒˆ) ã®å ´åˆ
                            attempt += 1
                            print(f"âš ï¸ APIåˆ¶é™ã¾ãŸã¯ä¸€æ™‚çš„ãªã‚µãƒ¼ãƒãƒ¼ã‚¨ãƒ©ãƒ¼ (ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚³ãƒ¼ãƒ‰: {e.response.status_code})ã€‚è©¦è¡Œ {attempt}/{MAX_RETRIES}: {e}")
                            
                            # GitHub APIã®ãƒ¬ãƒ¼ãƒˆãƒªãƒŸãƒƒãƒˆãƒ˜ãƒƒãƒ€ãƒ¼ãŒã‚ã‚Œã°åˆ©ç”¨
                            retry_after = e.response.headers.get("Retry-After")
                            if retry_after:
                                try:
                                    wait_time = int(retry_after) + 5 # Retry-Afterã«å°‘ã—ä½™è£•ã‚’æŒãŸã›ã‚‹
                                    current_wait_time = min(wait_time, MAX_RETRY_WAIT_SECONDS)
                                    print(f"ã€ŒRetry-Afterã€ãƒ˜ãƒƒãƒ€ãƒ¼ã«åŸºã¥ã„ã¦ {current_wait_time}ç§’å¾…æ©Ÿã—ã¾ã™...")
                                except ValueError:
                                    print(f"ã€ŒRetry-Afterã€ãƒ˜ãƒƒãƒ€ãƒ¼ã®å€¤ãŒç„¡åŠ¹ã§ã™ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§ {current_wait_time}ç§’å¾…æ©Ÿã—ã¾ã™...")
                                time.sleep(current_wait_time)
                            else:
                                print(f"{current_wait_time}ç§’å¾…æ©Ÿã—ã¦å†è©¦è¡Œã—ã¾ã™...")
                                time.sleep(current_wait_time)
                                current_wait_time = min(current_wait_time * 2, MAX_RETRY_WAIT_SECONDS) # Exponential backoff
                        else:
                            # ãã®ä»–ã®HTTPã‚¨ãƒ©ãƒ¼ï¼ˆ5xxç³»ãªã©ï¼‰
                            attempt += 1
                            print(f"âš ï¸ ãã®ä»–ã®HTTPã‚¨ãƒ©ãƒ¼ (ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚³ãƒ¼ãƒ‰: {e.response.status_code})ã€‚è©¦è¡Œ {attempt}/{MAX_RETRIES}: {e}")
                            print(f"{current_wait_time}ç§’å¾…æ©Ÿã—ã¦å†è©¦è¡Œã—ã¾ã™...")
                            time.sleep(current_wait_time)
                            current_wait_time = min(current_wait_time * 2, MAX_RETRY_WAIT_SECONDS)

                    except requests.exceptions.Timeout:
                        attempt += 1
                        print(f"âš ï¸ ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã‚¨ãƒ©ãƒ¼ (è©¦è¡Œ {attempt}/{MAX_RETRIES}): {url}")
                        print(f"{current_wait_time}ç§’å¾…æ©Ÿã—ã¦å†è©¦è¡Œã—ã¾ã™...")
                        time.sleep(current_wait_time)
                        current_wait_time = min(current_wait_time * 2, MAX_RETRY_WAIT_SECONDS)
                    
                    except requests.exceptions.ConnectionError as e:
                        attempt += 1
                        print(f"âš ï¸ æ¥ç¶šã‚¨ãƒ©ãƒ¼ (è©¦è¡Œ {attempt}/{MAX_RETRIES}): {e}")
                        print(f"{current_wait_time}ç§’å¾…æ©Ÿã—ã¦å†è©¦è¡Œã—ã¾ã™...")
                        time.sleep(current_wait_time)
                        current_wait_time = min(current_wait_time * 2, MAX_RETRY_WAIT_SECONDS)

                    except requests.exceptions.RequestException as e:
                        # ä¸Šè¨˜ä»¥å¤–ã®requestsé–¢é€£ã®ä¸€èˆ¬ã‚¨ãƒ©ãƒ¼
                        attempt += 1
                        print(f"âš ï¸ ãã®ä»–ã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼ (è©¦è¡Œ {attempt}/{MAX_RETRIES}): {e}")
                        print(f"{current_wait_time}ç§’å¾…æ©Ÿã—ã¦å†è©¦è¡Œã—ã¾ã™...")
                        time.sleep(current_wait_time)
                        current_wait_time = min(current_wait_time * 2, MAX_RETRY_WAIT_SECONDS)
                
                # ãƒªãƒˆãƒ©ã‚¤å›æ•°ã‚’ã‚ªãƒ¼ãƒãƒ¼ã—ã¦ã‚‚æˆåŠŸã—ãªã‹ã£ãŸå ´åˆ
                if not download_successful:
                    print(f"âŒ æœ€å¤§ãƒªãƒˆãƒ©ã‚¤å›æ•°ã«é”ã—ã¾ã—ãŸã€‚ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™: {url}")
                    log_failed_download(current_date, hour, url, "Max retries exceeded or unhandled error")

                time.sleep(3) # æ¬¡ã®æ™‚é–“ã¸ã®ã‚¤ãƒ³ã‚¿ãƒ¼ãƒãƒ«ã€‚ã‚µãƒ¼ãƒãƒ¼è² è·è»½æ¸›ã®ãŸã‚

            current_date += timedelta(days=1)
            
    finally:
        # ã‚¹ã‚¯ãƒªãƒ—ãƒˆçµ‚äº†æ™‚ã«é–‹ã„ã¦ã„ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å¿…ãšã‚¯ãƒ­ãƒ¼ã‚ºã™ã‚‹
        if output_file_handle:
            output_file_handle.close()
            print(f"\nâœ… æœ€å¾Œã®ãƒ•ã‚¡ã‚¤ãƒ« ({current_file_month}) ã‚’ä¿å­˜ã—ã¾ã—ãŸã€‚")
        
    print("ğŸ‰ å…¨ã¦ã®å‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸã€‚")

if __name__ == "__main__":
    download_and_filter_archive(OUTPUT_FILE_BASENAME)